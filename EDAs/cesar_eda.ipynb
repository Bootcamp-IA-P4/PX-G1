{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv(\"../data/youtoxic_english_1000.csv\")\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensiones del dataset\n",
    "print(f\"Filas: {df.shape[0]}, Columnas: {df.shape[1]}\")\n",
    "\n",
    "df.info()\n",
    "\n",
    "print(\"\\nValores nulos por columna:\")\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribución del target IsToxic\n",
    "sns.countplot(x=\"IsToxic\", data=df)\n",
    "plt.title(\"Distribución de comentarios tóxicos vs no tóxicos\")\n",
    "plt.xlabel(\"¿Es tóxico?\")\n",
    "plt.ylabel(\"Número de comentarios\")\n",
    "plt.show()\n",
    "\n",
    "# Porcentaje de cada clase\n",
    "toxicity_ratio = df[\"IsToxic\"].value_counts(normalize=True) * 100\n",
    "print(\"Distribución porcentual:\\n\", toxicity_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear columnas nuevas para análisis de texto\n",
    "df[\"text_length_chars\"] = df[\"Text\"].apply(len)\n",
    "df[\"text_length_words\"] = df[\"Text\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Mostrar estadísticos por tipo de comentario\n",
    "df.groupby(\"IsToxic\")[[\"text_length_chars\", \"text_length_words\"]].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots para comparar longitud de texto por clase\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "sns.boxplot(x=\"IsToxic\", y=\"text_length_chars\", data=df, ax=axes[0])\n",
    "axes[0].set_title(\"Longitud en caracteres por tipo de comentario\")\n",
    "\n",
    "sns.boxplot(x=\"IsToxic\", y=\"text_length_words\", data=df, ax=axes[1])\n",
    "axes[1].set_title(\"Longitud en palabras por tipo de comentario\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Comparativa de longitud de los comentarios (Boxplot)\n",
    "\n",
    "En esta visualización comparamos la **longitud de los comentarios tóxicos y no tóxicos**, tanto en número de caracteres como de palabras.\n",
    "\n",
    "#### ¿Qué es un boxplot?\n",
    "Un **boxplot** (o diagrama de caja) es una representación visual que nos permite ver:\n",
    "- La **mediana** del conjunto de datos (línea central de la caja).\n",
    "- El **rango intercuartílico** (donde se concentra el 50% de los valores).\n",
    "- Los **valores extremos o outliers** (representados como puntos).\n",
    "\n",
    "#### ¿Qué vemos en estos gráficos?\n",
    "- La longitud de los comentarios **tóxicos y no tóxicos** es muy similar.\n",
    "- Hay **muchos comentarios cortos** en ambos grupos.\n",
    "- Aparecen **algunos comentarios muy largos** (outliers), especialmente en caracteres.\n",
    "- No se observa una diferencia clara que nos permita decir que un tipo de comentario sea más largo que otro de forma sistemática.\n",
    "\n",
    "#### ¿Qué concluimos?\n",
    "Aunque puede haber pequeñas diferencias, **la longitud del comentario no parece ser un buen indicador por sí solo de si un comentario es tóxico o no**. Aun así, es útil conocer estas características para posibles decisiones de preprocesamiento, como filtrar comentarios excesivamente largos o cortos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar comentarios tóxicos y no tóxicos\n",
    "toxic_comments = df[df[\"IsToxic\"] == True][\"Text\"]\n",
    "nontoxic_comments = df[df[\"IsToxic\"] == False][\"Text\"]\n",
    "\n",
    "# Juntarlos en dos grandes textos para analizarlos\n",
    "toxic_text = \" \".join(toxic_comments).lower()\n",
    "nontoxic_text = \" \".join(nontoxic_comments).lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Análisis del contenido textual\n",
    "\n",
    "Para entender mejor las diferencias entre los comentarios tóxicos y no tóxicos, hemos separado los textos en dos grupos:\n",
    "\n",
    "- Comentarios etiquetados como **tóxicos**.\n",
    "- Comentarios etiquetados como **no tóxicos**.\n",
    "\n",
    "Hemos unido los comentarios de cada grupo en un único texto para poder analizar qué palabras aparecen con mayor frecuencia en cada uno. Este enfoque nos permitirá visualizar patrones de lenguaje característicos, que luego pueden ser clave para entrenar un modelo predictivo.\n",
    "\n",
    "En los próximos pasos generaremos:\n",
    "- Listados de palabras más frecuentes.\n",
    "- Nubes de palabras (*wordclouds*).\n",
    "- N-gramas más comunes (combinaciones típicas de 2-3 palabras).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Función para limpiar texto básico (sin lematizar aún)\n",
    "def limpiar_texto(texto):\n",
    "    texto = re.sub(r\"[^\\w\\s]\", \"\", texto)  # quitar signos de puntuación\n",
    "    texto = texto.lower()  # pasar a minúsculas\n",
    "    return texto\n",
    "\n",
    "# Aplicar limpieza y dividir en palabras\n",
    "palabras_toxicas = limpiar_texto(toxic_text).split()\n",
    "palabras_no_toxicas = limpiar_texto(nontoxic_text).split()\n",
    "\n",
    "# Contar palabras más comunes\n",
    "frecuentes_toxicas = Counter(palabras_toxicas).most_common(10)\n",
    "frecuentes_no_toxicas = Counter(palabras_no_toxicas).most_common(10)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"🔴 Palabras más frecuentes en comentarios tóxicos:\")\n",
    "for palabra, freq in frecuentes_toxicas:\n",
    "    print(f\"{palabra}: {freq}\")\n",
    "\n",
    "print(\"\\n🟢 Palabras más frecuentes en comentarios no tóxicos:\")\n",
    "for palabra, freq in frecuentes_no_toxicas:\n",
    "    print(f\"{palabra}: {freq}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Lista de stopwords en inglés\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Función mejorada que filtra stopwords\n",
    "def limpiar_y_filtrar(texto):\n",
    "    texto = re.sub(r\"[^\\w\\s]\", \"\", texto.lower())\n",
    "    palabras = texto.split()\n",
    "    return [p for p in palabras if p not in stop_words]\n",
    "\n",
    "# Aplicar función mejorada\n",
    "palabras_toxicas_filtradas = limpiar_y_filtrar(toxic_text)\n",
    "palabras_no_toxicas_filtradas = limpiar_y_filtrar(nontoxic_text)\n",
    "\n",
    "# Contar palabras más frecuentes (filtradas)\n",
    "frecuentes_toxicas_filtradas = Counter(palabras_toxicas_filtradas).most_common(10)\n",
    "frecuentes_no_toxicas_filtradas = Counter(palabras_no_toxicas_filtradas).most_common(10)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"🔴 Palabras más frecuentes (tóxicos, sin stopwords):\")\n",
    "for palabra, freq in frecuentes_toxicas_filtradas:\n",
    "    print(f\"{palabra}: {freq}\")\n",
    "\n",
    "print(\"\\n🟢 Palabras más frecuentes (no tóxicos, sin stopwords):\")\n",
    "for palabra, freq in frecuentes_no_toxicas_filtradas:\n",
    "    print(f\"{palabra}: {freq}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Análisis con eliminación de stopwords\n",
    "\n",
    "Las palabras más frecuentes que observamos inicialmente eran muy comunes y poco informativas. Por ello, hemos repetido el análisis **eliminando las stopwords**, es decir, palabras muy frecuentes en inglés que no aportan significado real (como \"the\", \"and\", \"is\", etc.).\n",
    "\n",
    "Esta limpieza **no forma aún parte del preprocesamiento oficial**, pero se introduce aquí como una forma de enriquecer el EDA y tomar decisiones más informadas.\n",
    "\n",
    "Ahora los resultados empiezan a revelar **patrones de contenido más relevantes** para entender qué vocabulario podría distinguir los comentarios tóxicos de los no tóxicos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Unir las palabras ya filtradas\n",
    "texto_toxico_filtrado = \" \".join(palabras_toxicas_filtradas)\n",
    "texto_nontoxico_filtrado = \" \".join(palabras_no_toxicas_filtradas)\n",
    "\n",
    "# Crear las nubes\n",
    "wc_toxico = WordCloud(width=800, height=400, background_color=\"white\").generate(texto_toxico_filtrado)\n",
    "wc_nontoxico = WordCloud(width=800, height=400, background_color=\"white\").generate(texto_nontoxico_filtrado)\n",
    "\n",
    "# Mostrar\n",
    "fig, axs = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "axs[0].imshow(wc_toxico, interpolation=\"bilinear\")\n",
    "axs[0].axis(\"off\")\n",
    "axs[0].set_title(\"🔴 Comentarios tóxicos (sin stopwords)\")\n",
    "\n",
    "axs[1].imshow(wc_nontoxico, interpolation=\"bilinear\")\n",
    "axs[1].axis(\"off\")\n",
    "axs[1].set_title(\"🟢 Comentarios no tóxicos (sin stopwords)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### WordClouds sin palabras vacías (stopwords)\n",
    "\n",
    "Las siguientes nubes de palabras muestran los términos más repetidos en los comentarios **tóxicos** y **no tóxicos**, tras eliminar las palabras vacías típicas del inglés (como “the”, “and”, “is”…).\n",
    "\n",
    "#### ¿Qué observamos?\n",
    "- En los comentarios tóxicos aparecen con más frecuencia palabras como **“fuck”**, lo que indica un tono agresivo.\n",
    "- También se observan términos raciales y relacionados con el orden público (**black, white, police**) en ambos grupos, lo que sugiere que el contexto es similar, pero el uso del lenguaje es lo que cambia.\n",
    "\n",
    "Esta visualización permite a cualquier lector, incluso sin formación técnica, entender mejor el tipo de lenguaje que caracteriza cada grupo de comentarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Utilizamos los textos ya filtrados de stopwords\n",
    "def mostrar_ngrams(textos, n=2, top=10):\n",
    "    vectorizer = CountVectorizer(ngram_range=(n, n))\n",
    "    X = vectorizer.fit_transform(textos)\n",
    "    suma = X.sum(axis=0)\n",
    "    freqs = [(word, suma[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "    freqs_sorted = sorted(freqs, key=lambda x: x[1], reverse=True)\n",
    "    return freqs_sorted[:top]\n",
    "\n",
    "# Crear listas de texto plano por grupo\n",
    "comentarios_toxicos = df[df[\"IsToxic\"] == True][\"Text\"].tolist()\n",
    "comentarios_no_toxicos = df[df[\"IsToxic\"] == False][\"Text\"].tolist()\n",
    "\n",
    "# Obtener n-gramas\n",
    "top_bigrams_toxicos = mostrar_ngrams(comentarios_toxicos, n=2)\n",
    "top_bigrams_nontoxicos = mostrar_ngrams(comentarios_no_toxicos, n=2)\n",
    "\n",
    "top_trigrams_toxicos = mostrar_ngrams(comentarios_toxicos, n=3)\n",
    "top_trigrams_nontoxicos = mostrar_ngrams(comentarios_no_toxicos, n=3)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"🔴 Bigrams más comunes en comentarios tóxicos:\")\n",
    "for frase, freq in top_bigrams_toxicos:\n",
    "    print(f\"{frase}: {freq}\")\n",
    "\n",
    "print(\"\\n🟢 Bigrams más comunes en comentarios no tóxicos:\")\n",
    "for frase, freq in top_bigrams_nontoxicos:\n",
    "    print(f\"{frase}: {freq}\")\n",
    "\n",
    "print(\"\\n🔴 Trigrams más comunes en comentarios tóxicos:\")\n",
    "for frase, freq in top_trigrams_toxicos:\n",
    "    print(f\"{frase}: {freq}\")\n",
    "\n",
    "print(\"\\n🟢 Trigrams más comunes en comentarios no tóxicos:\")\n",
    "for frase, freq in top_trigrams_nontoxicos:\n",
    "    print(f\"{frase}: {freq}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### 🔗 Análisis de n-gramas (bigrams y trigrams)\n",
    "\n",
    "Los **n-gramas** son combinaciones de palabras consecutivas que nos permiten identificar expresiones típicas o patrones de lenguaje. Son especialmente útiles en tareas como detección de discurso de odio, ya que muchas veces el contenido ofensivo se transmite en frases cortas y recurrentes.\n",
    "\n",
    "#### ¿Qué buscamos aquí?\n",
    "- Expresiones como “go back”, “shut up”, “you people” pueden tener fuerte carga ofensiva.\n",
    "- Otras como “thank you” o “great video” pueden caracterizar comentarios positivos o neutrales.\n",
    "\n",
    "Este análisis nos acerca a una comprensión más contextualizada del lenguaje usado, lo que será de gran valor a la hora de entrenar modelos de clasificación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extraer trigramas y frecuencias por separado\n",
    "trigrams_tox, freqs_tox = zip(*top_trigrams_toxicos)\n",
    "trigrams_notox, freqs_notox = zip(*top_trigrams_nontoxicos)\n",
    "\n",
    "# Crear gráfico de barras horizontales\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Trigramas tóxicos\n",
    "axes[0].barh(trigrams_tox[::-1], freqs_tox[::-1], color=\"crimson\")\n",
    "axes[0].set_title(\"🔴 Trigramas más comunes en comentarios tóxicos\")\n",
    "axes[0].set_xlabel(\"Frecuencia\")\n",
    "axes[0].tick_params(axis='y', labelsize=10)\n",
    "\n",
    "# Trigramas no tóxicos\n",
    "axes[1].barh(trigrams_notox[::-1], freqs_notox[::-1], color=\"seagreen\")\n",
    "axes[1].set_title(\"🟢 Trigramas más comunes en comentarios no tóxicos\")\n",
    "axes[1].set_xlabel(\"Frecuencia\")\n",
    "axes[1].tick_params(axis='y', labelsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Visualización de trigramas más frecuentes\n",
    "\n",
    "Los trigramas (combinaciones de tres palabras consecutivas) nos permiten observar expresiones completas y típicas de cada tipo de comentario.\n",
    "\n",
    "#### 🔴 En los comentarios tóxicos:\n",
    "- Aparecen insultos explícitos como **“piece of shit”**.\n",
    "- Expresiones violentas como **“run them over”** o **“shoot to apprehend”**.\n",
    "- Términos relacionados con movimientos sociales cargados emocionalmente: **“black lives matter”**.\n",
    "\n",
    "#### 🟢 En los comentarios no tóxicos:\n",
    "- Predominan expresiones de agradecimiento (**“thank you for”**) o frases explicativas/descriptivas.\n",
    "- Se nota un tono más **objetivo, civil o analítico**.\n",
    "\n",
    "Este contraste deja ver claramente cómo cambia la intención del lenguaje entre ambos grupos y valida el uso de trigramas como posibles características valiosas para entrenar el modelo de detección de toxicidad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de etiquetas del dataset\n",
    "etiquetas = [\n",
    "    \"IsToxic\", \"IsAbusive\", \"IsThreat\", \"IsProvocative\", \"IsObscene\",\n",
    "    \"IsHatespeech\", \"IsRacist\", \"IsNationalist\", \"IsSexist\", \"IsHomophobic\",\n",
    "    \"IsReligiousHate\", \"IsRadicalism\"\n",
    "]\n",
    "\n",
    "# Calcular la correlación entre etiquetas\n",
    "correlaciones = df[etiquetas].corr()\n",
    "\n",
    "# Calcular cuántas etiquetas activas hay por comentario\n",
    "df[\"TotalEtiquetas\"] = df[etiquetas].sum(axis=1)\n",
    "etiquetas_activas = df[\"TotalEtiquetas\"].value_counts().sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualización de correlación entre etiquetas\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlaciones, annot=True, cmap=\"coolwarm\", fmt=\".2f\", vmin=-1, vmax=1)\n",
    "plt.title(\"🔗 Correlación entre etiquetas de toxicidad\")\n",
    "plt.show()\n",
    "\n",
    "# Visualización de número de etiquetas activas por comentario\n",
    "etiquetas_activas.plot(kind=\"bar\", color=\"steelblue\", figsize=(8, 5))\n",
    "plt.title(\"🎯 Número de etiquetas activas por comentario\")\n",
    "plt.xlabel(\"Cantidad de etiquetas\")\n",
    "plt.ylabel(\"Número de comentarios\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis=\"y\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Análisis cruzado de etiquetas\n",
    "\n",
    "Además del análisis textual, es importante entender cómo se comportan las diferentes **subcategorías de toxicidad** que aparecen en el dataset (como `IsAbusive`, `IsThreat`, `IsRacist`, etc.).\n",
    "\n",
    "#### ¿Cuántas etiquetas tiene cada comentario?\n",
    "\n",
    "- Más de la mitad de los comentarios **no presentan ninguna etiqueta activa**.\n",
    "- El resto puede tener múltiples etiquetas, lo que sugiere que **el problema podría ser tratado como multietiqueta** si quisiéramos predecir más allá de `IsToxic`.\n",
    "\n",
    "#### ¿Qué etiquetas están relacionadas?\n",
    "\n",
    "- `IsToxic` tiene una fuerte correlación con `IsAbusive` (**0.80**) y `IsProvocative`.\n",
    "- `IsHatespeech` y `IsRacist` tienen una **correlación altísima** (**0.94**), lo que indica que suelen ir juntas.\n",
    "\n",
    "Este análisis refuerza la idea de que la toxicidad **no es una dimensión única**, sino que puede tener varias expresiones combinadas. Si decidiéramos construir un modelo más complejo en el futuro, podríamos abordar este problema como una clasificación **multietiqueta** en lugar de binaria.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
