{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv(\"../data/youtoxic_english_1000.csv\")\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensiones del dataset\n",
    "print(f\"Filas: {df.shape[0]}, Columnas: {df.shape[1]}\")\n",
    "\n",
    "df.info()\n",
    "\n",
    "print(\"\\nValores nulos por columna:\")\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribución del target IsToxic\n",
    "sns.countplot(x=\"IsToxic\", data=df)\n",
    "plt.title(\"Distribución de comentarios tóxicos vs no tóxicos\")\n",
    "plt.xlabel(\"¿Es tóxico?\")\n",
    "plt.ylabel(\"Número de comentarios\")\n",
    "plt.show()\n",
    "\n",
    "# Porcentaje de cada clase\n",
    "toxicity_ratio = df[\"IsToxic\"].value_counts(normalize=True) * 100\n",
    "print(\"Distribución porcentual:\\n\", toxicity_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear columnas nuevas para análisis de texto\n",
    "df[\"text_length_chars\"] = df[\"Text\"].apply(len)\n",
    "df[\"text_length_words\"] = df[\"Text\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Mostrar estadísticos por tipo de comentario\n",
    "df.groupby(\"IsToxic\")[[\"text_length_chars\", \"text_length_words\"]].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots para comparar longitud de texto por clase\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "sns.boxplot(x=\"IsToxic\", y=\"text_length_chars\", data=df, ax=axes[0])\n",
    "axes[0].set_title(\"Longitud en caracteres por tipo de comentario\")\n",
    "\n",
    "sns.boxplot(x=\"IsToxic\", y=\"text_length_words\", data=df, ax=axes[1])\n",
    "axes[1].set_title(\"Longitud en palabras por tipo de comentario\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Comparativa de longitud de los comentarios (Boxplot)\n",
    "\n",
    "En esta visualización comparamos la **longitud de los comentarios tóxicos y no tóxicos**, tanto en número de caracteres como de palabras.\n",
    "\n",
    "#### ¿Qué es un boxplot?\n",
    "Un **boxplot** (o diagrama de caja) es una representación visual que nos permite ver:\n",
    "- La **mediana** del conjunto de datos (línea central de la caja).\n",
    "- El **rango intercuartílico** (donde se concentra el 50% de los valores).\n",
    "- Los **valores extremos o outliers** (representados como puntos).\n",
    "\n",
    "#### ¿Qué vemos en estos gráficos?\n",
    "- La longitud de los comentarios **tóxicos y no tóxicos** es muy similar.\n",
    "- Hay **muchos comentarios cortos** en ambos grupos.\n",
    "- Aparecen **algunos comentarios muy largos** (outliers), especialmente en caracteres.\n",
    "- No se observa una diferencia clara que nos permita decir que un tipo de comentario sea más largo que otro de forma sistemática.\n",
    "\n",
    "#### ¿Qué concluimos?\n",
    "Aunque puede haber pequeñas diferencias, **la longitud del comentario no parece ser un buen indicador por sí solo de si un comentario es tóxico o no**. Aun así, es útil conocer estas características para posibles decisiones de preprocesamiento, como filtrar comentarios excesivamente largos o cortos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar comentarios tóxicos y no tóxicos\n",
    "toxic_comments = df[df[\"IsToxic\"] == True][\"Text\"]\n",
    "nontoxic_comments = df[df[\"IsToxic\"] == False][\"Text\"]\n",
    "\n",
    "# Juntarlos en dos grandes textos para analizarlos\n",
    "toxic_text = \" \".join(toxic_comments).lower()\n",
    "nontoxic_text = \" \".join(nontoxic_comments).lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Análisis del contenido textual\n",
    "\n",
    "Para entender mejor las diferencias entre los comentarios tóxicos y no tóxicos, hemos separado los textos en dos grupos:\n",
    "\n",
    "- Comentarios etiquetados como **tóxicos**.\n",
    "- Comentarios etiquetados como **no tóxicos**.\n",
    "\n",
    "Hemos unido los comentarios de cada grupo en un único texto para poder analizar qué palabras aparecen con mayor frecuencia en cada uno. Este enfoque nos permitirá visualizar patrones de lenguaje característicos, que luego pueden ser clave para entrenar un modelo predictivo.\n",
    "\n",
    "En los próximos pasos generaremos:\n",
    "- Listados de palabras más frecuentes.\n",
    "- Nubes de palabras (*wordclouds*).\n",
    "- N-gramas más comunes (combinaciones típicas de 2-3 palabras).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
