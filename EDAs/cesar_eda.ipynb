{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv(\"../data/youtoxic_english_1000.csv\")\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensiones del dataset\n",
    "print(f\"Filas: {df.shape[0]}, Columnas: {df.shape[1]}\")\n",
    "\n",
    "df.info()\n",
    "\n",
    "print(\"\\nValores nulos por columna:\")\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribución del target IsToxic\n",
    "sns.countplot(x=\"IsToxic\", data=df)\n",
    "plt.title(\"Distribución de comentarios tóxicos vs no tóxicos\")\n",
    "plt.xlabel(\"¿Es tóxico?\")\n",
    "plt.ylabel(\"Número de comentarios\")\n",
    "plt.show()\n",
    "\n",
    "# Porcentaje de cada clase\n",
    "toxicity_ratio = df[\"IsToxic\"].value_counts(normalize=True) * 100\n",
    "print(\"Distribución porcentual:\\n\", toxicity_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear columnas nuevas para análisis de texto\n",
    "df[\"text_length_chars\"] = df[\"Text\"].apply(len)\n",
    "df[\"text_length_words\"] = df[\"Text\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Mostrar estadísticos por tipo de comentario\n",
    "df.groupby(\"IsToxic\")[[\"text_length_chars\", \"text_length_words\"]].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots para comparar longitud de texto por clase\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "sns.boxplot(x=\"IsToxic\", y=\"text_length_chars\", data=df, ax=axes[0])\n",
    "axes[0].set_title(\"Longitud en caracteres por tipo de comentario\")\n",
    "\n",
    "sns.boxplot(x=\"IsToxic\", y=\"text_length_words\", data=df, ax=axes[1])\n",
    "axes[1].set_title(\"Longitud en palabras por tipo de comentario\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Comparativa de longitud de los comentarios (Boxplot)\n",
    "\n",
    "En esta visualización comparamos la **longitud de los comentarios tóxicos y no tóxicos**, tanto en número de caracteres como de palabras.\n",
    "\n",
    "#### ¿Qué es un boxplot?\n",
    "Un **boxplot** (o diagrama de caja) es una representación visual que nos permite ver:\n",
    "- La **mediana** del conjunto de datos (línea central de la caja).\n",
    "- El **rango intercuartílico** (donde se concentra el 50% de los valores).\n",
    "- Los **valores extremos o outliers** (representados como puntos).\n",
    "\n",
    "#### ¿Qué vemos en estos gráficos?\n",
    "- La longitud de los comentarios **tóxicos y no tóxicos** es muy similar.\n",
    "- Hay **muchos comentarios cortos** en ambos grupos.\n",
    "- Aparecen **algunos comentarios muy largos** (outliers), especialmente en caracteres.\n",
    "- No se observa una diferencia clara que nos permita decir que un tipo de comentario sea más largo que otro de forma sistemática.\n",
    "\n",
    "#### ¿Qué concluimos?\n",
    "Aunque puede haber pequeñas diferencias, **la longitud del comentario no parece ser un buen indicador por sí solo de si un comentario es tóxico o no**. Aun así, es útil conocer estas características para posibles decisiones de preprocesamiento, como filtrar comentarios excesivamente largos o cortos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar comentarios tóxicos y no tóxicos\n",
    "toxic_comments = df[df[\"IsToxic\"] == True][\"Text\"]\n",
    "nontoxic_comments = df[df[\"IsToxic\"] == False][\"Text\"]\n",
    "\n",
    "# Juntarlos en dos grandes textos para analizarlos\n",
    "toxic_text = \" \".join(toxic_comments).lower()\n",
    "nontoxic_text = \" \".join(nontoxic_comments).lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Análisis del contenido textual\n",
    "\n",
    "Para entender mejor las diferencias entre los comentarios tóxicos y no tóxicos, hemos separado los textos en dos grupos:\n",
    "\n",
    "- Comentarios etiquetados como **tóxicos**.\n",
    "- Comentarios etiquetados como **no tóxicos**.\n",
    "\n",
    "Hemos unido los comentarios de cada grupo en un único texto para poder analizar qué palabras aparecen con mayor frecuencia en cada uno. Este enfoque nos permitirá visualizar patrones de lenguaje característicos, que luego pueden ser clave para entrenar un modelo predictivo.\n",
    "\n",
    "En los próximos pasos generaremos:\n",
    "- Listados de palabras más frecuentes.\n",
    "- Nubes de palabras (*wordclouds*).\n",
    "- N-gramas más comunes (combinaciones típicas de 2-3 palabras).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Función para limpiar texto básico (sin lematizar aún)\n",
    "def limpiar_texto(texto):\n",
    "    texto = re.sub(r\"[^\\w\\s]\", \"\", texto)  # quitar signos de puntuación\n",
    "    texto = texto.lower()  # pasar a minúsculas\n",
    "    return texto\n",
    "\n",
    "# Aplicar limpieza y dividir en palabras\n",
    "palabras_toxicas = limpiar_texto(toxic_text).split()\n",
    "palabras_no_toxicas = limpiar_texto(nontoxic_text).split()\n",
    "\n",
    "# Contar palabras más comunes\n",
    "frecuentes_toxicas = Counter(palabras_toxicas).most_common(10)\n",
    "frecuentes_no_toxicas = Counter(palabras_no_toxicas).most_common(10)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"🔴 Palabras más frecuentes en comentarios tóxicos:\")\n",
    "for palabra, freq in frecuentes_toxicas:\n",
    "    print(f\"{palabra}: {freq}\")\n",
    "\n",
    "print(\"\\n🟢 Palabras más frecuentes en comentarios no tóxicos:\")\n",
    "for palabra, freq in frecuentes_no_toxicas:\n",
    "    print(f\"{palabra}: {freq}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Lista de stopwords en inglés\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Función mejorada que filtra stopwords\n",
    "def limpiar_y_filtrar(texto):\n",
    "    texto = re.sub(r\"[^\\w\\s]\", \"\", texto.lower())\n",
    "    palabras = texto.split()\n",
    "    return [p for p in palabras if p not in stop_words]\n",
    "\n",
    "# Aplicar función mejorada\n",
    "palabras_toxicas_filtradas = limpiar_y_filtrar(toxic_text)\n",
    "palabras_no_toxicas_filtradas = limpiar_y_filtrar(nontoxic_text)\n",
    "\n",
    "# Contar palabras más frecuentes (filtradas)\n",
    "frecuentes_toxicas_filtradas = Counter(palabras_toxicas_filtradas).most_common(10)\n",
    "frecuentes_no_toxicas_filtradas = Counter(palabras_no_toxicas_filtradas).most_common(10)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"🔴 Palabras más frecuentes (tóxicos, sin stopwords):\")\n",
    "for palabra, freq in frecuentes_toxicas_filtradas:\n",
    "    print(f\"{palabra}: {freq}\")\n",
    "\n",
    "print(\"\\n🟢 Palabras más frecuentes (no tóxicos, sin stopwords):\")\n",
    "for palabra, freq in frecuentes_no_toxicas_filtradas:\n",
    "    print(f\"{palabra}: {freq}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Análisis con eliminación de stopwords\n",
    "\n",
    "Las palabras más frecuentes que observamos inicialmente eran muy comunes y poco informativas. Por ello, hemos repetido el análisis **eliminando las stopwords**, es decir, palabras muy frecuentes en inglés que no aportan significado real (como \"the\", \"and\", \"is\", etc.).\n",
    "\n",
    "Esta limpieza **no forma aún parte del preprocesamiento oficial**, pero se introduce aquí como una forma de enriquecer el EDA y tomar decisiones más informadas.\n",
    "\n",
    "Ahora los resultados empiezan a revelar **patrones de contenido más relevantes** para entender qué vocabulario podría distinguir los comentarios tóxicos de los no tóxicos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Unir las palabras ya filtradas\n",
    "texto_toxico_filtrado = \" \".join(palabras_toxicas_filtradas)\n",
    "texto_nontoxico_filtrado = \" \".join(palabras_no_toxicas_filtradas)\n",
    "\n",
    "# Crear las nubes\n",
    "wc_toxico = WordCloud(width=800, height=400, background_color=\"white\").generate(texto_toxico_filtrado)\n",
    "wc_nontoxico = WordCloud(width=800, height=400, background_color=\"white\").generate(texto_nontoxico_filtrado)\n",
    "\n",
    "# Mostrar\n",
    "fig, axs = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "axs[0].imshow(wc_toxico, interpolation=\"bilinear\")\n",
    "axs[0].axis(\"off\")\n",
    "axs[0].set_title(\"🔴 Comentarios tóxicos (sin stopwords)\")\n",
    "\n",
    "axs[1].imshow(wc_nontoxico, interpolation=\"bilinear\")\n",
    "axs[1].axis(\"off\")\n",
    "axs[1].set_title(\"🟢 Comentarios no tóxicos (sin stopwords)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### WordClouds sin palabras vacías (stopwords)\n",
    "\n",
    "Las siguientes nubes de palabras muestran los términos más repetidos en los comentarios **tóxicos** y **no tóxicos**, tras eliminar las palabras vacías típicas del inglés (como “the”, “and”, “is”…).\n",
    "\n",
    "#### ¿Qué observamos?\n",
    "- En los comentarios tóxicos aparecen con más frecuencia palabras como **“fuck”**, lo que indica un tono agresivo.\n",
    "- También se observan términos raciales y relacionados con el orden público (**black, white, police**) en ambos grupos, lo que sugiere que el contexto es similar, pero el uso del lenguaje es lo que cambia.\n",
    "\n",
    "Esta visualización permite a cualquier lector, incluso sin formación técnica, entender mejor el tipo de lenguaje que caracteriza cada grupo de comentarios.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
