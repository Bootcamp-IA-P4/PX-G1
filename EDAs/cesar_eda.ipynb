{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv(\"../data/youtoxic_english_1000.csv\")\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensiones del dataset\n",
    "print(f\"Filas: {df.shape[0]}, Columnas: {df.shape[1]}\")\n",
    "\n",
    "df.info()\n",
    "\n",
    "print(\"\\nValores nulos por columna:\")\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribuci√≥n del target IsToxic\n",
    "sns.countplot(x=\"IsToxic\", data=df)\n",
    "plt.title(\"Distribuci√≥n de comentarios t√≥xicos vs no t√≥xicos\")\n",
    "plt.xlabel(\"¬øEs t√≥xico?\")\n",
    "plt.ylabel(\"N√∫mero de comentarios\")\n",
    "plt.show()\n",
    "\n",
    "# Porcentaje de cada clase\n",
    "toxicity_ratio = df[\"IsToxic\"].value_counts(normalize=True) * 100\n",
    "print(\"Distribuci√≥n porcentual:\\n\", toxicity_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear columnas nuevas para an√°lisis de texto\n",
    "df[\"text_length_chars\"] = df[\"Text\"].apply(len)\n",
    "df[\"text_length_words\"] = df[\"Text\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Mostrar estad√≠sticos por tipo de comentario\n",
    "df.groupby(\"IsToxic\")[[\"text_length_chars\", \"text_length_words\"]].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots para comparar longitud de texto por clase\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "sns.boxplot(x=\"IsToxic\", y=\"text_length_chars\", data=df, ax=axes[0])\n",
    "axes[0].set_title(\"Longitud en caracteres por tipo de comentario\")\n",
    "\n",
    "sns.boxplot(x=\"IsToxic\", y=\"text_length_words\", data=df, ax=axes[1])\n",
    "axes[1].set_title(\"Longitud en palabras por tipo de comentario\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Comparativa de longitud de los comentarios (Boxplot)\n",
    "\n",
    "En esta visualizaci√≥n comparamos la **longitud de los comentarios t√≥xicos y no t√≥xicos**, tanto en n√∫mero de caracteres como de palabras.\n",
    "\n",
    "#### ¬øQu√© es un boxplot?\n",
    "Un **boxplot** (o diagrama de caja) es una representaci√≥n visual que nos permite ver:\n",
    "- La **mediana** del conjunto de datos (l√≠nea central de la caja).\n",
    "- El **rango intercuart√≠lico** (donde se concentra el 50% de los valores).\n",
    "- Los **valores extremos o outliers** (representados como puntos).\n",
    "\n",
    "#### ¬øQu√© vemos en estos gr√°ficos?\n",
    "- La longitud de los comentarios **t√≥xicos y no t√≥xicos** es muy similar.\n",
    "- Hay **muchos comentarios cortos** en ambos grupos.\n",
    "- Aparecen **algunos comentarios muy largos** (outliers), especialmente en caracteres.\n",
    "- No se observa una diferencia clara que nos permita decir que un tipo de comentario sea m√°s largo que otro de forma sistem√°tica.\n",
    "\n",
    "#### ¬øQu√© concluimos?\n",
    "Aunque puede haber peque√±as diferencias, **la longitud del comentario no parece ser un buen indicador por s√≠ solo de si un comentario es t√≥xico o no**. Aun as√≠, es √∫til conocer estas caracter√≠sticas para posibles decisiones de preprocesamiento, como filtrar comentarios excesivamente largos o cortos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar comentarios t√≥xicos y no t√≥xicos\n",
    "toxic_comments = df[df[\"IsToxic\"] == True][\"Text\"]\n",
    "nontoxic_comments = df[df[\"IsToxic\"] == False][\"Text\"]\n",
    "\n",
    "# Juntarlos en dos grandes textos para analizarlos\n",
    "toxic_text = \" \".join(toxic_comments).lower()\n",
    "nontoxic_text = \" \".join(nontoxic_comments).lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### An√°lisis del contenido textual\n",
    "\n",
    "Para entender mejor las diferencias entre los comentarios t√≥xicos y no t√≥xicos, hemos separado los textos en dos grupos:\n",
    "\n",
    "- Comentarios etiquetados como **t√≥xicos**.\n",
    "- Comentarios etiquetados como **no t√≥xicos**.\n",
    "\n",
    "Hemos unido los comentarios de cada grupo en un √∫nico texto para poder analizar qu√© palabras aparecen con mayor frecuencia en cada uno. Este enfoque nos permitir√° visualizar patrones de lenguaje caracter√≠sticos, que luego pueden ser clave para entrenar un modelo predictivo.\n",
    "\n",
    "En los pr√≥ximos pasos generaremos:\n",
    "- Listados de palabras m√°s frecuentes.\n",
    "- Nubes de palabras (*wordclouds*).\n",
    "- N-gramas m√°s comunes (combinaciones t√≠picas de 2-3 palabras).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Funci√≥n para limpiar texto b√°sico (sin lematizar a√∫n)\n",
    "def limpiar_texto(texto):\n",
    "    texto = re.sub(r\"[^\\w\\s]\", \"\", texto)  # quitar signos de puntuaci√≥n\n",
    "    texto = texto.lower()  # pasar a min√∫sculas\n",
    "    return texto\n",
    "\n",
    "# Aplicar limpieza y dividir en palabras\n",
    "palabras_toxicas = limpiar_texto(toxic_text).split()\n",
    "palabras_no_toxicas = limpiar_texto(nontoxic_text).split()\n",
    "\n",
    "# Contar palabras m√°s comunes\n",
    "frecuentes_toxicas = Counter(palabras_toxicas).most_common(10)\n",
    "frecuentes_no_toxicas = Counter(palabras_no_toxicas).most_common(10)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"üî¥ Palabras m√°s frecuentes en comentarios t√≥xicos:\")\n",
    "for palabra, freq in frecuentes_toxicas:\n",
    "    print(f\"{palabra}: {freq}\")\n",
    "\n",
    "print(\"\\nüü¢ Palabras m√°s frecuentes en comentarios no t√≥xicos:\")\n",
    "for palabra, freq in frecuentes_no_toxicas:\n",
    "    print(f\"{palabra}: {freq}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Lista de stopwords en ingl√©s\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Funci√≥n mejorada que filtra stopwords\n",
    "def limpiar_y_filtrar(texto):\n",
    "    texto = re.sub(r\"[^\\w\\s]\", \"\", texto.lower())\n",
    "    palabras = texto.split()\n",
    "    return [p for p in palabras if p not in stop_words]\n",
    "\n",
    "# Aplicar funci√≥n mejorada\n",
    "palabras_toxicas_filtradas = limpiar_y_filtrar(toxic_text)\n",
    "palabras_no_toxicas_filtradas = limpiar_y_filtrar(nontoxic_text)\n",
    "\n",
    "# Contar palabras m√°s frecuentes (filtradas)\n",
    "frecuentes_toxicas_filtradas = Counter(palabras_toxicas_filtradas).most_common(10)\n",
    "frecuentes_no_toxicas_filtradas = Counter(palabras_no_toxicas_filtradas).most_common(10)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"üî¥ Palabras m√°s frecuentes (t√≥xicos, sin stopwords):\")\n",
    "for palabra, freq in frecuentes_toxicas_filtradas:\n",
    "    print(f\"{palabra}: {freq}\")\n",
    "\n",
    "print(\"\\nüü¢ Palabras m√°s frecuentes (no t√≥xicos, sin stopwords):\")\n",
    "for palabra, freq in frecuentes_no_toxicas_filtradas:\n",
    "    print(f\"{palabra}: {freq}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### An√°lisis con eliminaci√≥n de stopwords\n",
    "\n",
    "Las palabras m√°s frecuentes que observamos inicialmente eran muy comunes y poco informativas. Por ello, hemos repetido el an√°lisis **eliminando las stopwords**, es decir, palabras muy frecuentes en ingl√©s que no aportan significado real (como \"the\", \"and\", \"is\", etc.).\n",
    "\n",
    "Esta limpieza **no forma a√∫n parte del preprocesamiento oficial**, pero se introduce aqu√≠ como una forma de enriquecer el EDA y tomar decisiones m√°s informadas.\n",
    "\n",
    "Ahora los resultados empiezan a revelar **patrones de contenido m√°s relevantes** para entender qu√© vocabulario podr√≠a distinguir los comentarios t√≥xicos de los no t√≥xicos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Unir las palabras ya filtradas\n",
    "texto_toxico_filtrado = \" \".join(palabras_toxicas_filtradas)\n",
    "texto_nontoxico_filtrado = \" \".join(palabras_no_toxicas_filtradas)\n",
    "\n",
    "# Crear las nubes\n",
    "wc_toxico = WordCloud(width=800, height=400, background_color=\"white\").generate(texto_toxico_filtrado)\n",
    "wc_nontoxico = WordCloud(width=800, height=400, background_color=\"white\").generate(texto_nontoxico_filtrado)\n",
    "\n",
    "# Mostrar\n",
    "fig, axs = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "axs[0].imshow(wc_toxico, interpolation=\"bilinear\")\n",
    "axs[0].axis(\"off\")\n",
    "axs[0].set_title(\"üî¥ Comentarios t√≥xicos (sin stopwords)\")\n",
    "\n",
    "axs[1].imshow(wc_nontoxico, interpolation=\"bilinear\")\n",
    "axs[1].axis(\"off\")\n",
    "axs[1].set_title(\"üü¢ Comentarios no t√≥xicos (sin stopwords)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### WordClouds sin palabras vac√≠as (stopwords)\n",
    "\n",
    "Las siguientes nubes de palabras muestran los t√©rminos m√°s repetidos en los comentarios **t√≥xicos** y **no t√≥xicos**, tras eliminar las palabras vac√≠as t√≠picas del ingl√©s (como ‚Äúthe‚Äù, ‚Äúand‚Äù, ‚Äúis‚Äù‚Ä¶).\n",
    "\n",
    "#### ¬øQu√© observamos?\n",
    "- En los comentarios t√≥xicos aparecen con m√°s frecuencia palabras como **‚Äúfuck‚Äù**, lo que indica un tono agresivo.\n",
    "- Tambi√©n se observan t√©rminos raciales y relacionados con el orden p√∫blico (**black, white, police**) en ambos grupos, lo que sugiere que el contexto es similar, pero el uso del lenguaje es lo que cambia.\n",
    "\n",
    "Esta visualizaci√≥n permite a cualquier lector, incluso sin formaci√≥n t√©cnica, entender mejor el tipo de lenguaje que caracteriza cada grupo de comentarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Utilizamos los textos ya filtrados de stopwords\n",
    "def mostrar_ngrams(textos, n=2, top=10):\n",
    "    vectorizer = CountVectorizer(ngram_range=(n, n))\n",
    "    X = vectorizer.fit_transform(textos)\n",
    "    suma = X.sum(axis=0)\n",
    "    freqs = [(word, suma[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "    freqs_sorted = sorted(freqs, key=lambda x: x[1], reverse=True)\n",
    "    return freqs_sorted[:top]\n",
    "\n",
    "# Crear listas de texto plano por grupo\n",
    "comentarios_toxicos = df[df[\"IsToxic\"] == True][\"Text\"].tolist()\n",
    "comentarios_no_toxicos = df[df[\"IsToxic\"] == False][\"Text\"].tolist()\n",
    "\n",
    "# Obtener n-gramas\n",
    "top_bigrams_toxicos = mostrar_ngrams(comentarios_toxicos, n=2)\n",
    "top_bigrams_nontoxicos = mostrar_ngrams(comentarios_no_toxicos, n=2)\n",
    "\n",
    "top_trigrams_toxicos = mostrar_ngrams(comentarios_toxicos, n=3)\n",
    "top_trigrams_nontoxicos = mostrar_ngrams(comentarios_no_toxicos, n=3)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"üî¥ Bigrams m√°s comunes en comentarios t√≥xicos:\")\n",
    "for frase, freq in top_bigrams_toxicos:\n",
    "    print(f\"{frase}: {freq}\")\n",
    "\n",
    "print(\"\\nüü¢ Bigrams m√°s comunes en comentarios no t√≥xicos:\")\n",
    "for frase, freq in top_bigrams_nontoxicos:\n",
    "    print(f\"{frase}: {freq}\")\n",
    "\n",
    "print(\"\\nüî¥ Trigrams m√°s comunes en comentarios t√≥xicos:\")\n",
    "for frase, freq in top_trigrams_toxicos:\n",
    "    print(f\"{frase}: {freq}\")\n",
    "\n",
    "print(\"\\nüü¢ Trigrams m√°s comunes en comentarios no t√≥xicos:\")\n",
    "for frase, freq in top_trigrams_nontoxicos:\n",
    "    print(f\"{frase}: {freq}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### üîó An√°lisis de n-gramas (bigrams y trigrams)\n",
    "\n",
    "Los **n-gramas** son combinaciones de palabras consecutivas que nos permiten identificar expresiones t√≠picas o patrones de lenguaje. Son especialmente √∫tiles en tareas como detecci√≥n de discurso de odio, ya que muchas veces el contenido ofensivo se transmite en frases cortas y recurrentes.\n",
    "\n",
    "#### ¬øQu√© buscamos aqu√≠?\n",
    "- Expresiones como ‚Äúgo back‚Äù, ‚Äúshut up‚Äù, ‚Äúyou people‚Äù pueden tener fuerte carga ofensiva.\n",
    "- Otras como ‚Äúthank you‚Äù o ‚Äúgreat video‚Äù pueden caracterizar comentarios positivos o neutrales.\n",
    "\n",
    "Este an√°lisis nos acerca a una comprensi√≥n m√°s contextualizada del lenguaje usado, lo que ser√° de gran valor a la hora de entrenar modelos de clasificaci√≥n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extraer trigramas y frecuencias por separado\n",
    "trigrams_tox, freqs_tox = zip(*top_trigrams_toxicos)\n",
    "trigrams_notox, freqs_notox = zip(*top_trigrams_nontoxicos)\n",
    "\n",
    "# Crear gr√°fico de barras horizontales\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Trigramas t√≥xicos\n",
    "axes[0].barh(trigrams_tox[::-1], freqs_tox[::-1], color=\"crimson\")\n",
    "axes[0].set_title(\"üî¥ Trigramas m√°s comunes en comentarios t√≥xicos\")\n",
    "axes[0].set_xlabel(\"Frecuencia\")\n",
    "axes[0].tick_params(axis='y', labelsize=10)\n",
    "\n",
    "# Trigramas no t√≥xicos\n",
    "axes[1].barh(trigrams_notox[::-1], freqs_notox[::-1], color=\"seagreen\")\n",
    "axes[1].set_title(\"üü¢ Trigramas m√°s comunes en comentarios no t√≥xicos\")\n",
    "axes[1].set_xlabel(\"Frecuencia\")\n",
    "axes[1].tick_params(axis='y', labelsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Visualizaci√≥n de trigramas m√°s frecuentes\n",
    "\n",
    "Los trigramas (combinaciones de tres palabras consecutivas) nos permiten observar expresiones completas y t√≠picas de cada tipo de comentario.\n",
    "\n",
    "#### üî¥ En los comentarios t√≥xicos:\n",
    "- Aparecen insultos expl√≠citos como **‚Äúpiece of shit‚Äù**.\n",
    "- Expresiones violentas como **‚Äúrun them over‚Äù** o **‚Äúshoot to apprehend‚Äù**.\n",
    "- T√©rminos relacionados con movimientos sociales cargados emocionalmente: **‚Äúblack lives matter‚Äù**.\n",
    "\n",
    "#### üü¢ En los comentarios no t√≥xicos:\n",
    "- Predominan expresiones de agradecimiento (**‚Äúthank you for‚Äù**) o frases explicativas/descriptivas.\n",
    "- Se nota un tono m√°s **objetivo, civil o anal√≠tico**.\n",
    "\n",
    "Este contraste deja ver claramente c√≥mo cambia la intenci√≥n del lenguaje entre ambos grupos y valida el uso de trigramas como posibles caracter√≠sticas valiosas para entrenar el modelo de detecci√≥n de toxicidad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de etiquetas del dataset\n",
    "etiquetas = [\n",
    "    \"IsToxic\", \"IsAbusive\", \"IsThreat\", \"IsProvocative\", \"IsObscene\",\n",
    "    \"IsHatespeech\", \"IsRacist\", \"IsNationalist\", \"IsSexist\", \"IsHomophobic\",\n",
    "    \"IsReligiousHate\", \"IsRadicalism\"\n",
    "]\n",
    "\n",
    "# Calcular la correlaci√≥n entre etiquetas\n",
    "correlaciones = df[etiquetas].corr()\n",
    "\n",
    "# Calcular cu√°ntas etiquetas activas hay por comentario\n",
    "df[\"TotalEtiquetas\"] = df[etiquetas].sum(axis=1)\n",
    "etiquetas_activas = df[\"TotalEtiquetas\"].value_counts().sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualizaci√≥n de correlaci√≥n entre etiquetas\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlaciones, annot=True, cmap=\"coolwarm\", fmt=\".2f\", vmin=-1, vmax=1)\n",
    "plt.title(\"üîó Correlaci√≥n entre etiquetas de toxicidad\")\n",
    "plt.show()\n",
    "\n",
    "# Visualizaci√≥n de n√∫mero de etiquetas activas por comentario\n",
    "etiquetas_activas.plot(kind=\"bar\", color=\"steelblue\", figsize=(8, 5))\n",
    "plt.title(\"üéØ N√∫mero de etiquetas activas por comentario\")\n",
    "plt.xlabel(\"Cantidad de etiquetas\")\n",
    "plt.ylabel(\"N√∫mero de comentarios\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis=\"y\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### An√°lisis cruzado de etiquetas\n",
    "\n",
    "Adem√°s del an√°lisis textual, es importante entender c√≥mo se comportan las diferentes **subcategor√≠as de toxicidad** que aparecen en el dataset (como `IsAbusive`, `IsThreat`, `IsRacist`, etc.).\n",
    "\n",
    "#### ¬øCu√°ntas etiquetas tiene cada comentario?\n",
    "\n",
    "- M√°s de la mitad de los comentarios **no presentan ninguna etiqueta activa**.\n",
    "- El resto puede tener m√∫ltiples etiquetas, lo que sugiere que **el problema podr√≠a ser tratado como multietiqueta** si quisi√©ramos predecir m√°s all√° de `IsToxic`.\n",
    "\n",
    "#### ¬øQu√© etiquetas est√°n relacionadas?\n",
    "\n",
    "- `IsToxic` tiene una fuerte correlaci√≥n con `IsAbusive` (**0.80**) y `IsProvocative`.\n",
    "- `IsHatespeech` y `IsRacist` tienen una **correlaci√≥n alt√≠sima** (**0.94**), lo que indica que suelen ir juntas.\n",
    "\n",
    "Este an√°lisis refuerza la idea de que la toxicidad **no es una dimensi√≥n √∫nica**, sino que puede tener varias expresiones combinadas. Si decidi√©ramos construir un modelo m√°s complejo en el futuro, podr√≠amos abordar este problema como una clasificaci√≥n **multietiqueta** en lugar de binaria.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
