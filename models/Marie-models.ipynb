{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "\n",
    "# üéØ Objetivo: Detectar toxicidad con datos aumentados\n",
    "\n",
    "## üìã Estrategia:\n",
    "### - Eliminar columnas desbalanceadas\n",
    "### - Aplicar Data Augmentation con traducci√≥n\n",
    "### - Preprocesar texto eficientemente\n",
    "### - Entrenar XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Procesamiento de texto\n",
    "import re\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, learning_curve, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, roc_auc_score \n",
    "import xgboost as xgb\n",
    "\n",
    "# Para augmentaci√≥n simple\n",
    "import random\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Persistencia\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuraci√≥n\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Cargar datos y an√°lisis inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('../data/youtoxic_english_1000.csv')\n",
    "print(f\"‚úÖ Dataset original: {df.shape[0]} filas, {df.shape[1]} columnas\")\n",
    "\n",
    "# Columnas de toxicidad\n",
    "columnas_toxicidad = ['IsAbusive', 'IsThreat', 'IsProvocative', 'IsObscene', \n",
    "                      'IsHatespeech', 'IsRacist', 'IsNationalist', 'IsSexist', \n",
    "                      'IsHomophobic', 'IsReligiousHate', 'IsRadicalism']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# 2. Identificar y eliminar columnas desbalanceadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîç Analizando balance de columnas...\")\n",
    "\n",
    "# Calcular balance\n",
    "balance = {}\n",
    "UMBRAL = 5.0  # 5% m√≠nimo\n",
    "\n",
    "for col in columnas_toxicidad:\n",
    "    porcentaje = (df[col].sum() / len(df)) * 100\n",
    "    balance[col] = porcentaje\n",
    "    estado = \"‚úÖ Mantener\" if porcentaje >= UMBRAL else \"‚ùå Eliminar\"\n",
    "    print(f\"{col:20} -> {porcentaje:5.1f}% {estado}\")\n",
    "\n",
    "# Seleccionar solo columnas balanceadas\n",
    "columnas_mantener = [col for col in columnas_toxicidad if balance[col] >= UMBRAL]\n",
    "columnas_eliminar = [col for col in columnas_toxicidad if balance[col] < UMBRAL]\n",
    "\n",
    "print(f\"\\nüìä Resumen:\")\n",
    "print(f\"   - Columnas a mantener: {len(columnas_mantener)}\")\n",
    "print(f\"   - Columnas a eliminar: {len(columnas_eliminar)}\")\n",
    "\n",
    "# Crear etiqueta binaria solo con columnas balanceadas\n",
    "df['toxic_binary'] = (df[columnas_mantener].sum(axis=1) > 0).astype(int)\n",
    "\n",
    "# Eliminar columnas desbalanceadas del dataset\n",
    "df = df.drop(columns=columnas_eliminar)\n",
    "\n",
    "print(f\"\\n‚úÖ Nueva distribuci√≥n de toxicidad:\")\n",
    "print(df['toxic_binary'].value_counts())\n",
    "print(f\"Porcentaje t√≥xico: {df['toxic_binary'].mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# 3. Preprocesamiento de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüßπ Preparando funciones de preprocesamiento...\")\n",
    "\n",
    "# Inicializar herramientas\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def limpiar_texto(texto):\n",
    "    \"\"\"Limpieza r√°pida y eficiente del texto\"\"\"\n",
    "    if pd.isna(texto):\n",
    "        return \"\"\n",
    "    \n",
    "    texto = str(texto).lower()\n",
    "    texto = re.sub(r'@\\w+|http\\S+|www\\S+', '', texto)  # URLs y menciones\n",
    "    texto = re.sub(r'[^a-zA-Z\\s]', '', texto)  # Solo letras\n",
    "    texto = ' '.join(texto.split())  # Espacios extras\n",
    "    \n",
    "    return texto\n",
    "\n",
    "def procesar_texto(texto):\n",
    "    \"\"\"Procesamiento completo con lemmatizaci√≥n\"\"\"\n",
    "    # Tokenizar\n",
    "    palabras = word_tokenize(texto)\n",
    "    \n",
    "    # Filtrar stopwords y palabras cortas\n",
    "    palabras = [lemmatizer.lemmatize(p) for p in palabras \n",
    "                if p not in stop_words and len(p) > 2]\n",
    "    \n",
    "    return ' '.join(palabras)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "# 4. Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüåê Configurando Data Augmentation...\")\n",
    "\n",
    "def augmentar_texto_avanzado(texto):\n",
    "    \"\"\"\n",
    "    Aumenta datos usando t√©cnicas que no requieren APIs externas.\n",
    "    Genera m√∫ltiples variaciones del texto original.\n",
    "    \"\"\"\n",
    "    variaciones = []\n",
    "    \n",
    "    # 1. Reemplazo de sin√≥nimos comunes\n",
    "    sinonimos = {\n",
    "        'hate': ['despise', 'loathe', 'detest'],\n",
    "        'stupid': ['dumb', 'idiotic', 'foolish', 'moronic'],\n",
    "        'bad': ['terrible', 'awful', 'horrible'],\n",
    "        'good': ['great', 'excellent', 'wonderful'],\n",
    "        'idiot': ['fool', 'moron', 'imbecile'],\n",
    "        'ugly': ['hideous', 'repulsive', 'disgusting'],\n",
    "        'fat': ['obese', 'overweight', 'heavy'],\n",
    "        'kill': ['murder', 'destroy', 'eliminate'],\n",
    "        'die': ['perish', 'expire', 'pass away'],\n",
    "        'trash': ['garbage', 'rubbish', 'waste'],\n",
    "        'suck': ['terrible', 'awful', 'horrible'],\n",
    "        'dumb': ['stupid', 'idiotic', 'brainless']\n",
    "    }\n",
    "    \n",
    "    # Variaci√≥n 1: Reemplazar sin√≥nimos\n",
    "    texto_sinonimos = texto.lower()\n",
    "    for palabra, alternativas in sinonimos.items():\n",
    "        if palabra in texto_sinonimos:\n",
    "            for alternativa in alternativas[:2]:  # Usar m√°ximo 2 sin√≥nimos\n",
    "                nuevo_texto = texto_sinonimos.replace(palabra, alternativa)\n",
    "                if nuevo_texto != texto_sinonimos:\n",
    "                    variaciones.append(nuevo_texto)\n",
    "    \n",
    "    # 2. Inserci√≥n de ruido controlado\n",
    "    palabras = texto.split()\n",
    "    if len(palabras) > 5:\n",
    "        # Variaci√≥n 2: Eliminar palabras no esenciales\n",
    "        palabras_importantes = [p for p in palabras if len(p) > 3]\n",
    "        if len(palabras_importantes) > 3:\n",
    "            variaciones.append(' '.join(palabras_importantes))\n",
    "        \n",
    "        # Variaci√≥n 3: Cambiar orden de frases\n",
    "        if '.' in texto or '!' in texto or '?' in texto:\n",
    "            import re\n",
    "            frases = re.split(r'[.!?]+', texto)\n",
    "            frases = [f.strip() for f in frases if f.strip()]\n",
    "            if len(frases) > 1:\n",
    "                random.shuffle(frases)\n",
    "                variaciones.append('. '.join(frases) + '.')\n",
    "    \n",
    "    # 3. Par√°frasis simple usando TextBlob (sin traducci√≥n)\n",
    "    try:\n",
    "        blob = TextBlob(texto)\n",
    "        # Cambiar tiempos verbales si es posible\n",
    "        if blob.tags:\n",
    "            texto_modificado = str(blob)\n",
    "            # Intercambiar algunas palabras comunes\n",
    "            intercambios = [\n",
    "                ('you are', \"you're\"), (\"you're\", 'you are'),\n",
    "                ('i am', \"i'm\"), (\"i'm\", 'i am'),\n",
    "                ('do not', \"don't\"), (\"don't\", 'do not'),\n",
    "                ('will not', \"won't\"), (\"won't\", 'will not')\n",
    "            ]\n",
    "            for original, reemplazo in intercambios:\n",
    "                if original in texto_modificado.lower():\n",
    "                    texto_modificado = texto_modificado.lower().replace(original, reemplazo)\n",
    "                    if texto_modificado != texto.lower():\n",
    "                        variaciones.append(texto_modificado)\n",
    "                        break\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # 4. Agregar/quitar puntuaci√≥n excesiva (com√∫n en comentarios t√≥xicos)\n",
    "    if any(c in texto for c in ['!', '?', '...']):\n",
    "        # Reducir puntuaci√≥n\n",
    "        texto_reducido = re.sub(r'[!]+', '!', texto)\n",
    "        texto_reducido = re.sub(r'[?]+', '?', texto_reducido)\n",
    "        texto_reducido = re.sub(r'\\.{3,}', '...', texto_reducido)\n",
    "        if texto_reducido != texto:\n",
    "            variaciones.append(texto_reducido)\n",
    "        \n",
    "        # Aumentar puntuaci√≥n (para comentarios t√≥xicos)\n",
    "        texto_aumentado = texto.replace('!', '!!!')\n",
    "        texto_aumentado = texto_aumentado.replace('?', '???')\n",
    "        if texto_aumentado != texto:\n",
    "            variaciones.append(texto_aumentado)\n",
    "    \n",
    "    # Eliminar duplicados y retornar\n",
    "    return list(set(variaciones))[:3]  # M√°ximo 3 variaciones por texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# 5. Aplicar augmentation a comentarios t√≥xicos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîÑ Aplicando Data Augmentation a comentarios t√≥xicos...\")\n",
    "\n",
    "# Separar comentarios t√≥xicos para aumentar\n",
    "df_toxicos = df[df['toxic_binary'] == 1].copy()\n",
    "df_no_toxicos = df[df['toxic_binary'] == 0].copy()\n",
    "\n",
    "print(f\"Comentarios t√≥xicos originales: {len(df_toxicos)}\")\n",
    "print(f\"Comentarios no t√≥xicos: {len(df_no_toxicos)}\")\n",
    "\n",
    "# Aplicar augmentation\n",
    "comentarios_aumentados = []\n",
    "etiquetas_aumentadas = []\n",
    "contador = 0\n",
    "\n",
    "for idx, row in df_toxicos.iterrows():\n",
    "    texto_limpio = limpiar_texto(row['Text'])\n",
    "    \n",
    "    # Generar variaciones\n",
    "    variaciones = augmentar_texto_avanzado(texto_limpio)\n",
    "    \n",
    "    for variacion in variaciones:\n",
    "        if variacion and len(variacion) > 10:  # Solo agregar si tiene contenido\n",
    "            comentarios_aumentados.append(variacion)\n",
    "            etiquetas_aumentadas.append(1)\n",
    "            contador += 1\n",
    "    \n",
    "    # Mostrar progreso\n",
    "    if idx % 50 == 0:\n",
    "        print(f\"   Procesados: {idx}/{len(df_toxicos)} - Generados: {contador} nuevos\")\n",
    "\n",
    "# Crear DataFrame con datos aumentados\n",
    "df_aumentados = pd.DataFrame({\n",
    "    'Text': comentarios_aumentados,\n",
    "    'toxic_binary': etiquetas_aumentadas\n",
    "})\n",
    "\n",
    "print(f\"\\n‚úÖ Comentarios t√≥xicos generados: {len(df_aumentados)}\")\n",
    "\n",
    "# Combinar datasets\n",
    "df_final = pd.concat([\n",
    "    df[['Text', 'toxic_binary']],\n",
    "    df_aumentados\n",
    "], ignore_index=True)\n",
    "\n",
    "# Mezclar aleatoriamente\n",
    "df_final = df_final.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nüìä Dataset final:\")\n",
    "print(f\"   - Total comentarios: {len(df_final)}\")\n",
    "print(f\"   - T√≥xicos: {df_final['toxic_binary'].sum()}\")\n",
    "print(f\"   - No t√≥xicos: {len(df_final) - df_final['toxic_binary'].sum()}\")\n",
    "print(f\"   - Balance: {df_final['toxic_binary'].mean()*100:.1f}% t√≥xicos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "# 6. Preprocesar el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n‚è≥ Preprocesando todos los textos...\")\n",
    "\n",
    "# Aplicar limpieza y procesamiento\n",
    "df_final['texto_limpio'] = df_final['Text'].apply(limpiar_texto)\n",
    "df_final['texto_procesado'] = df_final['texto_limpio'].apply(procesar_texto)\n",
    "\n",
    "# Eliminar filas vac√≠as\n",
    "df_final = df_final[df_final['texto_procesado'].str.len() > 0]\n",
    "\n",
    "print(f\"‚úÖ Textos procesados: {len(df_final)}\")\n",
    "\n",
    "# Guardar dataset procesado\n",
    "df_final.to_csv('../data/dataset_toxicidad_aumentado.csv', index=False)\n",
    "print(\"üíæ Dataset guardado como: ../data/dataset_toxicidad_aumentado.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "# 7. Visualizaci√≥n de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Balance de clases\n",
    "df_final['toxic_binary'].value_counts().plot(kind='bar', ax=axes[0], \n",
    "                                            color=['lightgreen', 'salmon'])\n",
    "axes[0].set_title('Distribuci√≥n de Clases (Con Augmentation)')\n",
    "axes[0].set_xlabel('Clase')\n",
    "axes[0].set_ylabel('Cantidad')\n",
    "axes[0].set_xticklabels(['No T√≥xico', 'T√≥xico'], rotation=0)\n",
    "\n",
    "# Longitud de comentarios\n",
    "df_final['longitud'] = df_final['texto_procesado'].str.split().str.len()\n",
    "df_final.boxplot(column='longitud', by='toxic_binary', ax=axes[1])\n",
    "axes[1].set_title('Longitud de Comentarios por Clase')\n",
    "axes[1].set_xlabel('T√≥xico')\n",
    "axes[1].set_ylabel('N√∫mero de palabras')\n",
    "plt.suptitle('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "# Wordclouds comparativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä WORDCLOUDS COMPARATIVOS\")\n",
    "\n",
    "# Separar textos por categor√≠a usando los datos procesados\n",
    "textos_toxicos = df_final[df_final['toxic_binary'] == 1]['texto_procesado']\n",
    "textos_no_toxicos = df_final[df_final['toxic_binary'] == 0]['texto_procesado']\n",
    "\n",
    "print(f\"   ‚Ä¢ Comentarios t√≥xicos para WordCloud: {len(textos_toxicos)}\")\n",
    "print(f\"   ‚Ä¢ Comentarios no t√≥xicos para WordCloud: {len(textos_no_toxicos)}\")\n",
    "\n",
    "# Combinar textos por categor√≠a\n",
    "texto_toxico_combinado = ' '.join(textos_toxicos.dropna())\n",
    "texto_no_toxico_combinado = ' '.join(textos_no_toxicos.dropna())\n",
    "\n",
    "print(f\"   ‚Ä¢ Palabras en corpus t√≥xico: {len(texto_toxico_combinado.split())}\")\n",
    "print(f\"   ‚Ä¢ Palabras en corpus no t√≥xico: {len(texto_no_toxico_combinado.split())}\")\n",
    "\n",
    "# Verificar que tenemos suficiente texto\n",
    "if len(texto_toxico_combinado.split()) < 10:\n",
    "    print(\"‚ö†Ô∏è Advertencia: Poco texto t√≥xico disponible para WordCloud\")\n",
    "if len(texto_no_toxico_combinado.split()) < 10:\n",
    "    print(\"‚ö†Ô∏è Advertencia: Poco texto no t√≥xico disponible para WordCloud\")\n",
    "\n",
    "# Generar WordClouds\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Configuraci√≥n com√∫n para ambos WordClouds\n",
    "wordcloud_config = {\n",
    "    'width': 800,\n",
    "    'height': 400,\n",
    "    'background_color': 'white',\n",
    "    'max_words': 100,\n",
    "    'relative_scaling': 0.5,\n",
    "    'stopwords': stop_words,  # Usar las mismas stopwords del preprocesamiento\n",
    "    'collocation_threshold': 10\n",
    "}\n",
    "\n",
    "# WordCloud para comentarios t√≥xicos\n",
    "if len(texto_toxico_combinado.strip()) > 0:\n",
    "    wordcloud_toxico = WordCloud(\n",
    "        **wordcloud_config,\n",
    "        colormap='Reds'\n",
    "    ).generate(texto_toxico_combinado)\n",
    "    \n",
    "    axes[0].imshow(wordcloud_toxico, interpolation='bilinear')\n",
    "    axes[0].set_title('WordCloud - Comentarios T√ìXICOS', fontweight='bold', fontsize=16, color='darkred')\n",
    "    axes[0].axis('off')\n",
    "else:\n",
    "    axes[0].text(0.5, 0.5, 'No hay suficiente\\ntexto t√≥xico', \n",
    "                ha='center', va='center', transform=axes[0].transAxes, fontsize=16)\n",
    "    axes[0].set_title('WordCloud - Comentarios T√ìXICOS', fontweight='bold', fontsize=16, color='darkred')\n",
    "\n",
    "# WordCloud para comentarios no t√≥xicos\n",
    "if len(texto_no_toxico_combinado.strip()) > 0:\n",
    "    wordcloud_no_toxico = WordCloud(\n",
    "        **wordcloud_config,\n",
    "        colormap='Greens'\n",
    "    ).generate(texto_no_toxico_combinado)\n",
    "    \n",
    "    axes[1].imshow(wordcloud_no_toxico, interpolation='bilinear')\n",
    "    axes[1].set_title('WordCloud - Comentarios NO T√ìXICOS', fontweight='bold', fontsize=16, color='darkgreen')\n",
    "    axes[1].axis('off')\n",
    "else:\n",
    "    axes[1].text(0.5, 0.5, 'No hay suficiente\\ntexto no t√≥xico', \n",
    "                ha='center', va='center', transform=axes[1].transAxes, fontsize=16)\n",
    "    axes[1].set_title('WordCloud - Comentarios NO T√ìXICOS', fontweight='bold', fontsize=16, color='darkgreen')\n",
    "\n",
    "plt.suptitle('An√°lisis Visual del Vocabulario por Categor√≠a', fontsize=20, fontweight='bold', y=0.95)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "# 8. Preparaci√≥n para machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüéØ Preparando datos para entrenamiento...\")\n",
    "\n",
    "# Features y target\n",
    "X = df_final['texto_procesado']\n",
    "y = df_final['toxic_binary']\n",
    "\n",
    "# Divisi√≥n estratificada\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"üìä Divisi√≥n de datos:\")\n",
    "print(f\"   - Entrenamiento: {len(X_train)} ({y_train.mean()*100:.1f}% t√≥xicos)\")\n",
    "print(f\"   - Prueba: {len(X_test)} ({y_test.mean()*100:.1f}% t√≥xicos)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "# 9. Vectorizaci√≥n optimizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüî¢ Vectorizando con TF-IDF...\")\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=2000,      # M√°s features por m√°s datos\n",
    "    ngram_range=(1, 3),     # Incluir trigramas\n",
    "    min_df=2,               # M√≠nima frecuencia\n",
    "    max_df=0.95,            # M√°xima frecuencia\n",
    "    sublinear_tf=True,      # Escalado logar√≠tmico\n",
    "    use_idf=True,           # IDF para ponderar importancia\n",
    ")\n",
    "\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"‚úÖ Forma de datos vectorizados: {X_train_vec.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "# 10. Entrenamiento de XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüöÄ Entrenando XGBoost optimizado...\")\n",
    "\n",
    "# Calcular peso de clases para balanceo\n",
    "scale_pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1])\n",
    "\n",
    "# Modelo XGBoost con hiperpar√°metros optimizados\n",
    "modelo = xgb.XGBClassifier(\n",
    "    # Par√°metros b√°sicos\n",
    "    n_estimators=300,           # N√∫mero de √°rboles\n",
    "    max_depth=6,                # Profundidad m√°xima\n",
    "    learning_rate=0.1,          # Tasa de aprendizaje\n",
    "    \n",
    "    # Control de overfitting\n",
    "    subsample=0.8,              # Submuestreo de filas\n",
    "    colsample_bytree=0.8,       # Submuestreo de columnas\n",
    "    reg_alpha=0.1,              # Regularizaci√≥n L1\n",
    "    reg_lambda=1.0,             # Regularizaci√≥n L2\n",
    "    \n",
    "    # Balanceo de clases\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    \n",
    "    # Otros par√°metros\n",
    "    objective='binary:logistic',\n",
    "    eval_metric=['error', 'logloss'],  # M√©tricas de evaluaci√≥n\n",
    "    use_label_encoder=False,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,                  # Usar todos los cores\n",
    "    early_stopping_rounds=20    # Early stopping\n",
    ")\n",
    "\n",
    "# Entrenar con conjunto de validaci√≥n\n",
    "eval_set = [(X_train_vec, y_train), (X_test_vec, y_test)]\n",
    "modelo.fit(\n",
    "    X_train_vec, y_train,\n",
    "    eval_set=eval_set,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Obtener informaci√≥n del entrenamiento\n",
    "resultado_entrenamiento = modelo.evals_result()\n",
    "if resultado_entrenamiento:\n",
    "    # Obtener el mejor score de la validaci√≥n\n",
    "    val_scores = resultado_entrenamiento['validation_1']['logloss']\n",
    "    mejor_iteracion = np.argmin(val_scores)\n",
    "    mejor_score = val_scores[mejor_iteracion]\n",
    "    print(f\"‚úÖ Mejor iteraci√≥n: {mejor_iteracion + 1}\")\n",
    "    print(f\"‚úÖ Mejor score (logloss): {mejor_score:.4f}\")\n",
    "\n",
    "# Validaci√≥n cruzada con modelo sin early stopping\n",
    "print(\"\\nüìà Realizando validaci√≥n cruzada...\")\n",
    "modelo_cv = xgb.XGBClassifier(\n",
    "    n_estimators=100,  # Menos √°rboles para CV r√°pida\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=1.0,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    objective='binary:logistic',\n",
    "    use_label_encoder=False,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "scores_cv = cross_val_score(modelo_cv, X_train_vec, y_train, cv=5, scoring='f1')\n",
    "print(f\"   - F1-Scores: {[f'{s:.3f}' for s in scores_cv]}\")\n",
    "print(f\"   - Media: {scores_cv.mean():.3f} (+/- {scores_cv.std() * 2:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "# 11. Evaluaci√≥n detallada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä EVALUACI√ìN EN CONJUNTO DE PRUEBA:\")\n",
    "\n",
    "# Predicciones\n",
    "y_pred = modelo.predict(X_test_vec)\n",
    "y_pred_proba = modelo.predict_proba(X_test_vec)[:, 1]\n",
    "\n",
    "# M√©tricas\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nüéØ M√©tricas principales:\")\n",
    "print(f\"   - Accuracy: {accuracy:.3f}\")\n",
    "print(f\"   - F1-Score: {f1:.3f}\")\n",
    "\n",
    "# Reporte completo\n",
    "print(\"\\nüìã Reporte de clasificaci√≥n:\")\n",
    "print(classification_report(y_test, y_pred, \n",
    "                          target_names=['No T√≥xico', 'T√≥xico'],\n",
    "                          digits=3))\n",
    "\n",
    "# Matriz de confusi√≥n\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['No T√≥xico', 'T√≥xico'],\n",
    "            yticklabels=['No T√≥xico', 'T√≥xico'])\n",
    "plt.title('Matriz de Confusi√≥n - XGBoost')\n",
    "plt.ylabel('Valor Real')\n",
    "plt.xlabel('Predicci√≥n')\n",
    "\n",
    "# Agregar m√©tricas en el t√≠tulo\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "plt.text(0.5, -0.1, f'Precisi√≥n: {precision:.3f} | Recall: {recall:.3f} | F1: {f1:.3f}', \n",
    "         ha='center', transform=plt.gca().transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "# 12. An√°lisis de importancia de features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüî§ AN√ÅLISIS DE IMPORTANCIA DE FEATURES:\")\n",
    "\n",
    "# Obtener importancia de features de XGBoost\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "importancias = modelo.feature_importances_\n",
    "\n",
    "# Crear DataFrame de importancias\n",
    "df_importancia = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importancias\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Top 20 features m√°s importantes\n",
    "print(\"\\nüèÜ Top 20 features m√°s importantes:\")\n",
    "for idx, row in df_importancia.head(20).iterrows():\n",
    "    print(f\"   '{row['feature']}': {row['importance']:.4f}\")\n",
    "\n",
    "# Visualizar importancia de features\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_features = df_importancia.head(30)\n",
    "plt.barh(range(len(top_features)), top_features['importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Importancia')\n",
    "plt.title('Top 30 Features M√°s Importantes - XGBoost')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# An√°lisis adicional: Gain vs Cover\n",
    "if hasattr(modelo, 'get_booster'):\n",
    "    print(\"\\nüìä An√°lisis detallado de importancia:\")\n",
    "    importance_types = ['weight', 'gain', 'cover']\n",
    "    \n",
    "    for imp_type in importance_types:\n",
    "        importances_dict = modelo.get_booster().get_score(importance_type=imp_type)\n",
    "        if importances_dict:\n",
    "            print(f\"\\n{imp_type.upper()}:\")\n",
    "            sorted_imp = sorted(importances_dict.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "            for feat, score in sorted_imp:\n",
    "                if feat.startswith('f'):\n",
    "                    feat_idx = int(feat[1:])\n",
    "                    feat_name = feature_names[feat_idx]\n",
    "                    print(f\"   '{feat_name}': {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "# 13. Funci√≥n de predicci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predecir_toxicidad(texto, modelo=modelo, vectorizer=vectorizer):\n",
    "    \"\"\"\n",
    "    Predice si un comentario es t√≥xico.\n",
    "    \n",
    "    Retorna:\n",
    "    - etiqueta: 'T√ìXICO' o 'NO T√ìXICO'\n",
    "    - confianza: probabilidad de la predicci√≥n\n",
    "    \"\"\"\n",
    "    # Preprocesar\n",
    "    texto_limpio = limpiar_texto(texto)\n",
    "    texto_procesado = procesar_texto(texto_limpio)\n",
    "    \n",
    "    # Vectorizar\n",
    "    texto_vec = vectorizer.transform([texto_procesado])\n",
    "    \n",
    "    # Predecir\n",
    "    prediccion = modelo.predict(texto_vec)[0]\n",
    "    probabilidad = modelo.predict_proba(texto_vec)[0, 1]\n",
    "    \n",
    "    etiqueta = \"T√ìXICO ‚ö†Ô∏è\" if prediccion == 1 else \"NO T√ìXICO ‚úÖ\"\n",
    "    confianza = probabilidad if prediccion == 1 else (1 - probabilidad)\n",
    "    \n",
    "    return etiqueta, confianza\n",
    "\n",
    "# Probar con ejemplos\n",
    "print(\"\\nüß™ PRUEBAS CON COMENTARIOS NUEVOS:\")\n",
    "\n",
    "ejemplos = [\n",
    "    \"Great video, thanks for sharing!\",\n",
    "    \"You're so stupid and ignorant\",\n",
    "    \"I disagree with your opinion\",\n",
    "    \"This is garbage content, delete it\",\n",
    "    \"Interesting perspective, never thought about it that way\"\n",
    "]\n",
    "\n",
    "for comentario in ejemplos:\n",
    "    etiqueta, confianza = predecir_toxicidad(comentario)\n",
    "    print(f\"\\nüìù '{comentario}'\")\n",
    "    print(f\"   ‚Üí {etiqueta} (Confianza: {confianza:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "# 14. An√°lisis de overfitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analizar_overfitting(modelo_xgb, X_train_vec, y_train, X_test_vec, y_test):\n",
    "    \"\"\"\n",
    "    An√°lisis completo de overfitting del modelo XGBoost\n",
    "    Adaptado para el notebook de detecci√≥n de toxicidad\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üîç AN√ÅLISIS DE OVERFITTING\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. M√âTRICAS COMPARATIVAS ENTRE CONJUNTOS\n",
    "    print(\"\\nüìä COMPARACI√ìN DE M√âTRICAS POR CONJUNTO:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    conjuntos_datos = {\n",
    "        'Entrenamiento': (X_train_vec, y_train),\n",
    "        'Prueba': (X_test_vec, y_test)\n",
    "    }\n",
    "    \n",
    "    comparacion_metricas = {}\n",
    "    \n",
    "    for nombre, (X, y) in conjuntos_datos.items():\n",
    "        y_pred = modelo_xgb.predict(X)\n",
    "        y_proba = modelo_xgb.predict_proba(X)[:, 1]\n",
    "        \n",
    "        metricas = {\n",
    "            'accuracy': accuracy_score(y, y_pred),\n",
    "            'precision': precision_score(y, y_pred),\n",
    "            'recall': recall_score(y, y_pred),\n",
    "            'f1': f1_score(y, y_pred),\n",
    "            'auc': roc_auc_score(y, y_proba)\n",
    "        }\n",
    "        \n",
    "        comparacion_metricas[nombre] = metricas\n",
    "        \n",
    "        print(f\"\\n{nombre}:\")\n",
    "        for metrica, valor in metricas.items():\n",
    "            print(f\"  {metrica.upper()}: {valor:.4f}\")\n",
    "    \n",
    "    # 2. DETECTAR OVERFITTING\n",
    "    print(f\"\\nüö® DETECCI√ìN DE OVERFITTING:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    train_f1 = comparacion_metricas['Entrenamiento']['f1']\n",
    "    test_f1 = comparacion_metricas['Prueba']['f1']\n",
    "    \n",
    "    # Diferencias\n",
    "    train_test_diff = train_f1 - test_f1\n",
    "    \n",
    "    print(f\"üìà F1 Train vs Test: {train_test_diff:.4f}\")\n",
    "    \n",
    "    # An√°lisis de overfitting\n",
    "    overfitting_detectado = False\n",
    "    \n",
    "    if train_test_diff > 0.08:  # M√°s de 8% de diferencia es preocupante\n",
    "        print(\"‚ùå OVERFITTING SEVERO detectado (Train >> Test)\")\n",
    "        overfitting_detectado = True\n",
    "    elif train_test_diff > 0.05:  # M√°s de 5% de diferencia\n",
    "        print(\"‚ö†Ô∏è  OVERFITTING MODERADO detectado (Train >> Test)\")\n",
    "        overfitting_detectado = True\n",
    "    elif train_test_diff > 0.02:  # Ligero overfitting\n",
    "        print(\"‚ö†Ô∏è  LIGERO OVERFITTING detectado\")\n",
    "        overfitting_detectado = True\n",
    "    \n",
    "    if not overfitting_detectado:\n",
    "        print(\"‚úÖ NO se detecta overfitting significativo\")\n",
    "        print(\"‚úÖ Modelo tiene buena generalizaci√≥n\")\n",
    "    \n",
    "    return comparacion_metricas\n",
    "\n",
    "def graficar_analisis_overfitting(modelo_xgb, X_train_vec, y_train, X_test_vec, y_test, comparacion_metricas):\n",
    "    \"\"\"\n",
    "    Visualizaciones para an√°lisis de overfitting\n",
    "    Adaptado para el modelo XGBoost del notebook\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nüìä GENERANDO VISUALIZACIONES...\")\n",
    "    \n",
    "    # Crear subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('An√°lisis de Overfitting - Modelo XGBoost Toxicidad', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Subplot 1: Curvas de aprendizaje\n",
    "    print(\"üìà Calculando curvas de aprendizaje...\")\n",
    "    \n",
    "    # Crear un modelo XGBoost sin early stopping para las curvas de aprendizaje\n",
    "    modelo_sin_early_stopping = xgb.XGBClassifier(\n",
    "        n_estimators=100,  # Menos √°rboles para ser m√°s r√°pido\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=1.0,\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='logloss',\n",
    "        use_label_encoder=False,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "        # NO incluir early_stopping_rounds aqu√≠\n",
    "    )\n",
    "    \n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        modelo_sin_early_stopping, X_train_vec, y_train,\n",
    "        cv=3, \n",
    "        train_sizes=np.linspace(0.3, 1.0, 6),  # Empezar con m√°s datos para evitar problemas\n",
    "        scoring='f1',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Calcular medias y desviaciones\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    val_mean = np.mean(val_scores, axis=1)\n",
    "    val_std = np.std(val_scores, axis=1)\n",
    "    \n",
    "    axes[0, 0].plot(train_sizes, train_mean, 'o-', color='blue', label='Entrenamiento', linewidth=2)\n",
    "    axes[0, 0].fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.2, color='blue')\n",
    "    \n",
    "    axes[0, 0].plot(train_sizes, val_mean, 'o-', color='red', label='Validaci√≥n Cruzada', linewidth=2)\n",
    "    axes[0, 0].fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.2, color='red')\n",
    "    \n",
    "    axes[0, 0].set_xlabel('Tama√±o del conjunto de entrenamiento')\n",
    "    axes[0, 0].set_ylabel('F1 Score')\n",
    "    axes[0, 0].set_title('Curvas de Aprendizaje', fontweight='bold')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Subplot 2: Comparaci√≥n de m√©tricas Train vs Test\n",
    "    conjuntos = ['Entrenamiento', 'Prueba']\n",
    "    f1_scores = [\n",
    "        comparacion_metricas['Entrenamiento']['f1'],\n",
    "        comparacion_metricas['Prueba']['f1']\n",
    "    ]\n",
    "    \n",
    "    colores = ['lightblue', 'lightcoral']\n",
    "    barras = axes[0, 1].bar(conjuntos, f1_scores, color=colores)\n",
    "    axes[0, 1].set_ylabel('F1 Score')\n",
    "    axes[0, 1].set_title('F1 Score: Entrenamiento vs Prueba', fontweight='bold')\n",
    "    axes[0, 1].set_ylim(0, 1)\n",
    "    \n",
    "    # A√±adir valores en las barras\n",
    "    for barra, score in zip(barras, f1_scores):\n",
    "        axes[0, 1].text(barra.get_x() + barra.get_width()/2, barra.get_height() + 0.01,\n",
    "                       f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # L√≠nea de referencia para mostrar la diferencia\n",
    "    diferencia = abs(f1_scores[0] - f1_scores[1])\n",
    "    axes[0, 1].text(0.5, 0.5, f'Diferencia: {diferencia:.3f}', \n",
    "                   transform=axes[0, 1].transAxes, ha='center',\n",
    "                   bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\", alpha=0.7))\n",
    "    \n",
    "    # Subplot 3: Distribuci√≥n de probabilidades por conjunto\n",
    "    y_proba_train = modelo_xgb.predict_proba(X_train_vec)[:, 1]\n",
    "    y_proba_test = modelo_xgb.predict_proba(X_test_vec)[:, 1]\n",
    "    \n",
    "    axes[1, 0].hist(y_proba_train, bins=30, alpha=0.7, label='Entrenamiento', color='blue', density=True)\n",
    "    axes[1, 0].hist(y_proba_test, bins=30, alpha=0.7, label='Prueba', color='red', density=True)\n",
    "    axes[1, 0].set_xlabel('Probabilidad de Toxicidad')\n",
    "    axes[1, 0].set_ylabel('Densidad')\n",
    "    axes[1, 0].set_title('Distribuci√≥n de Probabilidades Predichas', fontweight='bold')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Subplot 4: Todas las m√©tricas comparadas\n",
    "    metricas = ['Accuracy', 'Precision', 'Recall', 'F1', 'AUC']\n",
    "    train_metricas = [comparacion_metricas['Entrenamiento'][m.lower()] for m in metricas]\n",
    "    test_metricas = [comparacion_metricas['Prueba'][m.lower()] for m in metricas]\n",
    "    \n",
    "    x = np.arange(len(metricas))\n",
    "    ancho = 0.35\n",
    "    \n",
    "    axes[1, 1].bar(x - ancho/2, train_metricas, ancho, label='Entrenamiento', color='lightblue')\n",
    "    axes[1, 1].bar(x + ancho/2, test_metricas, ancho, label='Prueba', color='lightcoral')\n",
    "    \n",
    "    axes[1, 1].set_xlabel('M√©tricas')\n",
    "    axes[1, 1].set_ylabel('Score')\n",
    "    axes[1, 1].set_title('Comparaci√≥n Completa de M√©tricas', fontweight='bold')\n",
    "    axes[1, 1].set_xticks(x)\n",
    "    axes[1, 1].set_xticklabels(metricas, rotation=45)\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    axes[1, 1].set_ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def generar_reporte_overfitting(comparacion_metricas):\n",
    "    \"\"\"\n",
    "    Generar reporte final de overfitting\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nüìã REPORTE FINAL DE OVERFITTING\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    train_f1 = comparacion_metricas['Entrenamiento']['f1']\n",
    "    test_f1 = comparacion_metricas['Prueba']['f1']\n",
    "    \n",
    "    # An√°lisis detallado\n",
    "    print(f\"üìä AN√ÅLISIS DETALLADO:\")\n",
    "    print(f\"   ‚Ä¢ F1 Entrenamiento: {train_f1:.4f}\")\n",
    "    print(f\"   ‚Ä¢ F1 Prueba:        {test_f1:.4f}\")\n",
    "    \n",
    "    gap_train_test = train_f1 - test_f1\n",
    "    \n",
    "    print(f\"\\nüîç GAP DE RENDIMIENTO:\")\n",
    "    print(f\"   ‚Ä¢ Train-Test gap:   {gap_train_test:.4f}\")\n",
    "    \n",
    "    # Diagn√≥stico\n",
    "    print(f\"\\nü©∫ DIAGN√ìSTICO:\")\n",
    "    \n",
    "    if gap_train_test < 0.02:\n",
    "        print(\"   ‚úÖ EXCELENTE: Modelo muy bien generalizado\")\n",
    "        recomendacion = \"El modelo est√° listo para producci√≥n\"\n",
    "        color_estado = \"üü¢\"\n",
    "        \n",
    "    elif gap_train_test < 0.05:\n",
    "        print(\"   ‚úÖ BUENO: Ligero overfitting, pero aceptable\")\n",
    "        recomendacion = \"Modelo aceptable para producci√≥n con monitoreo\"\n",
    "        color_estado = \"üü°\"\n",
    "        \n",
    "    elif gap_train_test < 0.08:\n",
    "        print(\"   ‚ö†Ô∏è  MODERADO: Overfitting detectado\")\n",
    "        recomendacion = \"Considerar m√°s regularizaci√≥n o early stopping m√°s agresivo\"\n",
    "        color_estado = \"üü†\"\n",
    "        \n",
    "    else:\n",
    "        print(\"   ‚ùå SEVERO: Overfitting significativo\")\n",
    "        recomendacion = \"Necesario ajustar hiperpar√°metros o reentrenar\"\n",
    "        color_estado = \"üî¥\"\n",
    "    \n",
    "    print(f\"\\nüí° RECOMENDACI√ìN:\")\n",
    "    print(f\"   {recomendacion}\")\n",
    "    \n",
    "    # M√©tricas de generalizaci√≥n (no \"confianza\" para evitar confusi√≥n)\n",
    "    puntaje_generalizacion = max(0, 100 - (gap_train_test * 100 * 15))\n",
    "    print(f\"\\nüéØ CAPACIDAD DE GENERALIZACI√ìN: {puntaje_generalizacion:.1f}/100 {color_estado}\")\n",
    "    \n",
    "    if puntaje_generalizacion >= 85:\n",
    "        print(\"   üèÜ EXCELENTE generalizaci√≥n - Modelo muy robusto\")\n",
    "    elif puntaje_generalizacion >= 70:\n",
    "        print(\"   üëç BUENA generalizaci√≥n - Modelo confiable\")\n",
    "    elif puntaje_generalizacion >= 50:\n",
    "        print(\"   ‚ö†Ô∏è  GENERALIZACI√ìN MEDIA - Modelo aceptable con reservas\")\n",
    "    else:\n",
    "        print(\"   üëé GENERALIZACI√ìN BAJA - Hay overfitting, revisar modelo\")\n",
    "    \n",
    "    # An√°lisis adicional espec√≠fico para detecci√≥n de toxicidad\n",
    "    print(f\"\\nüéØ AN√ÅLISIS ESPEC√çFICO PARA TOXICIDAD:\")\n",
    "    \n",
    "    train_precision = comparacion_metricas['Entrenamiento']['precision']\n",
    "    test_precision = comparacion_metricas['Prueba']['precision']\n",
    "    precision_gap = train_precision - test_precision\n",
    "    \n",
    "    train_recall = comparacion_metricas['Entrenamiento']['recall']\n",
    "    test_recall = comparacion_metricas['Prueba']['recall']\n",
    "    recall_gap = train_recall - test_recall\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Gap Precision: {precision_gap:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Gap Recall:    {recall_gap:.4f}\")\n",
    "    \n",
    "    if precision_gap > 0.1:\n",
    "        print(\"   ‚ö†Ô∏è  Modelo podr√≠a estar generando muchos falsos positivos en producci√≥n\")\n",
    "    if recall_gap > 0.1:\n",
    "        print(\"   ‚ö†Ô∏è  Modelo podr√≠a estar perdiendo comentarios t√≥xicos en producci√≥n\")\n",
    "\n",
    "# EJECUTAR AN√ÅLISIS COMPLETO DE OVERFITTING\n",
    "# 1. Analizar overfitting con las m√©tricas\n",
    "comparacion_metricas = analizar_overfitting(\n",
    "    modelo, X_train_vec, y_train, X_test_vec, y_test\n",
    ")\n",
    "\n",
    "# 2. Generar visualizaciones\n",
    "graficar_analisis_overfitting(\n",
    "    modelo, X_train_vec, y_train, X_test_vec, y_test, comparacion_metricas\n",
    ")\n",
    "\n",
    "# 3. Generar reporte final\n",
    "generar_reporte_overfitting(comparacion_metricas)\n",
    "\n",
    "print(f\"\\n‚úÖ AN√ÅLISIS DE OVERFITTING COMPLETADO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# AN√ÅLISIS ADICIONAL: PREDICCIONES POR CONFIANZA\n",
    "print(f\"\\nüìà AN√ÅLISIS ADICIONAL: DISTRIBUCI√ìN DE CONFIANZA\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Analizar las predicciones por nivel de confianza\n",
    "y_proba_test = modelo.predict_proba(X_test_vec)[:, 1]\n",
    "\n",
    "# Categorizar predicciones por confianza\n",
    "alta_confianza = (y_proba_test >= 0.8) | (y_proba_test <= 0.2)\n",
    "media_confianza = ((y_proba_test >= 0.6) & (y_proba_test < 0.8)) | ((y_proba_test > 0.2) & (y_proba_test <= 0.4))\n",
    "baja_confianza = (y_proba_test > 0.4) & (y_proba_test < 0.6)\n",
    "\n",
    "print(f\"üìä Distribuci√≥n de confianza en predicciones de prueba:\")\n",
    "print(f\"   ‚Ä¢ Alta confianza (>80% o <20%):  {alta_confianza.sum():3d} ({alta_confianza.mean()*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Media confianza (60-80%, 20-40%): {media_confianza.sum():3d} ({media_confianza.mean()*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Baja confianza (40-60%):       {baja_confianza.sum():3d} ({baja_confianza.mean()*100:.1f}%)\")\n",
    "\n",
    "# Calcular accuracy por nivel de confianza\n",
    "if alta_confianza.sum() > 0:\n",
    "    acc_alta = accuracy_score(y_test[alta_confianza], (y_proba_test[alta_confianza] > 0.5))\n",
    "    print(f\"\\nüéØ Accuracy por nivel de confianza:\")\n",
    "    print(f\"   ‚Ä¢ Alta confianza: {acc_alta:.3f}\")\n",
    "\n",
    "if media_confianza.sum() > 0:\n",
    "    acc_media = accuracy_score(y_test[media_confianza], (y_proba_test[media_confianza] > 0.5))\n",
    "    print(f\"   ‚Ä¢ Media confianza: {acc_media:.3f}\")\n",
    "\n",
    "if baja_confianza.sum() > 0:\n",
    "    acc_baja = accuracy_score(y_test[baja_confianza], (y_proba_test[baja_confianza] > 0.5))\n",
    "    print(f\"   ‚Ä¢ Baja confianza: {acc_baja:.3f}\")\n",
    "\n",
    "print(f\"\\nüí° Interpretaci√≥n:\")\n",
    "if baja_confianza.mean() < 0.15:  # Menos del 15% de predicciones inciertas\n",
    "    print(\"   ‚úÖ Modelo hace predicciones con alta certeza individual\")\n",
    "    print(\"   üìä La mayor√≠a de predicciones son muy seguras (>80% o <20%)\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  Considerable n√∫mero de predicciones con baja certeza\")\n",
    "    print(\"   üìä Muchas predicciones est√°n en zona gris (40-60%)\")\n",
    "\n",
    "print(f\"\\nüèÅ AN√ÅLISIS COMPLETO FINALIZADO\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "# 15. Optimizaci√≥n con Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"Funci√≥n objetivo para optimizar hiperpar√°metros anti-overfitting\"\"\"\n",
    "    try:\n",
    "        scale_pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1])\n",
    "        \n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 8),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "            'subsample': trial.suggest_float('subsample', 0.7, 0.9),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 0.9),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 0.01, 5.0),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 0.01, 5.0),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 7),\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': ['error', 'logloss'],  \n",
    "            'use_label_encoder': False,  \n",
    "            'random_state': 42,\n",
    "            'n_jobs': 1,  # Usar 1 core para evitar conflictos en CV\n",
    "            'scale_pos_weight': scale_pos_weight,\n",
    "            'verbosity': 0\n",
    "        }\n",
    "        \n",
    "        # Modelo para validaci√≥n cruzada\n",
    "        modelo_cv = xgb.XGBClassifier(**params)\n",
    "        \n",
    "        # Validaci√≥n cruzada\n",
    "        cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)  \n",
    "        f1_scores = cross_val_score(\n",
    "            modelo_cv, X_train_vec, y_train, \n",
    "            cv=cv, scoring='f1', error_score='raise'\n",
    "        )\n",
    "        \n",
    "        modelo_test = xgb.XGBClassifier(**params)\n",
    "        modelo_test.fit(X_train_vec, y_train)\n",
    "        \n",
    "        train_f1 = f1_score(y_train, modelo_test.predict(X_train_vec))\n",
    "        test_f1 = f1_score(y_test, modelo_test.predict(X_test_vec))\n",
    "        overfitting_gap = abs(train_f1 - test_f1)\n",
    "        \n",
    "        train_pred_proba = modelo_test.predict_proba(X_train_vec)[:, 1]\n",
    "        test_pred_proba = modelo_test.predict_proba(X_test_vec)[:, 1]\n",
    "        \n",
    "        from sklearn.metrics import log_loss\n",
    "        train_logloss = log_loss(y_train, train_pred_proba)\n",
    "        test_logloss = log_loss(y_test, test_pred_proba)\n",
    "        logloss_gap = abs(train_logloss - test_logloss)\n",
    "        \n",
    "        # Guardar m√©tricas\n",
    "        trial.set_user_attr('cv_f1', f1_scores.mean())\n",
    "        trial.set_user_attr('cv_f1_std', f1_scores.std())\n",
    "        trial.set_user_attr('train_f1', train_f1)\n",
    "        trial.set_user_attr('test_f1', test_f1)\n",
    "        trial.set_user_attr('overfitting_gap', overfitting_gap)\n",
    "        trial.set_user_attr('train_logloss', train_logloss)\n",
    "        trial.set_user_attr('test_logloss', test_logloss)\n",
    "        trial.set_user_attr('logloss_gap', logloss_gap)\n",
    "        \n",
    "        # Optimiza F1 pero penaliza overfitting en ambas m√©tricas\n",
    "        penalty = (overfitting_gap * 1.0) + (logloss_gap * 0.5)\n",
    "        return f1_scores.mean() - penalty\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error en trial: {e}\")\n",
    "        return -1.0  # Score muy bajo para trials fallidos\n",
    "\n",
    "def optimizar_xgboost(X_train_vec, y_train, X_test_vec, y_test, n_trials=50):\n",
    "    \"\"\"Optimiza hiperpar√°metros usando Optuna\"\"\"\n",
    "    # Hacer variables globales para objective\n",
    "    globals().update({\n",
    "        'X_train_vec': X_train_vec, 'y_train': y_train,\n",
    "        'X_test_vec': X_test_vec, 'y_test': y_test\n",
    "    })\n",
    "    \n",
    "    # Verificar que los datos sean v√°lidos\n",
    "    print(f\"Datos de entrenamiento: {X_train_vec.shape}, {len(y_train)}\")\n",
    "    print(f\"Datos de prueba: {X_test_vec.shape}, {len(y_test)}\")\n",
    "    print(f\"Distribuci√≥n y_train: {np.bincount(y_train)}\")\n",
    "    \n",
    "    study = optuna.create_study(\n",
    "        direction='maximize', \n",
    "        sampler=optuna.samplers.TPESampler(seed=42)\n",
    "    )\n",
    "    \n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "    \n",
    "    # Filtrar trials exitosos\n",
    "    successful_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE and t.value > -0.5]\n",
    "    \n",
    "    if not successful_trials:\n",
    "        raise ValueError(\"No se completaron trials exitosos. Revisa tus datos.\")\n",
    "    \n",
    "    best_trial = max(successful_trials, key=lambda x: x.value)\n",
    "    \n",
    "    print(f\"\\nMejor score: {best_trial.value:.4f}\")\n",
    "    print(f\"CV F1: {best_trial.user_attrs['cv_f1']:.4f} (¬±{best_trial.user_attrs['cv_f1_std']:.4f})\")\n",
    "    print(f\"Train F1: {best_trial.user_attrs['train_f1']:.4f}\")\n",
    "    print(f\"Test F1: {best_trial.user_attrs['test_f1']:.4f}\")\n",
    "    print(f\"Gap overfitting F1: {best_trial.user_attrs['overfitting_gap']:.4f}\")\n",
    "    print(f\"Train LogLoss: {best_trial.user_attrs['train_logloss']:.4f}\")  # NUEVO\n",
    "    print(f\"Test LogLoss: {best_trial.user_attrs['test_logloss']:.4f}\")    # NUEVO\n",
    "    print(f\"Gap overfitting LogLoss: {best_trial.user_attrs['logloss_gap']:.4f}\")  # NUEVO\n",
    "    print(f\"Trials exitosos: {len(successful_trials)}/{len(study.trials)}\")\n",
    "    \n",
    "    return best_trial.params\n",
    "\n",
    "def entrenar_modelo_final(best_params, X_train_vec, y_train, X_test_vec, y_test):\n",
    "    \"\"\"Entrena modelo final con par√°metros optimizados - VERSI√ìN CORREGIDA\"\"\"\n",
    "    scale_pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1])\n",
    "    \n",
    "    final_params = {\n",
    "        **best_params,\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': ['error', 'logloss'],  \n",
    "        'use_label_encoder': False,           \n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1,\n",
    "        'scale_pos_weight': scale_pos_weight,\n",
    "        'verbosity': 0\n",
    "    }\n",
    "    \n",
    "    modelo = xgb.XGBClassifier(**final_params)\n",
    "    modelo.fit(X_train_vec, y_train)\n",
    "    \n",
    "    # Evaluaci√≥n final\n",
    "    train_pred = modelo.predict(X_train_vec)\n",
    "    test_pred = modelo.predict(X_test_vec)\n",
    "    \n",
    "    train_f1 = f1_score(y_train, train_pred)\n",
    "    test_f1 = f1_score(y_test, test_pred)\n",
    "    \n",
    "    # A√±adir evaluaci√≥n de logloss como en modelo original\n",
    "    train_pred_proba = modelo.predict_proba(X_train_vec)[:, 1]\n",
    "    test_pred_proba = modelo.predict_proba(X_test_vec)[:, 1]\n",
    "    \n",
    "    from sklearn.metrics import log_loss\n",
    "    train_logloss = log_loss(y_train, train_pred_proba)\n",
    "    test_logloss = log_loss(y_test, test_pred_proba)\n",
    "    \n",
    "    # Validaci√≥n cruzada final\n",
    "    cv_scores = cross_val_score(\n",
    "        xgb.XGBClassifier(**final_params), \n",
    "        X_train_vec, y_train, \n",
    "        cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=42), \n",
    "        scoring='f1'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Modelo final:\")\n",
    "    print(f\"Train F1: {train_f1:.4f}\")\n",
    "    print(f\"Test F1: {test_f1:.4f}\")\n",
    "    print(f\"CV F1: {cv_scores.mean():.3f} (¬±{cv_scores.std() * 2:.3f})\")\n",
    "    print(f\"Overfitting gap F1: {abs(train_f1 - test_f1):.4f}\")\n",
    "    print(f\"Train LogLoss: {train_logloss:.4f}\")     \n",
    "    print(f\"Test LogLoss: {test_logloss:.4f}\")      \n",
    "    print(f\"Overfitting gap LogLoss: {abs(train_logloss - test_logloss):.4f}\") \n",
    "    \n",
    "    return modelo, final_params\n",
    "\n",
    "def optimizar_y_entrenar(X_train_vec, y_train, X_test_vec, y_test, n_trials=50):\n",
    "    \"\"\"Proceso completo de optimizaci√≥n y entrenamiento\"\"\"\n",
    "    print(\"üöÄ Optimizando XGBoost con Optuna...\")\n",
    "    \n",
    "    # Verificaciones iniciales\n",
    "    if len(np.unique(y_train)) != 2:\n",
    "        raise ValueError(\"y_train debe ser binario (0 y 1)\")\n",
    "    \n",
    "    if X_train_vec.shape[0] != len(y_train):\n",
    "        raise ValueError(\"X_train_vec y y_train deben tener el mismo n√∫mero de filas\")\n",
    "    \n",
    "    best_params = optimizar_xgboost(X_train_vec, y_train, X_test_vec, y_test, n_trials)\n",
    "    modelo_final, final_params = entrenar_modelo_final(best_params, X_train_vec, y_train, X_test_vec, y_test)\n",
    "    \n",
    "    return modelo_final, final_params\n",
    "\n",
    "def guardar_modelos(modelo, vectorizer, nombre_base=\"modelo_toxicidad_xgboost\"):\n",
    "    \"\"\"Guarda el modelo y vectorizer en archivos pickle\"\"\"\n",
    "    import pickle\n",
    "    \n",
    "    # Nombres de archivos\n",
    "    nombre_modelo = f\"../final_model/{nombre_base}_final.pkl\"\n",
    "    nombre_vectorizer = f\"../final_model/vectorizer_toxicidad_final.pkl\"\n",
    "    \n",
    "    try:\n",
    "        # Guardar modelo\n",
    "        with open(nombre_modelo, 'wb') as f:\n",
    "            pickle.dump(modelo, f)\n",
    "        \n",
    "        # Guardar vectorizer\n",
    "        with open(nombre_vectorizer, 'wb') as f:\n",
    "            pickle.dump(vectorizer, f)\n",
    "        \n",
    "        print(f\"‚úÖ Archivos guardados exitosamente:\")\n",
    "        print(f\"   - {nombre_modelo}\")\n",
    "        print(f\"   - {nombre_vectorizer}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error al guardar archivos: {e}\")\n",
    "        return False\n",
    "\n",
    "try:\n",
    "    # Ejecutar optimizaci√≥n\n",
    "    modelo_optimizado, params_optimizados = optimizar_y_entrenar(\n",
    "        X_train_vec, y_train, X_test_vec, y_test, n_trials=60  \n",
    "    )\n",
    "    \n",
    "    # Mostrar mejores par√°metros\n",
    "    print(\"\\nüîß MEJORES PAR√ÅMETROS ENCONTRADOS:\")\n",
    "    for param, valor in params_optimizados.items():\n",
    "        if param not in ['objective', 'eval_metric', 'use_label_encoder', 'random_state', 'n_jobs', 'scale_pos_weight', 'verbosity']:\n",
    "            if isinstance(valor, float):\n",
    "                print(f\"   {param}: {valor:.4f}\")\n",
    "            else:\n",
    "                print(f\"   {param}: {valor}\")\n",
    "    \n",
    "    print(f\"\\nüéØ Optimizaci√≥n completada exitosamente!\")\n",
    "    \n",
    "    # Guardar modelos   \n",
    "    if 'vectorizer' in globals():\n",
    "        guardar_modelos(modelo_optimizado, vectorizer)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Advertencia: 'vectorizer' no est√° definido. Solo guardando el modelo.\")\n",
    "        import pickle\n",
    "        with open('../final_model/modelo_toxicidad_xgboost_final.pkl', 'wb') as f:\n",
    "            pickle.dump(modelo_optimizado, f)\n",
    "        print(\"‚úÖ Modelo guardado: ../final_model/modelo_toxicidad_xgboost_final.pkl\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error durante la optimizaci√≥n: {e}\")\n",
    "    print(\"\\nüîç Verificando datos...\")\n",
    "    \n",
    "    # Diagn√≥stico de datos\n",
    "    print(f\"Forma X_train_vec: {X_train_vec.shape if 'X_train_vec' in globals() else 'No definido'}\")\n",
    "    print(f\"Forma y_train: {y_train.shape if 'y_train' in globals() else 'No definido'}\")\n",
    "    print(f\"Forma X_test_vec: {X_test_vec.shape if 'X_test_vec' in globals() else 'No definido'}\")\n",
    "    print(f\"Forma y_test: {y_test.shape if 'y_test' in globals() else 'No definido'}\")\n",
    "    \n",
    "    if 'y_train' in globals():\n",
    "        print(f\"Valores √∫nicos en y_train: {np.unique(y_train)}\")\n",
    "        print(f\"Distribuci√≥n y_train: {np.bincount(y_train)}\")\n",
    "    \n",
    "    if 'X_train_vec' in globals():\n",
    "        print(f\"Tipo X_train_vec: {type(X_train_vec)}\")\n",
    "        print(f\"¬øHay NaN en X_train_vec?: {np.isnan(X_train_vec).any() if hasattr(X_train_vec, 'shape') else 'No es array'}\")\n",
    "    \n",
    "    print(\"\\nüí° Sugerencias:\")\n",
    "    print(\"1. Aseg√∫rate de que y_train e y_test contengan solo 0s y 1s\")\n",
    "    print(\"2. Verifica que no hay valores NaN en los datos\")\n",
    "    print(\"3. Confirma que X_train_vec y y_train tienen el mismo n√∫mero de filas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "# 16. Probar modelo optimizado con Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_modelo():\n",
    "    \"\"\"Carga el modelo y vectorizer guardados\"\"\"\n",
    "    try:\n",
    "        with open('../final_model/modelo_toxicidad_xgboost_final.pkl', 'rb') as f:\n",
    "            modelo = pickle.load(f)\n",
    "        \n",
    "        with open('../final_model/vectorizer_toxicidad_final.pkl', 'rb') as f:\n",
    "            vectorizer = pickle.load(f)\n",
    "        \n",
    "        print(\"‚úÖ Modelo y vectorizer cargados exitosamente\")\n",
    "        return modelo, vectorizer\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"‚ùå Error: No se encontr√≥ el archivo {e}\")\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error al cargar modelo: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def probar_frase(modelo, vectorizer, frase):\n",
    "    \"\"\"\n",
    "    Prueba una frase y devuelve si es t√≥xica o no\n",
    "    \n",
    "    Args:\n",
    "        modelo: Modelo XGBoost entrenado\n",
    "        vectorizer: Vectorizer entrenado\n",
    "        frase: String con la frase a probar\n",
    "    \n",
    "    Returns:\n",
    "        str: 'toxico' o 'no toxico'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Vectorizar la frase\n",
    "        frase_vectorizada = vectorizer.transform([frase])\n",
    "        \n",
    "        # Predecir\n",
    "        prediccion = modelo.predict(frase_vectorizada)[0]\n",
    "        \n",
    "        # Convertir a texto\n",
    "        return \"toxico\" if prediccion == 1 else \"no toxico\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error al procesar frase: {e}\")\n",
    "        return \"error\"\n",
    "\n",
    "def probar_frases(modelo, vectorizer, frases):\n",
    "    \"\"\"\n",
    "    Prueba m√∫ltiples frases\n",
    "    \n",
    "    Args:\n",
    "        modelo: Modelo XGBoost entrenado\n",
    "        vectorizer: Vectorizer entrenado\n",
    "        frases: Lista de frases o string √∫nico\n",
    "    \n",
    "    Returns:\n",
    "        Lista de diccionarios con frase y predicci√≥n\n",
    "    \"\"\"\n",
    "    # Si es una sola frase, convertir a lista\n",
    "    if isinstance(frases, str):\n",
    "        frases = [frases]\n",
    "    \n",
    "    resultados = []\n",
    "    \n",
    "    for frase in frases:\n",
    "        try:\n",
    "            # Vectorizar\n",
    "            frase_vectorizada = vectorizer.transform([frase])\n",
    "            \n",
    "            # Predecir\n",
    "            prediccion = modelo.predict(frase_vectorizada)[0]\n",
    "            probabilidad = modelo.predict_proba(frase_vectorizada)[0]\n",
    "            \n",
    "            # Confianza (probabilidad m√°xima)\n",
    "            confianza = max(probabilidad)\n",
    "            \n",
    "            resultados.append({\n",
    "                'frase': frase,\n",
    "                'prediccion': \"toxico\" if prediccion == 1 else \"no toxico\",\n",
    "                'confianza': round(confianza, 3),\n",
    "                'prob_toxico': round(probabilidad[1], 3)\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            resultados.append({\n",
    "                'frase': frase,\n",
    "                'prediccion': \"error\",\n",
    "                'confianza': 0.0,\n",
    "                'prob_toxico': 0.0\n",
    "            })\n",
    "    \n",
    "    return resultados\n",
    "\n",
    "def mostrar_resultados(resultados):\n",
    "    \"\"\"Muestra los resultados de forma legible\"\"\"\n",
    "    print(\"\\n=== RESULTADOS DE CLASIFICACI√ìN ===\")\n",
    "    for r in resultados:\n",
    "        if r['prediccion'] != 'error':\n",
    "            print(f'\"{r[\"frase\"]}\" -> {r[\"prediccion\"]} (confianza: {r[\"confianza\"]:.3f})')\n",
    "        else:\n",
    "            print(f'\"{r[\"frase\"]}\" -> ERROR')\n",
    "\n",
    "def clasificar_frases(frases):\n",
    "    \"\"\"\n",
    "    Funci√≥n todo-en-uno para clasificar frases\n",
    "    \n",
    "    Args:\n",
    "        frases: String o lista de strings\n",
    "    \n",
    "    Returns:\n",
    "        Lista de resultados\n",
    "    \"\"\"\n",
    "    # Cargar modelo\n",
    "    modelo, vectorizer = cargar_modelo()\n",
    "    \n",
    "    if modelo is None or vectorizer is None:\n",
    "        return []\n",
    "    \n",
    "    resultados = probar_frases(modelo, vectorizer, frases)\n",
    "\n",
    "    mostrar_resultados(resultados)\n",
    "    \n",
    "    return resultados\n",
    "\n",
    "# EJEMPLO DE USO\n",
    "if __name__ == \"__main__\":\n",
    "    # Frases de prueba\n",
    "    frases_test = [\n",
    "        \"you are stupid\",\n",
    "        \"thank you very much\",\n",
    "        \"this is amazing\",\n",
    "        \"I hate you\",\n",
    "        \"have a great day\",\n",
    "        \"you suck\",\n",
    "        \"this is wonderful\"\n",
    "    ]\n",
    "    \n",
    "    # Usar funci√≥n principal\n",
    "    resultados = clasificar_frases(frases_test)\n",
    "\n",
    "# VERSI√ìN SIMPLE\n",
    "def es_toxico(frase):\n",
    "    \"\"\"Versi√≥n ultra simple que solo devuelve True/False\"\"\"\n",
    "    modelo, vectorizer = cargar_modelo()\n",
    "    if modelo and vectorizer:\n",
    "        resultado = probar_frase(modelo, vectorizer, frase)\n",
    "        return resultado == \"toxico\"\n",
    "    return False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
