{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "\n",
    "# üéØ Objetivo: Detectar toxicidad con datos aumentados\n",
    "\n",
    "## üìã Estrategia:\n",
    "### - Eliminar columnas desbalanceadas\n",
    "### - Aplicar Data Augmentation con traducci√≥n\n",
    "### - Preprocesar texto eficientemente\n",
    "### - Entrenar XGboost\n",
    "### - Evaluar m√©tricas\n",
    "### - Optimizar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Procesamiento de texto\n",
    "import re\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, learning_curve, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, roc_auc_score \n",
    "import xgboost as xgb\n",
    "\n",
    "# Para augmentaci√≥n simple\n",
    "import random\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Persistencia\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuraci√≥n\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# 1. Cargar datos y an√°lisis inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('../data/youtoxic_english_1000.csv')\n",
    "print(f\"‚úÖ Dataset original: {df.shape[0]} filas, {df.shape[1]} columnas\")\n",
    "\n",
    "# Columnas de toxicidad\n",
    "columnas_toxicidad = ['IsAbusive', 'IsThreat', 'IsProvocative', 'IsObscene', \n",
    "                      'IsHatespeech', 'IsRacist', 'IsNationalist', 'IsSexist', \n",
    "                      'IsHomophobic', 'IsReligiousHate', 'IsRadicalism']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# 2. Identificar y eliminar columnas desbalanceadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîç Analizando balance de columnas...\")\n",
    "\n",
    "# Calcular balance\n",
    "balance = {}\n",
    "UMBRAL = 5.0  # 5% m√≠nimo\n",
    "\n",
    "for col in columnas_toxicidad:\n",
    "    porcentaje = (df[col].sum() / len(df)) * 100\n",
    "    balance[col] = porcentaje\n",
    "    estado = \"‚úÖ Mantener\" if porcentaje >= UMBRAL else \"‚ùå Eliminar\"\n",
    "    print(f\"{col:20} -> {porcentaje:5.1f}% {estado}\")\n",
    "\n",
    "# Seleccionar solo columnas balanceadas\n",
    "columnas_mantener = [col for col in columnas_toxicidad if balance[col] >= UMBRAL]\n",
    "columnas_eliminar = [col for col in columnas_toxicidad if balance[col] < UMBRAL]\n",
    "\n",
    "print(f\"\\nüìä Resumen:\")\n",
    "print(f\"   - Columnas a mantener: {len(columnas_mantener)}\")\n",
    "print(f\"   - Columnas a eliminar: {len(columnas_eliminar)}\")\n",
    "\n",
    "# Crear etiqueta binaria solo con columnas balanceadas\n",
    "df['toxic_binary'] = (df[columnas_mantener].sum(axis=1) > 0).astype(int)\n",
    "\n",
    "# Eliminar columnas desbalanceadas del dataset\n",
    "df = df.drop(columns=columnas_eliminar)\n",
    "\n",
    "print(f\"\\n‚úÖ Nueva distribuci√≥n de toxicidad:\")\n",
    "print(df['toxic_binary'].value_counts())\n",
    "print(f\"Porcentaje t√≥xico: {df['toxic_binary'].mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# 3. Preprocesamiento de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüßπ Preparando funciones de preprocesamiento...\")\n",
    "\n",
    "# Inicializar herramientas\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def limpiar_texto(texto):\n",
    "    \"\"\"Limpieza r√°pida y eficiente del texto\"\"\"\n",
    "    if pd.isna(texto):\n",
    "        return \"\"\n",
    "    \n",
    "    texto = str(texto).lower()\n",
    "    texto = re.sub(r'@\\w+|http\\S+|www\\S+', '', texto)  # URLs y menciones\n",
    "    texto = re.sub(r'[^a-zA-Z\\s]', '', texto)  # Solo letras\n",
    "    texto = ' '.join(texto.split())  # Espacios extras\n",
    "    \n",
    "    return texto\n",
    "\n",
    "def procesar_texto(texto):\n",
    "    \"\"\"Procesamiento completo con lemmatizaci√≥n\"\"\"\n",
    "    # Tokenizar\n",
    "    palabras = word_tokenize(texto)\n",
    "    \n",
    "    # Filtrar stopwords y palabras cortas\n",
    "    palabras = [lemmatizer.lemmatize(p) for p in palabras \n",
    "                if p not in stop_words and len(p) > 2]\n",
    "    \n",
    "    return ' '.join(palabras)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "# 4. Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüöÄ Iniciando proceso de Data Augmentation...\")\n",
    "\n",
    "# Augmentation con Wordnet (m√°s preciso que sin√≥nimos manuales)\n",
    "def augmentar_con_wordnet(texto, num_variaciones=2):\n",
    "    \"\"\"Usa WordNet para encontrar sin√≥nimos m√°s precisos\"\"\"\n",
    "    variaciones = []\n",
    "    \n",
    "    try:\n",
    "        # Obtener palabras y sus POS tags\n",
    "        blob = TextBlob(texto)\n",
    "        palabras_tagged = blob.tags\n",
    "        \n",
    "        for _ in range(num_variaciones):\n",
    "            nuevo_texto = texto\n",
    "            palabras_cambiadas = 0\n",
    "            \n",
    "            for palabra, pos in palabras_tagged:\n",
    "                if palabras_cambiadas >= 2:  # Cambiar m√°ximo 2 palabras\n",
    "                    break\n",
    "                \n",
    "                # Obtener sin√≥nimos de WordNet\n",
    "                sinonimos = []\n",
    "                for syn in wordnet.synsets(palabra):\n",
    "                    for lemma in syn.lemmas():\n",
    "                        sinonimo = lemma.name().replace('_', ' ')\n",
    "                        if sinonimo.lower() != palabra.lower():\n",
    "                            sinonimos.append(sinonimo)\n",
    "                \n",
    "                if sinonimos and random.random() < 0.3:  # 30% probabilidad\n",
    "                    sinonimo = random.choice(sinonimos[:3])\n",
    "                    nuevo_texto = nuevo_texto.replace(palabra, sinonimo, 1)\n",
    "                    palabras_cambiadas += 1\n",
    "            \n",
    "            if nuevo_texto != texto:\n",
    "                variaciones.append(nuevo_texto)\n",
    "    \n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    return variaciones\n",
    "\n",
    "# Back traslation\n",
    "def back_translation_simple(texto, idiomas=['es', 'fr', 'de']):\n",
    "    \"\"\"Traduce a otro idioma y de vuelta al ingl√©s\"\"\"\n",
    "    variaciones = []\n",
    "    \n",
    "    for idioma in idiomas:\n",
    "        try:\n",
    "            # Traducir al idioma intermedio\n",
    "            blob_original = TextBlob(texto)\n",
    "            traducido = blob_original.translate(to=idioma)\n",
    "            \n",
    "            # Traducir de vuelta al ingl√©s\n",
    "            vuelta_ingles = traducido.translate(to='en')\n",
    "            \n",
    "            texto_final = str(vuelta_ingles)\n",
    "            if texto_final != texto and len(texto_final) > 5:\n",
    "                variaciones.append(texto_final)\n",
    "        \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    return variaciones\n",
    "\n",
    "# Augmentation contextual avanzado\n",
    "def augmentar_texto_avanzado_v2(texto):\n",
    "    \"\"\"Versi√≥n mejorada del augmentation\"\"\"\n",
    "    variaciones = []\n",
    "    \n",
    "    # Diccionario de sin√≥nimos m√°s extenso y contextual\n",
    "    sinonimos_contextuales = {\n",
    "        'offensive': {\n",
    "            'hate': ['despise', 'loathe', 'detest', 'abhor'],\n",
    "            'stupid': ['dumb', 'idiotic', 'foolish', 'moronic', 'brainless'],\n",
    "            'ugly': ['hideous', 'repulsive', 'disgusting', 'revolting'],\n",
    "            'kill': ['murder', 'destroy', 'eliminate', 'annihilate'],\n",
    "            'idiot': ['fool', 'moron', 'imbecile', 'dimwit'],\n",
    "            'suck': ['terrible', 'awful', 'horrible', 'atrocious'],\n",
    "            'trash': ['garbage', 'rubbish', 'waste', 'junk']\n",
    "        },\n",
    "        'neutral': {\n",
    "            'good': ['great', 'excellent', 'wonderful', 'amazing'],\n",
    "            'bad': ['poor', 'subpar', 'inadequate', 'unsatisfactory'],\n",
    "            'nice': ['pleasant', 'lovely', 'delightful', 'charming'],\n",
    "            'big': ['large', 'huge', 'enormous', 'massive'],\n",
    "            'small': ['tiny', 'little', 'miniature', 'compact']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # T√©cnica 1: Reemplazo contextual\n",
    "    for categoria, palabras in sinonimos_contextuales.items():\n",
    "        for palabra, alternativas in palabras.items():\n",
    "            if palabra in texto.lower():\n",
    "                for alt in alternativas[:2]:\n",
    "                    nuevo_texto = texto.lower().replace(palabra, alt)\n",
    "                    if nuevo_texto != texto.lower():\n",
    "                        variaciones.append(nuevo_texto)\n",
    "    \n",
    "    # T√©cnica 2: Inserci√≥n de intensificadores y modificadores\n",
    "    intensificadores = ['really', 'very', 'extremely', 'totally', 'completely', 'absolutely', 'quite', 'rather']\n",
    "    modificadores = ['honestly', 'actually', 'seriously', 'definitely', 'certainly', 'obviously', 'clearly']\n",
    "    adjetivos = ['stupid', 'dumb', 'ugly', 'bad', 'good', 'nice', 'terrible', 'awful', 'great', 'amazing']\n",
    "    \n",
    "    for adj in adjetivos:\n",
    "        if adj in texto.lower():\n",
    "            for intensif in intensificadores[:3]:\n",
    "                nuevo_texto = texto.lower().replace(adj, f\"{intensif} {adj}\")\n",
    "                if nuevo_texto != texto.lower():\n",
    "                    variaciones.append(nuevo_texto)\n",
    "    \n",
    "    # T√©cnica 2.5: A√±adir modificadores al inicio\n",
    "    for mod in modificadores[:3]:\n",
    "        if len(texto.split()) > 3:  # Solo para textos con m√°s de 3 palabras\n",
    "            nuevo_texto = f\"{mod}, {texto.lower()}\"\n",
    "            if nuevo_texto != texto.lower():\n",
    "                variaciones.append(nuevo_texto)\n",
    "    \n",
    "    # T√©cnica 3: Cambio de perspectiva\n",
    "    cambios_perspectiva = [\n",
    "        ('you are', 'you seem to be'),\n",
    "        ('you are', 'you appear to be'),\n",
    "        ('i think', 'in my opinion'),\n",
    "        ('i believe', 'it seems to me'),\n",
    "        ('this is', 'this seems to be')\n",
    "    ]\n",
    "    \n",
    "    for original, reemplazo in cambios_perspectiva:\n",
    "        if original in texto.lower():\n",
    "            nuevo_texto = texto.lower().replace(original, reemplazo)\n",
    "            if nuevo_texto != texto.lower():\n",
    "                variaciones.append(nuevo_texto)\n",
    "    \n",
    "    # T√©cnica 4: Variaciones simples adicionales\n",
    "    variaciones_simples = [\n",
    "        f\"{texto} really\",\n",
    "        f\"{texto} though\",\n",
    "        f\"i think {texto}\",\n",
    "        f\"honestly {texto}\",\n",
    "        f\"{texto} definitely\",\n",
    "        f\"actually {texto}\",\n",
    "        f\"{texto} for sure\"\n",
    "    ]\n",
    "    \n",
    "    for var in variaciones_simples:\n",
    "        if var.lower() != texto.lower():\n",
    "            variaciones.append(var.lower())\n",
    "    \n",
    "    return list(set(variaciones))[:8]  # M√°ximo 8 variaciones\n",
    "\n",
    "# 4. FUNCI√ìN PRINCIPAL DE AUGMENTATION\n",
    "def aumentar_dataset_completo(df_input, columna_texto='Text', columna_label='toxic_binary', \n",
    "                             factor_aumento=2.5, usar_back_translation=False):\n",
    "    \"\"\"\n",
    "    Funci√≥n principal para aumentar el dataset completo\n",
    "    \n",
    "    Args:\n",
    "        df_input: DataFrame original\n",
    "        columna_texto: Nombre de la columna con texto\n",
    "        columna_label: Nombre de la columna con labels\n",
    "        factor_aumento: Factor de aumento (2.5 = 150% m√°s datos)\n",
    "        usar_back_translation: Si usar back-translation (lento pero efectivo)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame aumentado\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"üîç Analizando dataset para augmentaci√≥n...\")\n",
    "    \n",
    "    # An√°lisis inicial\n",
    "    print(f\"Dataset original: {len(df_input)} filas\")\n",
    "    distribucion = df_input[columna_label].value_counts()\n",
    "    print(f\"Distribuci√≥n actual: {dict(distribucion)}\")\n",
    "    \n",
    "    # Calcular cu√°ntas muestras generar\n",
    "    total_deseado = int(len(df_input) * factor_aumento)\n",
    "    muestras_generar = total_deseado - len(df_input)\n",
    "    \n",
    "    print(f\"Muestras a generar: {muestras_generar}\")\n",
    "    \n",
    "    # Verificar si hay suficiente desequilibrio para augmentar\n",
    "    if len(distribucion) < 2:\n",
    "        print(\"‚ö†Ô∏è Solo hay una clase en el dataset. No es necesario augmentar.\")\n",
    "        return df_input\n",
    "    \n",
    "    # Seleccionar muestras para augmentar (priorizando clase minoritaria)\n",
    "    clase_minoritaria = distribucion.index[-1]\n",
    "    df_minoritaria = df_input[df_input[columna_label] == clase_minoritaria]\n",
    "    df_mayoritaria = df_input[df_input[columna_label] != clase_minoritaria]\n",
    "    \n",
    "    print(f\"Clase minoritaria ({clase_minoritaria}): {len(df_minoritaria)} muestras\")\n",
    "    print(f\"Clase mayoritaria: {len(df_mayoritaria)} muestras\")\n",
    "    \n",
    "    # ESTRATEGIA DE BALANCEO PERFECTO (50-50)\n",
    "    print(\"‚öñÔ∏è Aplicando estrategia de balanceo...\")\n",
    "    \n",
    "    # Determinar el n√∫mero objetivo para cada clase (50-50)\n",
    "    total_objetivo_por_clase = total_deseado // 2\n",
    "    \n",
    "    # Calcular cu√°ntas muestras generar para cada clase\n",
    "    muestras_gen_minoritaria = total_objetivo_por_clase - len(df_minoritaria)\n",
    "    muestras_gen_mayoritaria = total_objetivo_por_clase - len(df_mayoritaria)\n",
    "    \n",
    "    print(f\"Objetivo por clase: {total_objetivo_por_clase} muestras\")\n",
    "    print(f\"Clase minoritaria ({clase_minoritaria}): {len(df_minoritaria)} ‚Üí {total_objetivo_por_clase} (generar {muestras_gen_minoritaria})\")\n",
    "    print(f\"Clase mayoritaria: {len(df_mayoritaria)} ‚Üí {total_objetivo_por_clase} (generar {muestras_gen_mayoritaria})\")\n",
    "    \n",
    "    # Ajustar n√∫meros si son negativos\n",
    "    muestras_gen_minoritaria = max(0, muestras_gen_minoritaria)\n",
    "    muestras_gen_mayoritaria = max(0, muestras_gen_mayoritaria)\n",
    "    \n",
    "    # Generar muestras\n",
    "    datos_nuevos = []\n",
    "    \n",
    "    # Augmentar clase minoritaria (BALANCEO)\n",
    "    print(\"üìà Augmentando clase minoritaria (balanceo perfecto)...\")\n",
    "    if len(df_minoritaria) > 0 and muestras_gen_minoritaria > 0:\n",
    "        contador_minoritaria = 0\n",
    "        intentos = 0\n",
    "        max_intentos = 8  # M√°s intentos\n",
    "        \n",
    "        while contador_minoritaria < muestras_gen_minoritaria and intentos < max_intentos:\n",
    "            print(f\"   - Ciclo {intentos + 1}: Generadas {contador_minoritaria}/{muestras_gen_minoritaria} muestras\")\n",
    "            \n",
    "            for _, row in df_minoritaria.iterrows():\n",
    "                if contador_minoritaria >= muestras_gen_minoritaria:\n",
    "                    break\n",
    "                    \n",
    "                texto_original = row[columna_texto]\n",
    "                \n",
    "                # Generar m√∫ltiples variaciones usando TODAS las t√©cnicas\n",
    "                variaciones = []\n",
    "                variaciones.extend(augmentar_texto_avanzado_v2(texto_original))\n",
    "                variaciones.extend(augmentar_con_wordnet(texto_original, num_variaciones=4))\n",
    "                \n",
    "                if usar_back_translation:\n",
    "                    variaciones.extend(back_translation_simple(texto_original))\n",
    "                \n",
    "                # Si no hay suficientes variaciones, crear variaciones simples\n",
    "                if len(variaciones) < 5:\n",
    "                    # A√±adir variaciones simples con modificaciones menores\n",
    "                    variaciones.append(texto_original + \" really\")\n",
    "                    variaciones.append(\"honestly \" + texto_original)\n",
    "                    variaciones.append(texto_original.replace(\".\", \" definitely.\") if \".\" in texto_original else texto_original + \" definitely\")\n",
    "                    variaciones.append(\"actually \" + texto_original)\n",
    "                    variaciones.append(texto_original + \" for sure\")\n",
    "                \n",
    "                # Usar todas las variaciones disponibles\n",
    "                for variacion in variaciones:\n",
    "                    if contador_minoritaria >= muestras_gen_minoritaria:\n",
    "                        break\n",
    "                    if variacion and len(variacion.strip()) > 5:  # Verificar que sea v√°lida\n",
    "                        nueva_fila = row.copy()\n",
    "                        nueva_fila[columna_texto] = variacion\n",
    "                        datos_nuevos.append(nueva_fila)\n",
    "                        contador_minoritaria += 1\n",
    "            \n",
    "            intentos += 1\n",
    "        \n",
    "        print(f\"   - Clase minoritaria: {contador_minoritaria} muestras generadas\")\n",
    "    \n",
    "    # Augmentar clase mayoritaria (BALANCEO)\n",
    "    print(\"üìä Augmentando clase mayoritaria (balanceo perfecto)...\")\n",
    "    if muestras_gen_mayoritaria > 0 and len(df_mayoritaria) > 0:\n",
    "        contador_mayoritaria = 0\n",
    "        intentos = 0\n",
    "        max_intentos = 6  # Suficientes intentos para balanceo\n",
    "        \n",
    "        while contador_mayoritaria < muestras_gen_mayoritaria and intentos < max_intentos:\n",
    "            print(f\"   - Ciclo {intentos + 1}: Generadas {contador_mayoritaria}/{muestras_gen_mayoritaria} muestras\")\n",
    "            \n",
    "            for _, row in df_mayoritaria.iterrows():\n",
    "                if contador_mayoritaria >= muestras_gen_mayoritaria:\n",
    "                    break\n",
    "                    \n",
    "                texto_original = row[columna_texto]\n",
    "                \n",
    "                # Generar m√∫ltiples variaciones\n",
    "                variaciones = []\n",
    "                variaciones.extend(augmentar_texto_avanzado_v2(texto_original))\n",
    "                variaciones.extend(augmentar_con_wordnet(texto_original, num_variaciones=3))\n",
    "                \n",
    "                # Si no hay suficientes variaciones, crear variaciones simples\n",
    "                if len(variaciones) < 4:\n",
    "                    variaciones.append(texto_original + \" though\")\n",
    "                    variaciones.append(\"i think \" + texto_original)\n",
    "                    variaciones.append(texto_original + \" probably\")\n",
    "                    variaciones.append(\"maybe \" + texto_original)\n",
    "                \n",
    "                # Usar m√∫ltiples variaciones por texto\n",
    "                for variacion in variaciones:\n",
    "                    if contador_mayoritaria >= muestras_gen_mayoritaria:\n",
    "                        break\n",
    "                    if variacion and len(variacion.strip()) > 5:  # Verificar que sea v√°lida\n",
    "                        nueva_fila = row.copy()\n",
    "                        nueva_fila[columna_texto] = variacion\n",
    "                        datos_nuevos.append(nueva_fila)\n",
    "                        contador_mayoritaria += 1\n",
    "            \n",
    "            intentos += 1\n",
    "        \n",
    "        print(f\"   - Clase mayoritaria: {contador_mayoritaria} muestras generadas\")\n",
    "    \n",
    "    # Combinar datasets\n",
    "    if datos_nuevos:\n",
    "        df_nuevos = pd.DataFrame(datos_nuevos)\n",
    "        df_final = pd.concat([df_input, df_nuevos], ignore_index=True)\n",
    "        print(f\"‚úÖ Se generaron {len(datos_nuevos)} nuevas muestras\")\n",
    "    else:\n",
    "        df_final = df_input.copy()\n",
    "        print(\"‚ö†Ô∏è No se generaron datos nuevos.\")\n",
    "    \n",
    "    # Resultados\n",
    "    print(f\"\\nüìä Resultados del augmentation:\")\n",
    "    print(f\"Dataset original: {len(df_input)} filas\")\n",
    "    print(f\"Dataset aumentado: {len(df_final)} filas\")\n",
    "    if len(df_final) > len(df_input):\n",
    "        incremento = ((len(df_final) - len(df_input)) / len(df_input) * 100)\n",
    "        print(f\"Incremento: {len(df_final) - len(df_input)} filas ({incremento:.1f}%)\")\n",
    "    \n",
    "    # Nueva distribuci√≥n\n",
    "    nueva_distribucion = df_final[columna_label].value_counts()\n",
    "    print(f\"Nueva distribuci√≥n: {dict(nueva_distribucion)}\")\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "# üöÄ EJECUTAR AUGMENTATION EN TU DATASET\n",
    "print(f\"\\nüìã Dataset actual antes del augmentation:\")\n",
    "print(f\"   - Filas: {len(df)}\")\n",
    "print(f\"   - Columnas: {list(df.columns)}\")\n",
    "print(f\"   - Distribuci√≥n toxic_binary: {dict(df['toxic_binary'].value_counts())}\")\n",
    "\n",
    "# Ejecutar augmentaci√≥n en tu dataset existente\n",
    "df_aumentado = aumentar_dataset_completo(\n",
    "    df_input=df,  # Tu dataset ya procesado\n",
    "    columna_texto='Text',  # Columna de texto en tu dataset\n",
    "    columna_label='toxic_binary',  # Tu columna de etiquetas binarias\n",
    "    factor_aumento=2.5,  # 150% m√°s datos para llegar a ~2500\n",
    "    usar_back_translation=False  # Cambiar a True si quieres a√∫n m√°s variedad (m√°s lento)\n",
    ")\n",
    "\n",
    "# CREAR df_final (para preprocesamiento)\n",
    "df_final = df_aumentado.copy()\n",
    "\n",
    "print(f\"\\nüéâ Augmentation completado!\")\n",
    "print(f\"‚úÖ Variable 'df_final' creada con {len(df_final)} filas, lista para preprocesamiento.\")\n",
    "\n",
    "# Mostrar estad√≠sticas finales\n",
    "print(f\"\\nüìà Estad√≠sticas finales:\")\n",
    "print(f\"   - Total de filas: {len(df_final)}\")\n",
    "print(f\"   - Distribuci√≥n final: {dict(df_final['toxic_binary'].value_counts())}\")\n",
    "porcentaje_toxico = df_final['toxic_binary'].mean() * 100\n",
    "print(f\"   - Porcentaje t√≥xico: {porcentaje_toxico:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# 5. Preprocesar el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n‚è≥ Preprocesando todos los textos...\")\n",
    "\n",
    "# Aplicar limpieza y procesamiento\n",
    "df_final['texto_limpio'] = df_final['Text'].apply(limpiar_texto)\n",
    "df_final['texto_procesado'] = df_final['texto_limpio'].apply(procesar_texto)\n",
    "\n",
    "# Eliminar filas vac√≠as\n",
    "df_final = df_final[df_final['texto_procesado'].str.len() > 0]\n",
    "\n",
    "print(f\"‚úÖ Textos procesados: {len(df_final)}\")\n",
    "\n",
    "# Guardar dataset procesado\n",
    "df_final.to_csv('../data/dataset_toxicidad_aumentado.csv', index=False)\n",
    "print(\"üíæ Dataset guardado como: ../data/dataset_toxicidad_aumentado.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "# 6. Visualizaci√≥n de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Balance de clases\n",
    "df_final['toxic_binary'].value_counts().plot(kind='bar', ax=axes[0], \n",
    "                                            color=['lightgreen', 'salmon'])\n",
    "axes[0].set_title('Distribuci√≥n de Clases (Con Augmentation)')\n",
    "axes[0].set_xlabel('Clase')\n",
    "axes[0].set_ylabel('Cantidad')\n",
    "axes[0].set_xticklabels(['No T√≥xico', 'T√≥xico'], rotation=0)\n",
    "\n",
    "# Longitud de comentarios\n",
    "df_final['longitud'] = df_final['texto_procesado'].str.split().str.len()\n",
    "df_final.boxplot(column='longitud', by='toxic_binary', ax=axes[1])\n",
    "axes[1].set_title('Longitud de Comentarios por Clase')\n",
    "axes[1].set_xlabel('T√≥xico')\n",
    "axes[1].set_ylabel('N√∫mero de palabras')\n",
    "plt.suptitle('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "# Wordclouds comparativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä WORDCLOUDS COMPARATIVOS\")\n",
    "\n",
    "# Separar textos por categor√≠a usando los datos procesados\n",
    "textos_toxicos = df_final[df_final['toxic_binary'] == 1]['texto_procesado']\n",
    "textos_no_toxicos = df_final[df_final['toxic_binary'] == 0]['texto_procesado']\n",
    "\n",
    "print(f\"   ‚Ä¢ Comentarios t√≥xicos para WordCloud: {len(textos_toxicos)}\")\n",
    "print(f\"   ‚Ä¢ Comentarios no t√≥xicos para WordCloud: {len(textos_no_toxicos)}\")\n",
    "\n",
    "# Combinar textos por categor√≠a\n",
    "texto_toxico_combinado = ' '.join(textos_toxicos.dropna())\n",
    "texto_no_toxico_combinado = ' '.join(textos_no_toxicos.dropna())\n",
    "\n",
    "print(f\"   ‚Ä¢ Palabras en corpus t√≥xico: {len(texto_toxico_combinado.split())}\")\n",
    "print(f\"   ‚Ä¢ Palabras en corpus no t√≥xico: {len(texto_no_toxico_combinado.split())}\")\n",
    "\n",
    "# Verificar que tenemos suficiente texto\n",
    "if len(texto_toxico_combinado.split()) < 10:\n",
    "    print(\"‚ö†Ô∏è Advertencia: Poco texto t√≥xico disponible para WordCloud\")\n",
    "if len(texto_no_toxico_combinado.split()) < 10:\n",
    "    print(\"‚ö†Ô∏è Advertencia: Poco texto no t√≥xico disponible para WordCloud\")\n",
    "\n",
    "# Generar WordClouds\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Configuraci√≥n com√∫n para ambos WordClouds\n",
    "wordcloud_config = {\n",
    "    'width': 800,\n",
    "    'height': 400,\n",
    "    'background_color': 'white',\n",
    "    'max_words': 100,\n",
    "    'relative_scaling': 0.5,\n",
    "    'stopwords': stop_words,  # Usar las mismas stopwords del preprocesamiento\n",
    "    'collocation_threshold': 10\n",
    "}\n",
    "\n",
    "# WordCloud para comentarios t√≥xicos\n",
    "if len(texto_toxico_combinado.strip()) > 0:\n",
    "    wordcloud_toxico = WordCloud(\n",
    "        **wordcloud_config,\n",
    "        colormap='Reds'\n",
    "    ).generate(texto_toxico_combinado)\n",
    "    \n",
    "    axes[0].imshow(wordcloud_toxico, interpolation='bilinear')\n",
    "    axes[0].set_title('WordCloud - Comentarios T√ìXICOS', fontweight='bold', fontsize=16, color='darkred')\n",
    "    axes[0].axis('off')\n",
    "else:\n",
    "    axes[0].text(0.5, 0.5, 'No hay suficiente\\ntexto t√≥xico', \n",
    "                ha='center', va='center', transform=axes[0].transAxes, fontsize=16)\n",
    "    axes[0].set_title('WordCloud - Comentarios T√ìXICOS', fontweight='bold', fontsize=16, color='darkred')\n",
    "\n",
    "# WordCloud para comentarios no t√≥xicos\n",
    "if len(texto_no_toxico_combinado.strip()) > 0:\n",
    "    wordcloud_no_toxico = WordCloud(\n",
    "        **wordcloud_config,\n",
    "        colormap='Greens'\n",
    "    ).generate(texto_no_toxico_combinado)\n",
    "    \n",
    "    axes[1].imshow(wordcloud_no_toxico, interpolation='bilinear')\n",
    "    axes[1].set_title('WordCloud - Comentarios NO T√ìXICOS', fontweight='bold', fontsize=16, color='darkgreen')\n",
    "    axes[1].axis('off')\n",
    "else:\n",
    "    axes[1].text(0.5, 0.5, 'No hay suficiente\\ntexto no t√≥xico', \n",
    "                ha='center', va='center', transform=axes[1].transAxes, fontsize=16)\n",
    "    axes[1].set_title('WordCloud - Comentarios NO T√ìXICOS', fontweight='bold', fontsize=16, color='darkgreen')\n",
    "\n",
    "plt.suptitle('An√°lisis Visual del Vocabulario por Categor√≠a', fontsize=20, fontweight='bold', y=0.95)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "# 7. Preparaci√≥n para machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüéØ Preparando datos para entrenamiento...\")\n",
    "\n",
    "# Features y target\n",
    "X = df_final['texto_procesado']\n",
    "y = df_final['toxic_binary']\n",
    "\n",
    "# Divisi√≥n estratificada\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"üìä Divisi√≥n de datos:\")\n",
    "print(f\"   - Entrenamiento: {len(X_train)} ({y_train.mean()*100:.1f}% t√≥xicos)\")\n",
    "print(f\"   - Prueba: {len(X_test)} ({y_test.mean()*100:.1f}% t√≥xicos)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "# 8. Vectorizaci√≥n optimizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüî¢ Vectorizando con TF-IDF...\")\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=2000,      # M√°s features por m√°s datos\n",
    "    ngram_range=(1, 3),     # Incluir trigramas\n",
    "    min_df=2,               # M√≠nima frecuencia\n",
    "    max_df=0.95,            # M√°xima frecuencia\n",
    "    sublinear_tf=True,      # Escalado logar√≠tmico\n",
    "    use_idf=True,           # IDF para ponderar importancia\n",
    ")\n",
    "\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"‚úÖ Forma de datos vectorizados: {X_train_vec.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "# 9. Entrenamiento de XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüöÄ Entrenando XGBoost optimizado...\")\n",
    "\n",
    "# Calcular peso de clases para balanceo\n",
    "scale_pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1])\n",
    "\n",
    "# Modelo XGBoost con hiperpar√°metros optimizados\n",
    "modelo = xgb.XGBClassifier(\n",
    "    # Par√°metros b√°sicos\n",
    "    n_estimators=300,           # N√∫mero de √°rboles\n",
    "    max_depth=6,                # Profundidad m√°xima\n",
    "    learning_rate=0.1,          # Tasa de aprendizaje\n",
    "    \n",
    "    # Control de overfitting\n",
    "    subsample=0.8,              # Submuestreo de filas\n",
    "    colsample_bytree=0.8,       # Submuestreo de columnas\n",
    "    reg_alpha=0.1,              # Regularizaci√≥n L1\n",
    "    reg_lambda=1.0,             # Regularizaci√≥n L2\n",
    "    \n",
    "    # Balanceo de clases\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    \n",
    "    # Otros par√°metros\n",
    "    objective='binary:logistic',\n",
    "    eval_metric=['error', 'logloss'],  # M√©tricas de evaluaci√≥n\n",
    "    use_label_encoder=False,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,                  # Usar todos los cores\n",
    "    early_stopping_rounds=20    # Early stopping\n",
    ")\n",
    "\n",
    "# Entrenar con conjunto de validaci√≥n\n",
    "eval_set = [(X_train_vec, y_train), (X_test_vec, y_test)]\n",
    "modelo.fit(\n",
    "    X_train_vec, y_train,\n",
    "    eval_set=eval_set,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Obtener informaci√≥n del entrenamiento\n",
    "resultado_entrenamiento = modelo.evals_result()\n",
    "if resultado_entrenamiento:\n",
    "    # Obtener el mejor score de la validaci√≥n\n",
    "    val_scores = resultado_entrenamiento['validation_1']['logloss']\n",
    "    mejor_iteracion = np.argmin(val_scores)\n",
    "    mejor_score = val_scores[mejor_iteracion]\n",
    "    print(f\"‚úÖ Mejor iteraci√≥n: {mejor_iteracion + 1}\")\n",
    "    print(f\"‚úÖ Mejor score (logloss): {mejor_score:.4f}\")\n",
    "\n",
    "# Validaci√≥n cruzada con modelo sin early stopping\n",
    "print(\"\\nüìà Realizando validaci√≥n cruzada...\")\n",
    "modelo_cv = xgb.XGBClassifier(\n",
    "    n_estimators=100,  # Menos √°rboles para CV r√°pida\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=1.0,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    objective='binary:logistic',\n",
    "    use_label_encoder=False,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "scores_cv = cross_val_score(modelo_cv, X_train_vec, y_train, cv=5, scoring='f1')\n",
    "print(f\"   - F1-Scores: {[f'{s:.3f}' for s in scores_cv]}\")\n",
    "print(f\"   - Media: {scores_cv.mean():.3f} (+/- {scores_cv.std() * 2:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "# 10. Evaluaci√≥n detallada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä EVALUACI√ìN EN CONJUNTO DE PRUEBA:\")\n",
    "\n",
    "# Predicciones\n",
    "y_pred = modelo.predict(X_test_vec)\n",
    "y_pred_proba = modelo.predict_proba(X_test_vec)[:, 1]\n",
    "\n",
    "# M√©tricas\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nüéØ M√©tricas principales:\")\n",
    "print(f\"   - Accuracy: {accuracy:.3f}\")\n",
    "print(f\"   - F1-Score: {f1:.3f}\")\n",
    "\n",
    "# Reporte completo\n",
    "print(\"\\nüìã Reporte de clasificaci√≥n:\")\n",
    "print(classification_report(y_test, y_pred, \n",
    "                          target_names=['No T√≥xico', 'T√≥xico'],\n",
    "                          digits=3))\n",
    "\n",
    "# Matriz de confusi√≥n\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['No T√≥xico', 'T√≥xico'],\n",
    "            yticklabels=['No T√≥xico', 'T√≥xico'])\n",
    "plt.title('Matriz de Confusi√≥n - XGBoost')\n",
    "plt.ylabel('Valor Real')\n",
    "plt.xlabel('Predicci√≥n')\n",
    "\n",
    "# Agregar m√©tricas en el t√≠tulo\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "plt.text(0.5, -0.1, f'Precisi√≥n: {precision:.3f} | Recall: {recall:.3f} | F1: {f1:.3f}', \n",
    "         ha='center', transform=plt.gca().transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "# 11. An√°lisis de importancia de features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüî§ AN√ÅLISIS DE IMPORTANCIA DE FEATURES:\")\n",
    "\n",
    "# Obtener importancia de features de XGBoost\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "importancias = modelo.feature_importances_\n",
    "\n",
    "# Crear DataFrame de importancias\n",
    "df_importancia = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importancias\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Top 20 features m√°s importantes\n",
    "print(\"\\nüèÜ Top 20 features m√°s importantes:\")\n",
    "for idx, row in df_importancia.head(20).iterrows():\n",
    "    print(f\"   '{row['feature']}': {row['importance']:.4f}\")\n",
    "\n",
    "# Visualizar importancia de features\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_features = df_importancia.head(30)\n",
    "plt.barh(range(len(top_features)), top_features['importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Importancia')\n",
    "plt.title('Top 30 Features M√°s Importantes - XGBoost')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# An√°lisis adicional: Gain vs Cover\n",
    "if hasattr(modelo, 'get_booster'):\n",
    "    print(\"\\nüìä An√°lisis detallado de importancia:\")\n",
    "    importance_types = ['weight', 'gain', 'cover']\n",
    "    \n",
    "    for imp_type in importance_types:\n",
    "        importances_dict = modelo.get_booster().get_score(importance_type=imp_type)\n",
    "        if importances_dict:\n",
    "            print(f\"\\n{imp_type.upper()}:\")\n",
    "            sorted_imp = sorted(importances_dict.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "            for feat, score in sorted_imp:\n",
    "                if feat.startswith('f'):\n",
    "                    feat_idx = int(feat[1:])\n",
    "                    feat_name = feature_names[feat_idx]\n",
    "                    print(f\"   '{feat_name}': {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "# 12. Funci√≥n de predicci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predecir_toxicidad(texto, modelo=modelo, vectorizer=vectorizer):\n",
    "    \"\"\"\n",
    "    Predice si un comentario es t√≥xico.\n",
    "    \n",
    "    Retorna:\n",
    "    - etiqueta: 'T√ìXICO' o 'NO T√ìXICO'\n",
    "    - confianza: probabilidad de la predicci√≥n\n",
    "    \"\"\"\n",
    "    # Preprocesar\n",
    "    texto_limpio = limpiar_texto(texto)\n",
    "    texto_procesado = procesar_texto(texto_limpio)\n",
    "    \n",
    "    # Vectorizar\n",
    "    texto_vec = vectorizer.transform([texto_procesado])\n",
    "    \n",
    "    # Predecir\n",
    "    prediccion = modelo.predict(texto_vec)[0]\n",
    "    probabilidad = modelo.predict_proba(texto_vec)[0, 1]\n",
    "    \n",
    "    etiqueta = \"T√ìXICO ‚ö†Ô∏è\" if prediccion == 1 else \"NO T√ìXICO ‚úÖ\"\n",
    "    confianza = probabilidad if prediccion == 1 else (1 - probabilidad)\n",
    "    \n",
    "    return etiqueta, confianza\n",
    "\n",
    "# Probar con ejemplos\n",
    "print(\"\\nüß™ PRUEBAS CON COMENTARIOS NUEVOS:\")\n",
    "\n",
    "ejemplos = [\n",
    "    \"Great video, thanks for sharing!\",\n",
    "    \"You're so stupid and ignorant\",\n",
    "    \"I disagree with your opinion\",\n",
    "    \"This is garbage content, delete it\",\n",
    "    \"Interesting perspective, never thought about it that way\"\n",
    "]\n",
    "\n",
    "for comentario in ejemplos:\n",
    "    etiqueta, confianza = predecir_toxicidad(comentario)\n",
    "    print(f\"\\nüìù '{comentario}'\")\n",
    "    print(f\"   ‚Üí {etiqueta} (Confianza: {confianza:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "# 13. An√°lisis de overfitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analizar_overfitting(modelo_xgb, X_train_vec, y_train, X_test_vec, y_test):\n",
    "    \"\"\"\n",
    "    An√°lisis completo de overfitting del modelo XGBoost\n",
    "    Adaptado para el notebook de detecci√≥n de toxicidad\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üîç AN√ÅLISIS DE OVERFITTING\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. M√âTRICAS COMPARATIVAS ENTRE CONJUNTOS\n",
    "    print(\"\\nüìä COMPARACI√ìN DE M√âTRICAS POR CONJUNTO:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    conjuntos_datos = {\n",
    "        'Entrenamiento': (X_train_vec, y_train),\n",
    "        'Prueba': (X_test_vec, y_test)\n",
    "    }\n",
    "    \n",
    "    comparacion_metricas = {}\n",
    "    \n",
    "    for nombre, (X, y) in conjuntos_datos.items():\n",
    "        y_pred = modelo_xgb.predict(X)\n",
    "        y_proba = modelo_xgb.predict_proba(X)[:, 1]\n",
    "        \n",
    "        metricas = {\n",
    "            'accuracy': accuracy_score(y, y_pred),\n",
    "            'precision': precision_score(y, y_pred),\n",
    "            'recall': recall_score(y, y_pred),\n",
    "            'f1': f1_score(y, y_pred),\n",
    "            'auc': roc_auc_score(y, y_proba)\n",
    "        }\n",
    "        \n",
    "        comparacion_metricas[nombre] = metricas\n",
    "        \n",
    "        print(f\"\\n{nombre}:\")\n",
    "        for metrica, valor in metricas.items():\n",
    "            print(f\"  {metrica.upper()}: {valor:.4f}\")\n",
    "    \n",
    "    # 2. DETECTAR OVERFITTING\n",
    "    print(f\"\\nüö® DETECCI√ìN DE OVERFITTING:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    train_f1 = comparacion_metricas['Entrenamiento']['f1']\n",
    "    test_f1 = comparacion_metricas['Prueba']['f1']\n",
    "    \n",
    "    # Diferencias\n",
    "    train_test_diff = train_f1 - test_f1\n",
    "    \n",
    "    print(f\"üìà F1 Train vs Test: {train_test_diff:.4f}\")\n",
    "    \n",
    "    # An√°lisis de overfitting\n",
    "    overfitting_detectado = False\n",
    "    \n",
    "    if train_test_diff > 0.08:  # M√°s de 8% de diferencia es preocupante\n",
    "        print(\"‚ùå OVERFITTING SEVERO detectado (Train >> Test)\")\n",
    "        overfitting_detectado = True\n",
    "    elif train_test_diff > 0.05:  # M√°s de 5% de diferencia\n",
    "        print(\"‚ö†Ô∏è  OVERFITTING MODERADO detectado (Train >> Test)\")\n",
    "        overfitting_detectado = True\n",
    "    elif train_test_diff > 0.02:  # Ligero overfitting\n",
    "        print(\"‚ö†Ô∏è  LIGERO OVERFITTING detectado\")\n",
    "        overfitting_detectado = True\n",
    "    \n",
    "    if not overfitting_detectado:\n",
    "        print(\"‚úÖ NO se detecta overfitting significativo\")\n",
    "        print(\"‚úÖ Modelo tiene buena generalizaci√≥n\")\n",
    "    \n",
    "    return comparacion_metricas\n",
    "\n",
    "def graficar_analisis_overfitting(modelo_xgb, X_train_vec, y_train, X_test_vec, y_test, comparacion_metricas):\n",
    "    \"\"\"\n",
    "    Visualizaciones para an√°lisis de overfitting\n",
    "    Adaptado para el modelo XGBoost del notebook\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nüìä GENERANDO VISUALIZACIONES...\")\n",
    "    \n",
    "    # Crear subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('An√°lisis de Overfitting - Modelo XGBoost Toxicidad', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Subplot 1: Curvas de aprendizaje\n",
    "    print(\"üìà Calculando curvas de aprendizaje...\")\n",
    "    \n",
    "    # Crear un modelo XGBoost sin early stopping para las curvas de aprendizaje\n",
    "    modelo_sin_early_stopping = xgb.XGBClassifier(\n",
    "        n_estimators=100,  # Menos √°rboles para ser m√°s r√°pido\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=1.0,\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='logloss',\n",
    "        use_label_encoder=False,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        modelo_sin_early_stopping, X_train_vec, y_train,\n",
    "        cv=3, \n",
    "        train_sizes=np.linspace(0.3, 1.0, 6),  # Empezar con m√°s datos para evitar problemas\n",
    "        scoring='f1',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Calcular medias y desviaciones\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    val_mean = np.mean(val_scores, axis=1)\n",
    "    val_std = np.std(val_scores, axis=1)\n",
    "    \n",
    "    axes[0, 0].plot(train_sizes, train_mean, 'o-', color='blue', label='Entrenamiento', linewidth=2)\n",
    "    axes[0, 0].fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.2, color='blue')\n",
    "    \n",
    "    axes[0, 0].plot(train_sizes, val_mean, 'o-', color='red', label='Validaci√≥n Cruzada', linewidth=2)\n",
    "    axes[0, 0].fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.2, color='red')\n",
    "    \n",
    "    axes[0, 0].set_xlabel('Tama√±o del conjunto de entrenamiento')\n",
    "    axes[0, 0].set_ylabel('F1 Score')\n",
    "    axes[0, 0].set_title('Curvas de Aprendizaje', fontweight='bold')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Subplot 2: Comparaci√≥n de m√©tricas Train vs Test\n",
    "    conjuntos = ['Entrenamiento', 'Prueba']\n",
    "    f1_scores = [\n",
    "        comparacion_metricas['Entrenamiento']['f1'],\n",
    "        comparacion_metricas['Prueba']['f1']\n",
    "    ]\n",
    "    \n",
    "    colores = ['lightblue', 'lightcoral']\n",
    "    barras = axes[0, 1].bar(conjuntos, f1_scores, color=colores)\n",
    "    axes[0, 1].set_ylabel('F1 Score')\n",
    "    axes[0, 1].set_title('F1 Score: Entrenamiento vs Prueba', fontweight='bold')\n",
    "    axes[0, 1].set_ylim(0, 1)\n",
    "    \n",
    "    # A√±adir valores en las barras\n",
    "    for barra, score in zip(barras, f1_scores):\n",
    "        axes[0, 1].text(barra.get_x() + barra.get_width()/2, barra.get_height() + 0.01,\n",
    "                       f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # L√≠nea de referencia para mostrar la diferencia\n",
    "    diferencia = abs(f1_scores[0] - f1_scores[1])\n",
    "    axes[0, 1].text(0.5, 0.5, f'Diferencia: {diferencia:.3f}', \n",
    "                   transform=axes[0, 1].transAxes, ha='center',\n",
    "                   bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\", alpha=0.7))\n",
    "    \n",
    "    # Subplot 3: Distribuci√≥n de probabilidades por conjunto\n",
    "    y_proba_train = modelo_xgb.predict_proba(X_train_vec)[:, 1]\n",
    "    y_proba_test = modelo_xgb.predict_proba(X_test_vec)[:, 1]\n",
    "    \n",
    "    axes[1, 0].hist(y_proba_train, bins=30, alpha=0.7, label='Entrenamiento', color='blue', density=True)\n",
    "    axes[1, 0].hist(y_proba_test, bins=30, alpha=0.7, label='Prueba', color='red', density=True)\n",
    "    axes[1, 0].set_xlabel('Probabilidad de Toxicidad')\n",
    "    axes[1, 0].set_ylabel('Densidad')\n",
    "    axes[1, 0].set_title('Distribuci√≥n de Probabilidades Predichas', fontweight='bold')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Subplot 4: Todas las m√©tricas comparadas\n",
    "    metricas = ['Accuracy', 'Precision', 'Recall', 'F1', 'AUC']\n",
    "    train_metricas = [comparacion_metricas['Entrenamiento'][m.lower()] for m in metricas]\n",
    "    test_metricas = [comparacion_metricas['Prueba'][m.lower()] for m in metricas]\n",
    "    \n",
    "    x = np.arange(len(metricas))\n",
    "    ancho = 0.35\n",
    "    \n",
    "    axes[1, 1].bar(x - ancho/2, train_metricas, ancho, label='Entrenamiento', color='lightblue')\n",
    "    axes[1, 1].bar(x + ancho/2, test_metricas, ancho, label='Prueba', color='lightcoral')\n",
    "    \n",
    "    axes[1, 1].set_xlabel('M√©tricas')\n",
    "    axes[1, 1].set_ylabel('Score')\n",
    "    axes[1, 1].set_title('Comparaci√≥n Completa de M√©tricas', fontweight='bold')\n",
    "    axes[1, 1].set_xticks(x)\n",
    "    axes[1, 1].set_xticklabels(metricas, rotation=45)\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    axes[1, 1].set_ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def generar_reporte_overfitting(comparacion_metricas):\n",
    "    \"\"\"\n",
    "    Generar reporte final de overfitting\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nüìã REPORTE FINAL DE OVERFITTING\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    train_f1 = comparacion_metricas['Entrenamiento']['f1']\n",
    "    test_f1 = comparacion_metricas['Prueba']['f1']\n",
    "    \n",
    "    # An√°lisis detallado\n",
    "    print(f\"üìä AN√ÅLISIS DETALLADO:\")\n",
    "    print(f\"   ‚Ä¢ F1 Entrenamiento: {train_f1:.4f}\")\n",
    "    print(f\"   ‚Ä¢ F1 Prueba:        {test_f1:.4f}\")\n",
    "    \n",
    "    gap_train_test = train_f1 - test_f1\n",
    "    \n",
    "    print(f\"\\nüîç GAP DE RENDIMIENTO:\")\n",
    "    print(f\"   ‚Ä¢ Train-Test gap:   {gap_train_test:.4f}\")\n",
    "    \n",
    "    # Diagn√≥stico\n",
    "    print(f\"\\nü©∫ DIAGN√ìSTICO:\")\n",
    "    \n",
    "    if gap_train_test < 0.02:\n",
    "        print(\"   ‚úÖ EXCELENTE: Modelo muy bien generalizado\")\n",
    "        recomendacion = \"El modelo est√° listo para producci√≥n\"\n",
    "        color_estado = \"üü¢\"\n",
    "        \n",
    "    elif gap_train_test < 0.05:\n",
    "        print(\"   ‚úÖ BUENO: Ligero overfitting, pero aceptable\")\n",
    "        recomendacion = \"Modelo aceptable para producci√≥n con monitoreo\"\n",
    "        color_estado = \"üü°\"\n",
    "        \n",
    "    elif gap_train_test < 0.08:\n",
    "        print(\"   ‚ö†Ô∏è  MODERADO: Overfitting detectado\")\n",
    "        recomendacion = \"Considerar m√°s regularizaci√≥n o early stopping m√°s agresivo\"\n",
    "        color_estado = \"üü†\"\n",
    "        \n",
    "    else:\n",
    "        print(\"   ‚ùå SEVERO: Overfitting significativo\")\n",
    "        recomendacion = \"Necesario ajustar hiperpar√°metros o reentrenar\"\n",
    "        color_estado = \"üî¥\"\n",
    "    \n",
    "    print(f\"\\nüí° RECOMENDACI√ìN:\")\n",
    "    print(f\"   {recomendacion}\")\n",
    "    \n",
    "    # M√©tricas de generalizaci√≥n (no \"confianza\" para evitar confusi√≥n)\n",
    "    puntaje_generalizacion = max(0, 100 - (gap_train_test * 100 * 15))\n",
    "    print(f\"\\nüéØ CAPACIDAD DE GENERALIZACI√ìN: {puntaje_generalizacion:.1f}/100 {color_estado}\")\n",
    "    \n",
    "    if puntaje_generalizacion >= 85:\n",
    "        print(\"   üèÜ EXCELENTE generalizaci√≥n - Modelo muy robusto\")\n",
    "    elif puntaje_generalizacion >= 70:\n",
    "        print(\"   üëç BUENA generalizaci√≥n - Modelo confiable\")\n",
    "    elif puntaje_generalizacion >= 50:\n",
    "        print(\"   ‚ö†Ô∏è  GENERALIZACI√ìN MEDIA - Modelo aceptable con reservas\")\n",
    "    else:\n",
    "        print(\"   üëé GENERALIZACI√ìN BAJA - Hay overfitting, revisar modelo\")\n",
    "    \n",
    "    # An√°lisis adicional espec√≠fico para detecci√≥n de toxicidad\n",
    "    print(f\"\\nüéØ AN√ÅLISIS ESPEC√çFICO PARA TOXICIDAD:\")\n",
    "    \n",
    "    train_precision = comparacion_metricas['Entrenamiento']['precision']\n",
    "    test_precision = comparacion_metricas['Prueba']['precision']\n",
    "    precision_gap = train_precision - test_precision\n",
    "    \n",
    "    train_recall = comparacion_metricas['Entrenamiento']['recall']\n",
    "    test_recall = comparacion_metricas['Prueba']['recall']\n",
    "    recall_gap = train_recall - test_recall\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Gap Precision: {precision_gap:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Gap Recall:    {recall_gap:.4f}\")\n",
    "    \n",
    "    if precision_gap > 0.1:\n",
    "        print(\"   ‚ö†Ô∏è  Modelo podr√≠a estar generando muchos falsos positivos en producci√≥n\")\n",
    "    if recall_gap > 0.1:\n",
    "        print(\"   ‚ö†Ô∏è  Modelo podr√≠a estar perdiendo comentarios t√≥xicos en producci√≥n\")\n",
    "\n",
    "# EJECUTAR AN√ÅLISIS COMPLETO DE OVERFITTING\n",
    "# 1. Analizar overfitting con las m√©tricas\n",
    "comparacion_metricas = analizar_overfitting(\n",
    "    modelo, X_train_vec, y_train, X_test_vec, y_test\n",
    ")\n",
    "\n",
    "# 2. Generar visualizaciones\n",
    "graficar_analisis_overfitting(\n",
    "    modelo, X_train_vec, y_train, X_test_vec, y_test, comparacion_metricas\n",
    ")\n",
    "\n",
    "# 3. Generar reporte final\n",
    "generar_reporte_overfitting(comparacion_metricas)\n",
    "\n",
    "print(f\"\\n‚úÖ AN√ÅLISIS DE OVERFITTING COMPLETADO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# AN√ÅLISIS ADICIONAL: PREDICCIONES POR CONFIANZA\n",
    "print(f\"\\nüìà AN√ÅLISIS ADICIONAL: DISTRIBUCI√ìN DE CONFIANZA\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Analizar las predicciones por nivel de confianza\n",
    "y_proba_test = modelo.predict_proba(X_test_vec)[:, 1]\n",
    "\n",
    "# Categorizar predicciones por confianza\n",
    "alta_confianza = (y_proba_test >= 0.8) | (y_proba_test <= 0.2)\n",
    "media_confianza = ((y_proba_test >= 0.6) & (y_proba_test < 0.8)) | ((y_proba_test > 0.2) & (y_proba_test <= 0.4))\n",
    "baja_confianza = (y_proba_test > 0.4) & (y_proba_test < 0.6)\n",
    "\n",
    "print(f\"üìä Distribuci√≥n de confianza en predicciones de prueba:\")\n",
    "print(f\"   ‚Ä¢ Alta confianza (>80% o <20%):  {alta_confianza.sum():3d} ({alta_confianza.mean()*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Media confianza (60-80%, 20-40%): {media_confianza.sum():3d} ({media_confianza.mean()*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Baja confianza (40-60%):       {baja_confianza.sum():3d} ({baja_confianza.mean()*100:.1f}%)\")\n",
    "\n",
    "# Calcular accuracy por nivel de confianza\n",
    "if alta_confianza.sum() > 0:\n",
    "    acc_alta = accuracy_score(y_test[alta_confianza], (y_proba_test[alta_confianza] > 0.5))\n",
    "    print(f\"\\nüéØ Accuracy por nivel de confianza:\")\n",
    "    print(f\"   ‚Ä¢ Alta confianza: {acc_alta:.3f}\")\n",
    "\n",
    "if media_confianza.sum() > 0:\n",
    "    acc_media = accuracy_score(y_test[media_confianza], (y_proba_test[media_confianza] > 0.5))\n",
    "    print(f\"   ‚Ä¢ Media confianza: {acc_media:.3f}\")\n",
    "\n",
    "if baja_confianza.sum() > 0:\n",
    "    acc_baja = accuracy_score(y_test[baja_confianza], (y_proba_test[baja_confianza] > 0.5))\n",
    "    print(f\"   ‚Ä¢ Baja confianza: {acc_baja:.3f}\")\n",
    "\n",
    "print(f\"\\nüí° Interpretaci√≥n:\")\n",
    "if baja_confianza.mean() < 0.15:  # Menos del 15% de predicciones inciertas\n",
    "    print(\"   ‚úÖ Modelo hace predicciones con alta certeza individual\")\n",
    "    print(\"   üìä La mayor√≠a de predicciones son muy seguras (>80% o <20%)\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  Considerable n√∫mero de predicciones con baja certeza\")\n",
    "    print(\"   üìä Muchas predicciones est√°n en zona gris (40-60%)\")\n",
    "\n",
    "print(f\"\\nüèÅ AN√ÅLISIS COMPLETO FINALIZADO\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "# 14. Optimizaci√≥n con Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"Funci√≥n objetivo para optimizar hiperpar√°metros anti-overfitting\"\"\"\n",
    "    try:\n",
    "        scale_pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1])\n",
    "        \n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 8),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "            'subsample': trial.suggest_float('subsample', 0.7, 0.9),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 0.9),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 0.01, 5.0),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 0.01, 5.0),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 7),\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': ['error', 'logloss'],  \n",
    "            'use_label_encoder': False,  \n",
    "            'random_state': 42,\n",
    "            'n_jobs': 1,  # Usar 1 core para evitar conflictos en CV\n",
    "            'scale_pos_weight': scale_pos_weight,\n",
    "            'verbosity': 0\n",
    "        }\n",
    "        \n",
    "        # Modelo para validaci√≥n cruzada\n",
    "        modelo_cv = xgb.XGBClassifier(**params)\n",
    "        \n",
    "        # Validaci√≥n cruzada\n",
    "        cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)  \n",
    "        f1_scores = cross_val_score(\n",
    "            modelo_cv, X_train_vec, y_train, \n",
    "            cv=cv, scoring='f1', error_score='raise'\n",
    "        )\n",
    "        \n",
    "        modelo_test = xgb.XGBClassifier(**params)\n",
    "        modelo_test.fit(X_train_vec, y_train)\n",
    "        \n",
    "        train_f1 = f1_score(y_train, modelo_test.predict(X_train_vec))\n",
    "        test_f1 = f1_score(y_test, modelo_test.predict(X_test_vec))\n",
    "        overfitting_gap = abs(train_f1 - test_f1)\n",
    "        \n",
    "        train_pred_proba = modelo_test.predict_proba(X_train_vec)[:, 1]\n",
    "        test_pred_proba = modelo_test.predict_proba(X_test_vec)[:, 1]\n",
    "        \n",
    "        from sklearn.metrics import log_loss\n",
    "        train_logloss = log_loss(y_train, train_pred_proba)\n",
    "        test_logloss = log_loss(y_test, test_pred_proba)\n",
    "        logloss_gap = abs(train_logloss - test_logloss)\n",
    "        \n",
    "        # Guardar m√©tricas\n",
    "        trial.set_user_attr('cv_f1', f1_scores.mean())\n",
    "        trial.set_user_attr('cv_f1_std', f1_scores.std())\n",
    "        trial.set_user_attr('train_f1', train_f1)\n",
    "        trial.set_user_attr('test_f1', test_f1)\n",
    "        trial.set_user_attr('overfitting_gap', overfitting_gap)\n",
    "        trial.set_user_attr('train_logloss', train_logloss)\n",
    "        trial.set_user_attr('test_logloss', test_logloss)\n",
    "        trial.set_user_attr('logloss_gap', logloss_gap)\n",
    "        \n",
    "        # Optimiza F1 pero penaliza overfitting en ambas m√©tricas\n",
    "        penalty = (overfitting_gap * 1.0) + (logloss_gap * 0.5)\n",
    "        return f1_scores.mean() - penalty\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error en trial: {e}\")\n",
    "        return -1.0  # Score muy bajo para trials fallidos\n",
    "\n",
    "def optimizar_xgboost(X_train_vec, y_train, X_test_vec, y_test, n_trials=50):\n",
    "    \"\"\"Optimiza hiperpar√°metros usando Optuna\"\"\"\n",
    "    # Hacer variables globales para objective\n",
    "    globals().update({\n",
    "        'X_train_vec': X_train_vec, 'y_train': y_train,\n",
    "        'X_test_vec': X_test_vec, 'y_test': y_test\n",
    "    })\n",
    "    \n",
    "    # Verificar que los datos sean v√°lidos\n",
    "    print(f\"Datos de entrenamiento: {X_train_vec.shape}, {len(y_train)}\")\n",
    "    print(f\"Datos de prueba: {X_test_vec.shape}, {len(y_test)}\")\n",
    "    print(f\"Distribuci√≥n y_train: {np.bincount(y_train)}\")\n",
    "    \n",
    "    study = optuna.create_study(\n",
    "        direction='maximize', \n",
    "        sampler=optuna.samplers.TPESampler(seed=42)\n",
    "    )\n",
    "    \n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "    \n",
    "    # Filtrar trials exitosos\n",
    "    successful_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE and t.value > -0.5]\n",
    "    \n",
    "    if not successful_trials:\n",
    "        raise ValueError(\"No se completaron trials exitosos. Revisa tus datos.\")\n",
    "    \n",
    "    best_trial = max(successful_trials, key=lambda x: x.value)\n",
    "    \n",
    "    print(f\"\\nMejor score: {best_trial.value:.4f}\")\n",
    "    print(f\"CV F1: {best_trial.user_attrs['cv_f1']:.4f} (¬±{best_trial.user_attrs['cv_f1_std']:.4f})\")\n",
    "    print(f\"Train F1: {best_trial.user_attrs['train_f1']:.4f}\")\n",
    "    print(f\"Test F1: {best_trial.user_attrs['test_f1']:.4f}\")\n",
    "    print(f\"Gap overfitting F1: {best_trial.user_attrs['overfitting_gap']:.4f}\")\n",
    "    print(f\"Train LogLoss: {best_trial.user_attrs['train_logloss']:.4f}\")  # NUEVO\n",
    "    print(f\"Test LogLoss: {best_trial.user_attrs['test_logloss']:.4f}\")    # NUEVO\n",
    "    print(f\"Gap overfitting LogLoss: {best_trial.user_attrs['logloss_gap']:.4f}\")  # NUEVO\n",
    "    print(f\"Trials exitosos: {len(successful_trials)}/{len(study.trials)}\")\n",
    "    \n",
    "    return best_trial.params\n",
    "\n",
    "def entrenar_modelo_final(best_params, X_train_vec, y_train, X_test_vec, y_test):\n",
    "    \"\"\"Entrena modelo final con par√°metros optimizados - VERSI√ìN CORREGIDA\"\"\"\n",
    "    scale_pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1])\n",
    "    \n",
    "    final_params = {\n",
    "        **best_params,\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': ['error', 'logloss'],  \n",
    "        'use_label_encoder': False,           \n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1,\n",
    "        'scale_pos_weight': scale_pos_weight,\n",
    "        'verbosity': 0\n",
    "    }\n",
    "    \n",
    "    modelo = xgb.XGBClassifier(**final_params)\n",
    "    modelo.fit(X_train_vec, y_train)\n",
    "    \n",
    "    # Evaluaci√≥n final\n",
    "    train_pred = modelo.predict(X_train_vec)\n",
    "    test_pred = modelo.predict(X_test_vec)\n",
    "    \n",
    "    train_f1 = f1_score(y_train, train_pred)\n",
    "    test_f1 = f1_score(y_test, test_pred)\n",
    "    \n",
    "    # A√±adir evaluaci√≥n de logloss\n",
    "    train_pred_proba = modelo.predict_proba(X_train_vec)[:, 1]\n",
    "    test_pred_proba = modelo.predict_proba(X_test_vec)[:, 1]\n",
    "    \n",
    "    from sklearn.metrics import log_loss\n",
    "    train_logloss = log_loss(y_train, train_pred_proba)\n",
    "    test_logloss = log_loss(y_test, test_pred_proba)\n",
    "    \n",
    "    # Validaci√≥n cruzada final\n",
    "    cv_scores = cross_val_score(\n",
    "        xgb.XGBClassifier(**final_params), \n",
    "        X_train_vec, y_train, \n",
    "        cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=42), \n",
    "        scoring='f1'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Modelo final:\")\n",
    "    print(f\"Train F1: {train_f1:.4f}\")\n",
    "    print(f\"Test F1: {test_f1:.4f}\")\n",
    "    print(f\"CV F1: {cv_scores.mean():.3f} (¬±{cv_scores.std() * 2:.3f})\")\n",
    "    print(f\"Overfitting gap F1: {abs(train_f1 - test_f1):.4f}\")\n",
    "    print(f\"Train LogLoss: {train_logloss:.4f}\")     \n",
    "    print(f\"Test LogLoss: {test_logloss:.4f}\")      \n",
    "    print(f\"Overfitting gap LogLoss: {abs(train_logloss - test_logloss):.4f}\") \n",
    "    \n",
    "    return modelo, final_params\n",
    "\n",
    "def optimizar_y_entrenar(X_train_vec, y_train, X_test_vec, y_test, n_trials=50):\n",
    "    \"\"\"Proceso completo de optimizaci√≥n y entrenamiento\"\"\"\n",
    "    print(\"üöÄ Optimizando XGBoost con Optuna...\")\n",
    "    \n",
    "    # Verificaciones iniciales\n",
    "    if len(np.unique(y_train)) != 2:\n",
    "        raise ValueError(\"y_train debe ser binario (0 y 1)\")\n",
    "    \n",
    "    if X_train_vec.shape[0] != len(y_train):\n",
    "        raise ValueError(\"X_train_vec y y_train deben tener el mismo n√∫mero de filas\")\n",
    "    \n",
    "    best_params = optimizar_xgboost(X_train_vec, y_train, X_test_vec, y_test, n_trials)\n",
    "    modelo_final, final_params = entrenar_modelo_final(best_params, X_train_vec, y_train, X_test_vec, y_test)\n",
    "    \n",
    "    return modelo_final, final_params\n",
    "\n",
    "def guardar_modelos(modelo, vectorizer, nombre_base=\"modelo_toxicidad_xgboost\"):\n",
    "    \"\"\"Guarda el modelo y vectorizer en archivos pickle\"\"\"\n",
    "    import pickle\n",
    "    \n",
    "    # Nombres de archivos\n",
    "    nombre_modelo = f\"../final_model/{nombre_base}_final.pkl\"\n",
    "    nombre_vectorizer = f\"../final_model/vectorizer_toxicidad_final.pkl\"\n",
    "    \n",
    "    try:\n",
    "        # Guardar modelo\n",
    "        with open(nombre_modelo, 'wb') as f:\n",
    "            pickle.dump(modelo, f)\n",
    "        \n",
    "        # Guardar vectorizer\n",
    "        with open(nombre_vectorizer, 'wb') as f:\n",
    "            pickle.dump(vectorizer, f)\n",
    "        \n",
    "        print(f\"‚úÖ Archivos guardados exitosamente:\")\n",
    "        print(f\"   - {nombre_modelo}\")\n",
    "        print(f\"   - {nombre_vectorizer}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error al guardar archivos: {e}\")\n",
    "        return False\n",
    "\n",
    "try:\n",
    "    # Ejecutar optimizaci√≥n\n",
    "    modelo_optimizado, params_optimizados = optimizar_y_entrenar(\n",
    "        X_train_vec, y_train, X_test_vec, y_test, n_trials=60  \n",
    "    )\n",
    "    \n",
    "    # Mostrar mejores par√°metros\n",
    "    print(\"\\nüîß MEJORES PAR√ÅMETROS ENCONTRADOS:\")\n",
    "    for param, valor in params_optimizados.items():\n",
    "        if param not in ['objective', 'eval_metric', 'use_label_encoder', 'random_state', 'n_jobs', 'scale_pos_weight', 'verbosity']:\n",
    "            if isinstance(valor, float):\n",
    "                print(f\"   {param}: {valor:.4f}\")\n",
    "            else:\n",
    "                print(f\"   {param}: {valor}\")\n",
    "    \n",
    "    print(f\"\\nüéØ Optimizaci√≥n completada exitosamente!\")\n",
    "    \n",
    "    # Guardar modelos   \n",
    "    if 'vectorizer' in globals():\n",
    "        guardar_modelos(modelo_optimizado, vectorizer)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Advertencia: 'vectorizer' no est√° definido. Solo guardando el modelo.\")\n",
    "        import pickle\n",
    "        with open('../final_model/modelo_toxicidad_xgboost_final.pkl', 'wb') as f:\n",
    "            pickle.dump(modelo_optimizado, f)\n",
    "        print(\"‚úÖ Modelo guardado: ../final_model/modelo_toxicidad_xgboost_final.pkl\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error durante la optimizaci√≥n: {e}\")\n",
    "    print(\"\\nüîç Verificando datos...\")\n",
    "    \n",
    "    # Diagn√≥stico de datos\n",
    "    print(f\"Forma X_train_vec: {X_train_vec.shape if 'X_train_vec' in globals() else 'No definido'}\")\n",
    "    print(f\"Forma y_train: {y_train.shape if 'y_train' in globals() else 'No definido'}\")\n",
    "    print(f\"Forma X_test_vec: {X_test_vec.shape if 'X_test_vec' in globals() else 'No definido'}\")\n",
    "    print(f\"Forma y_test: {y_test.shape if 'y_test' in globals() else 'No definido'}\")\n",
    "    \n",
    "    if 'y_train' in globals():\n",
    "        print(f\"Valores √∫nicos en y_train: {np.unique(y_train)}\")\n",
    "        print(f\"Distribuci√≥n y_train: {np.bincount(y_train)}\")\n",
    "    \n",
    "    if 'X_train_vec' in globals():\n",
    "        print(f\"Tipo X_train_vec: {type(X_train_vec)}\")\n",
    "        print(f\"¬øHay NaN en X_train_vec?: {np.isnan(X_train_vec).any() if hasattr(X_train_vec, 'shape') else 'No es array'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
