{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos las dependencias\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    f1_score,\n",
    "    make_scorer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "Importamos el dataset inicial y elejimos las 2 columnas que hemos decidido usar para el entrenamiento del modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "starter_df = pd.read_csv(\"../data/youtoxic_english_1000.csv\")\n",
    "starter_df = starter_df[[\"Text\", \"IsToxic\"]]\n",
    "\n",
    "starter_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Imprimimos información básica del dataset y vemos que hay 1000 filas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "starter_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Imprimmos la suma de cada variable en la columna \"IsToxic\" para ver el balanceo del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_hate = (starter_df[\"IsToxic\"] == True).sum()\n",
    "count_no_hate = (starter_df[\"IsToxic\"] == False).sum()\n",
    "print(f\"Hay {count_hate} comentarios de odio\")\n",
    "print(f\"Hay {count_no_hate} comentarios sin odio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = ['Toxico', 'No Toxico']\n",
    "counts = [count_hate, count_no_hate]\n",
    "\n",
    "plt.bar(labels, counts)\n",
    "plt.title('Distribución de Comentarios Tóxicos')\n",
    "plt.xlabel('Tipo de comentario')\n",
    "plt.ylabel('Cantidad')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Como vemos el dataset está bastante balanceado pero al ser tan pocos datos el modelo puede tener problemas para predecir, por lo que decidimos incluir más datos desde [otro dataset de Kaggle.](https://www.kaggle.com/code/giovanimachado/hate-speech-bert-cnn-and-bert-mlp-in-tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.read_csv(\"../data/labeled_data.csv\")\n",
    "\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = (new_df[\"hate_speech\"] == 0).sum()\n",
    "count_1 = (new_df[\"hate_speech\"] == 2).sum()\n",
    "\n",
    "print(f\"Hay {count} comentarios de odio en el nuevo dataset\")\n",
    "print(f\"Hay {count_1} comentarios de no odio en el nuevo dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "Vemos que este dataset tiene bastantes datos que podemos integrar en nuestro dataset. Decidimos traer 1250 filas random de cada tipo de dato, es decir cuando \"hate_speech\" sea 0 y 2, Sí, No, respectivamente. Lo guardamos en un nuevo archivo llamado extra_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar y renombrar 1250 ejemplos de cada clase\n",
    "df_0 = new_df[new_df[\"hate_speech\"] == 0][[\"tweet\", \"hate_speech\"]].sample(n=1250, random_state=42)\n",
    "df_2 = new_df[new_df[\"hate_speech\"] == 2][[\"tweet\", \"hate_speech\"]].sample(n=1250, random_state=42)\n",
    "\n",
    "nuevo_df = pd.concat([df_0, df_2], ignore_index=True)\n",
    "\n",
    "nuevo_df = nuevo_df.rename(columns={\n",
    "    \"tweet\": \"Text\",\n",
    "    \"hate_speech\": \"IsToxic\"\n",
    "})\n",
    "\n",
    "# Crear columna booleana: True si es tóxico (hate_speech == 0), False si no (hate_speech == 2)\n",
    "nuevo_df[\"IsToxic\"] = nuevo_df[\"IsToxic\"] == 0\n",
    "\n",
    "nuevo_df.to_csv(\"../data/extra_data.csv\", index=False)\n",
    "\n",
    "print(nuevo_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuevo_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Concatenamos el dataset extra_data.csv con nuestro dataset inicial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_df = pd.read_csv(\"../data/extra_data.csv\")\n",
    "df = pd.concat([starter_df, extra_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "count_hate = (df[\"IsToxic\"] == True).sum()\n",
    "count_no_hate = (df[\"IsToxic\"] == False).sum()\n",
    "print(f\"Hay {count_hate} comentarios de odio\")\n",
    "print(f\"Hay {count_no_hate} comentarios sin odio\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = ['Toxico', 'No Toxico']\n",
    "counts = [count_hate, count_no_hate]\n",
    "\n",
    "plt.bar(labels, counts)\n",
    "plt.title('Distribución de Comentarios Tóxicos')\n",
    "plt.xlabel('Tipo de comentario')\n",
    "plt.ylabel('Cantidad')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Y hemos pasado de tener 1000 filas, a tener 3500 filas y sigue siendo un dataset balanceado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def limpiar_texto(texto):\n",
    "    tokens = nltk.word_tokenize(str(texto).lower())\n",
    "    tokens = [t for t in tokens if t.isalpha()]\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df['texto_limpio'] = df['Text'].apply(limpiar_texto)\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=150,\n",
    "    min_df=35,\n",
    "    max_df=0.6,\n",
    "    ngram_range=(1, 2),\n",
    "    sublinear_tf=True\n",
    ")\n",
    "X = vectorizer.fit_transform(df['texto_limpio'])\n",
    "y = df['IsToxic'].astype(int)\n",
    "\n",
    "modelo = LogisticRegression(C=0.06, max_iter=1000, class_weight=\"balanced\")\n",
    "f1_scorer = make_scorer(f1_score, pos_label=1)\n",
    "scores = cross_val_score(modelo, X, y, cv=5, scoring=f1_scorer)\n",
    "\n",
    "print(f\"\\n=== Validación cruzada (F1, clase tóxica) ===\")\n",
    "print(f\"F1-score promedio: {scores.mean():.3f}\")\n",
    "print(f\"Desviación estándar: {scores.std():.3f}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "modelo.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = modelo.predict(X_train)\n",
    "print(\"\\n=== Evaluación en TRAIN ===\")\n",
    "print(confusion_matrix(y_train, y_train_pred))\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "y_test_pred = modelo.predict(X_test)\n",
    "print(\"\\n=== Evaluación en TEST ===\")\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "f1_train = f1_score(y_train, y_train_pred, pos_label=1)\n",
    "f1_test = f1_score(y_test, y_test_pred, pos_label=1)\n",
    "\n",
    "overfitting_f1_pct = ((f1_train - f1_test) / f1_train) * 100 if f1_train != 0 else 0.0\n",
    "print(f\"F1-score TRAIN (clase tóxica): {f1_train:.3f}\")\n",
    "print(f\"F1-score TEST  (clase tóxica): {f1_test:.3f}\")\n",
    "print(f\"Overfitting (basado en F1): {overfitting_f1_pct:.2f}%\")\n",
    "\n",
    "def predict_true_false(prob):\n",
    "    return prob >= 0.5\n",
    "\n",
    "def predecir_toxicidad(texto):\n",
    "    texto_limpio = limpiar_texto(texto)\n",
    "    texto_vect = vectorizer.transform([texto_limpio])\n",
    "    prob = modelo.predict_proba(texto_vect)\n",
    "    print(f\"Probabilidades de cada clase [no tóxico, tóxico]: {prob}\")\n",
    "    prob_toxico = prob[0][1]\n",
    "    print(f\"Probabilidad de toxicidad: {prob_toxico:.3f}\")\n",
    "    prediccion_final = predict_true_false(prob_toxico)\n",
    "    return {\n",
    "        \"prediccion\": prediccion_final,\n",
    "        \"probabilidad_toxico\": round(prob_toxico, 3)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predecir_toxicidad(\"I love this product! It's amazing.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predecir_toxicidad(\"This song is terrible, I hate it.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "Como vemos estas métricas no están del todo bien, porque el modelo no está prediciendo bien los casos Tóxicos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "Decidimos probar otro modelo, primero añadiendo datos con sinónimos, y usando Transformers para ver si las métricas mejoran."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from transformers import (\n",
    "    BertTokenizerFast,\n",
    "    BertForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from wordfreq import zipf_frequency\n",
    "\n",
    "# Descargar recursos NLTK\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"🔧 Usando device: {device}\")\n",
    "\n",
    "def synonym_replacement(text, n=2):\n",
    "    words = text.split()\n",
    "    new_words = words.copy()\n",
    "    random_word_list = list(set([w for w in words if len(w)>3]))\n",
    "    random.shuffle(random_word_list)\n",
    "    num_replaced = 0\n",
    "    for word in random_word_list:\n",
    "        synonyms = set()\n",
    "        for syn in wordnet.synsets(word):\n",
    "            for lemma in syn.lemmas():\n",
    "                synonym = lemma.name().replace(\"_\", \" \").lower()\n",
    "                if synonym != word and zipf_frequency(synonym, \"en\") > 2:\n",
    "                    synonyms.add(synonym)\n",
    "        if len(synonyms) > 0:\n",
    "            new_word = random.choice(list(synonyms))\n",
    "            new_words = [new_word if w == word else w for w in new_words]\n",
    "            num_replaced += 1\n",
    "        if num_replaced >= n:\n",
    "            break\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "df[\"IsToxic\"] = df[\"IsToxic\"].astype(int)\n",
    "\n",
    "tox_df = df[df[\"IsToxic\"] == 1]\n",
    "augmented = []\n",
    "for _, row in tox_df.sample(n=len(tox_df), replace=True, random_state=42).iterrows():\n",
    "    augmented.append({\n",
    "        \"Text\": synonym_replacement(row[\"Text\"]),\n",
    "        \"IsToxic\": 1\n",
    "    })\n",
    "aug_df = pd.DataFrame(augmented)\n",
    "df_aug = pd.concat([df, aug_df], ignore_index=True)\n",
    "print(f\"Datos después de augmentation: {df_aug.shape}\")\n",
    "\n",
    "train_df, test_df = train_test_split(df_aug, test_size=0.2, stratify=df_aug[\"IsToxic\"], random_state=42)\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"Text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df).map(tokenize, batched=True)\n",
    "test_ds = Dataset.from_pandas(test_df).map(tokenize, batched=True)\n",
    "\n",
    "train_ds = train_ds.remove_columns([\"Text\", \"__index_level_0__\"]).rename_column(\"IsToxic\", \"labels\")\n",
    "test_ds = test_ds.remove_columns([\"Text\", \"__index_level_0__\"]).rename_column(\"IsToxic\", \"labels\")\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2).to(device)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"precision\": precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\": recall_score(labels, preds, zero_division=0),\n",
    "        \"f1\": f1_score(labels, preds, zero_division=0)\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    use_mps_device=True,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    weight_decay=0.1,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "test_metrics = trainer.evaluate(eval_dataset=test_ds)\n",
    "print(\"\\n=== RESULTADOS EN TEST ===\")\n",
    "for k, v in test_metrics.items():\n",
    "    print(f\"{k}: {v:.3f}\")\n",
    "\n",
    "train_metrics = trainer.evaluate(eval_dataset=train_ds)\n",
    "print(\"\\n=== RESULTADOS EN TRAIN ===\")\n",
    "for k, v in train_metrics.items():\n",
    "    print(f\"{k}: {v:.3f}\")\n",
    "\n",
    "f1_train = train_metrics[\"eval_f1\"]\n",
    "f1_test = test_metrics[\"eval_f1\"]\n",
    "overfitting_pct = ((f1_train - f1_test) / f1_train) * 100 if f1_train != 0 else 0.0\n",
    "print(f\"\\nOverfitting basado en F1: {overfitting_pct:.2f}%\")\n",
    "\n",
    "def predecir_toxicidad(texto):\n",
    "    inputs = tokenizer(texto, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128).to(device)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    probs = outputs.logits.softmax(dim=1)[0].cpu().numpy()\n",
    "    print(f\"pred_toxico: {bool(probs[1] >= 0.5)}, prob_toxico: {round(float(probs[1]), 3)}, prob_no_toxico: {round(float(probs[0]), 3)}\")\n",
    "\n",
    "    return {\n",
    "        \"pred_toxico\": bool(probs[1] >= 0.5),\n",
    "        \"prob_toxico\": round(float(probs[1]), 3),\n",
    "        \"prob_no_toxico\": round(float(probs[0]), 3)\n",
    "\n",
    "    }\n",
    "\n",
    "print(predecir_toxicidad(\"You're a disgusting human.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "El modelo da buenas métricas pero 9% de overfitting. \n",
    "\n",
    "TODO: mejorar el overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
