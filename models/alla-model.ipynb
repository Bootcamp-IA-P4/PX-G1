{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/youtoxic_english_1000.csv\")\n",
    "df = df.drop(columns=[\"IsHomophobic\", \"IsRadicalism\", \"IsSexist\"])\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos las dependencias\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargamos los datos \n",
    "df = df[['Text', 'IsToxic']]  # solo nos interesa esta columna como target\n",
    "\n",
    "# preprocesamiento del texto\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def limpiar_texto(texto):\n",
    "    tokens = nltk.word_tokenize(str(texto).lower())\n",
    "    tokens = [t for t in tokens if t.isalpha()]  # elimina puntuación y números\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df['texto_limpio'] = df['Text'].apply(limpiar_texto)\n",
    "\n",
    "# vectorización\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(df['texto_limpio'])\n",
    "\n",
    "# variable objetivo\n",
    "y = df['IsToxic'].astype(int)\n",
    "\n",
    "# división en train y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# entrenamiento del modelo\n",
    "modelo = MultinomialNB()\n",
    "modelo.fit(X_train, y_train)\n",
    "\n",
    "# evaluación\n",
    "y_pred = modelo.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# función para pasar de 0 y 1 a True y False\n",
    "def predict_true_false(prob):\n",
    "    if prob >= 0.5:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Función para predecir nuevos textos\n",
    "def predecir_toxicidad(texto):\n",
    "    texto_limpio = limpiar_texto(texto)\n",
    "    texto_vect = vectorizer.transform([texto_limpio])\n",
    "    prob = modelo.predict_proba(texto_vect) # prob nos devuelve las probabilidades de cada clase, por ejemplo: [[0.2, 0.8]] donde 0.2 es la probabilidad de que no sea tóxico y 0.8 es la probabilidad de que sí lo sea.\n",
    "    print(f\"Probabilidades de cada clase, [not-toxic, toxic]: {prob}\")\n",
    "    prob_toxico = prob[0][1]  # accedemos a la probabilidad de la clase tóxica mediante índices, donde 0 es la primera fila (la única en este caso) y 1 es la segunda columna (la probabilidad de ser tóxico). \n",
    "    print(f\"Probabilidad de toxicidad: {prob_toxico}\")\n",
    "    prediccion_final = predict_true_false(prob_toxico)\n",
    "    return {\"prediccion\": prediccion_final, \"probabilidad_toxico\": round(prob_toxico, 3)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predecir_toxicidad(\"If only people would just take a step back and not make this case about them, because it wasn't about anyone except the two people in that situation.  To lump yourself into this mess and take matters into your own hands makes these kinds of protests selfish and without rational thought and investigation.  The guy in this video is heavily emotional and hyped up and wants to be heard, and when he gets heard he just presses more and more.  He was never out to have a reasonable discussion.  Kudos to the Smerconish for keeping level the whole time and letting Masri make himself out to be a fool.  How dare he and those that tore that city down in protest make this about themselves and to dishonor the entire incident with their own hate.  By the way, since when did police brutality become an epidemic?  I wish everyone would just stop pretending like they were there and they knew EXACTLY what was going on, because there's no measurable amount of people that honestly witnessed this incident, so none of us have a clue on which way this whole issue should have swung.  The grand jury were the most informed, we have to trust the majority rule was the right course of action and let it be.  Also, thank you to the 99.999% of police officers in America that actually serve & protect, even if you're a bit of a jerk when you pull me over, I respect your job and know that someone has to do it and that many people are going to pout about being held accountable to their actions.  People hate police until they need an officer or two around in an emergency.\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
