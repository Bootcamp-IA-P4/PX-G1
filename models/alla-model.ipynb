{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos las dependencias\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    f1_score,\n",
    "    make_scorer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "starter_df = pd.read_csv(\"../data/youtoxic_english_1000.csv\")\n",
    "starter_df = starter_df[[\"Text\", \"IsToxic\"]]\n",
    "\n",
    "starter_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "starter_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_hate = (starter_df[\"IsToxic\"] == True).sum()\n",
    "count_no_hate = (starter_df[\"IsToxic\"] == False).sum()\n",
    "print(f\"Hay {count_hate} comentarios de odio\")\n",
    "print(f\"Hay {count_no_hate} comentarios sin odio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.read_csv(\"../data/labeled_data.csv\")\n",
    "\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = (new_df[\"hate_speech\"] == 0).sum()\n",
    "print(f\"Hay {count} comentarios de odio en el nuevo dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuevo_df = new_df[new_df[\"hate_speech\"] == 0][[\"tweet\", \"hate_speech\"]]\n",
    "nuevo_df = nuevo_df.rename(columns={\n",
    "    \"tweet\": \"Text\",\n",
    "    \"hate_speech\": \"IsToxic\"\n",
    "})\n",
    "nuevo_df[\"IsToxic\"] = nuevo_df[\"IsToxic\"] == 0\n",
    "\n",
    "nuevo_df = nuevo_df.sample(n=500, random_state=42)\n",
    "\n",
    "nuevo_df.to_csv(\"../data/extra_data.csv\", index=False)\n",
    "\n",
    "print(nuevo_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuevo_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_df = pd.read_csv(\"../data/extra_data.csv\")\n",
    "df = pd.concat([starter_df, extra_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def limpiar_texto(texto):\n",
    "    tokens = nltk.word_tokenize(str(texto).lower())\n",
    "    tokens = [t for t in tokens if t.isalpha()]\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df['texto_limpio'] = df['Text'].apply(limpiar_texto)\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=170,\n",
    "    min_df=35,\n",
    "    max_df=0.6,\n",
    "    ngram_range=(1, 2),\n",
    "    sublinear_tf=True\n",
    ")\n",
    "X = vectorizer.fit_transform(df['texto_limpio'])\n",
    "y = df['IsToxic'].astype(int)\n",
    "\n",
    "modelo = LogisticRegression(C=0.055, max_iter=1500, class_weight=\"balanced\")\n",
    "f1_scorer = make_scorer(f1_score, pos_label=1)\n",
    "scores = cross_val_score(modelo, X, y, cv=5, scoring=f1_scorer)\n",
    "\n",
    "print(f\"\\n=== Validación cruzada (F1, clase tóxica) ===\")\n",
    "print(f\"F1-score promedio: {scores.mean():.3f}\")\n",
    "print(f\"Desviación estándar: {scores.std():.3f}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "modelo.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = modelo.predict(X_train)\n",
    "print(\"\\n=== Evaluación en TRAIN ===\")\n",
    "print(confusion_matrix(y_train, y_train_pred))\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "y_test_pred = modelo.predict(X_test)\n",
    "print(\"\\n=== Evaluación en TEST ===\")\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "f1_train = f1_score(y_train, y_train_pred, pos_label=1)\n",
    "f1_test = f1_score(y_test, y_test_pred, pos_label=1)\n",
    "\n",
    "overfitting_f1_pct = ((f1_train - f1_test) / f1_train) * 100 if f1_train != 0 else 0.0\n",
    "print(f\"F1-score TRAIN (clase tóxica): {f1_train:.3f}\")\n",
    "print(f\"F1-score TEST  (clase tóxica): {f1_test:.3f}\")\n",
    "print(f\"Overfitting (basado en F1): {overfitting_f1_pct:.2f}%\")\n",
    "\n",
    "def predict_true_false(prob):\n",
    "    return prob >= 0.5\n",
    "\n",
    "def predecir_toxicidad(texto):\n",
    "    texto_limpio = limpiar_texto(texto)\n",
    "    texto_vect = vectorizer.transform([texto_limpio])\n",
    "    prob = modelo.predict_proba(texto_vect)\n",
    "    print(f\"Probabilidades de cada clase [no tóxico, tóxico]: {prob}\")\n",
    "    prob_toxico = prob[0][1]\n",
    "    print(f\"Probabilidad de toxicidad: {prob_toxico:.3f}\")\n",
    "    prediccion_final = predict_true_false(prob_toxico)\n",
    "    return {\n",
    "        \"prediccion\": prediccion_final,\n",
    "        \"probabilidad_toxico\": round(prob_toxico, 3)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predecir_toxicidad(\"stupid\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
