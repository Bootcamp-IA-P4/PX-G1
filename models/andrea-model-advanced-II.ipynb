{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7E4UFtHnGcP"
   },
   "source": [
    "#¬†Modelos Avanzados y Ajuste de Hiperpar√°metros\n",
    "\n",
    "Este notebook implementa modelos avanzados con t√©cnicas de regularizaci√≥n,\n",
    "embeddings y deep learning para mejorar la robustez de las predicciones\n",
    "y controlar el overfitting.\n",
    "\n",
    "Objetivos:\n",
    "- Ajuste de hiperpar√°metros con validaci√≥n cruzada\n",
    "- Implementaci√≥n de embeddings pre-entrenados\n",
    "- Modelos de deep learning con regularizaci√≥n\n",
    "- Control de overfitting (diferencia train-test < 5%)\n",
    "- Ensemble methods para mayor robustez\n",
    "- Validaci√≥n exhaustiva y early stopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7DlI3pWinmFy"
   },
   "source": [
    "## Librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JiC-jPfznkfe",
    "outputId": "c5ec6bf1-693c-47f9-deab-9772b63a7a7b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Librer√≠as adicionales para modelos avanzados\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV, RandomizedSearchCV, cross_val_score,\n",
    "    StratifiedKFold, validation_curve, learning_curve\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score,\n",
    "    roc_curve, precision_recall_curve, f1_score, accuracy_score\n",
    ")\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, GradientBoostingClassifier,\n",
    "    VotingClassifier, BaggingClassifier\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression, ElasticNet\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Deep Learning y Embeddings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "try:\n",
    "    import gensim.downloader as api\n",
    "except ImportError:\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install --upgrade gensim\n",
    "    import gensim.downloader as api\n",
    "\n",
    "# Utilidades\n",
    "import pickle\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import joblib\n",
    "\n",
    "print(\"üìö Librer√≠as avanzadas cargadas exitosamente\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y5d2xe-SnvxF"
   },
   "source": [
    "## Pipeline principal para un modelado robusto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RutJnk0eoIWz"
   },
   "outputs": [],
   "source": [
    "class AdvancedModelPipeline:\n",
    "    \"\"\"Pipeline avanzado para modelos robustos con control de overfitting\"\"\"\n",
    "\n",
    "    def __init__(self, overfitting_threshold=0.05, random_state=42):\n",
    "        self.overfitting_threshold = overfitting_threshold\n",
    "        self.random_state = random_state\n",
    "        self.models = {}\n",
    "        self.best_params = {}\n",
    "        self.results = {}\n",
    "        self.embeddings_cache = {}\n",
    "\n",
    "    def setup_hyperparameter_grids(self):\n",
    "        \"\"\"Configurar grids de hiperpar√°metros para optimizaci√≥n\"\"\"\n",
    "        return {\n",
    "            'logistic_regression': {\n",
    "                'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "                'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "                'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "                'max_iter': [1000, 2000],\n",
    "                'class_weight': ['balanced', None]\n",
    "            },\n",
    "            'random_forest': {\n",
    "                'n_estimators': [50, 100, 200, 300],\n",
    "                'max_depth': [3, 5, 7, 10, None],\n",
    "                'min_samples_split': [2, 5, 10, 20],\n",
    "                'min_samples_leaf': [1, 2, 4, 8],\n",
    "                'max_features': ['sqrt', 'log2', None],\n",
    "                'class_weight': ['balanced', None]\n",
    "            },\n",
    "            'gradient_boosting': {\n",
    "                'n_estimators': [50, 100, 200],\n",
    "                'learning_rate': [0.01, 0.1, 0.2],\n",
    "                'max_depth': [3, 5, 7],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'min_samples_leaf': [1, 2, 4],\n",
    "                'subsample': [0.8, 0.9, 1.0]\n",
    "            },\n",
    "            'svm': {\n",
    "                'C': [0.1, 1, 10, 100],\n",
    "                'kernel': ['linear', 'rbf'],\n",
    "                'gamma': ['scale', 'auto', 0.001, 0.01, 0.1],\n",
    "                'class_weight': ['balanced', None]\n",
    "            },\n",
    "            'mlp': {\n",
    "                'hidden_layer_sizes': [(50,), (100,), (100, 50), (200, 100)],\n",
    "                'activation': ['relu', 'tanh'],\n",
    "                'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "                'learning_rate': ['constant', 'adaptive'],\n",
    "                'max_iter': [500, 1000],\n",
    "                'early_stopping': [True],\n",
    "                'validation_fraction': [0.1, 0.2]\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def hyperparameter_tuning(self, X_train, y_train, model_name='logistic_regression',\n",
    "                            cv_folds=5, n_iter=50, use_randomized=True):\n",
    "        \"\"\"Ajuste de hiperpar√°metros con validaci√≥n cruzada\"\"\"\n",
    "        print(f\"üìö Ajustando hiperpar√°metros para {model_name}...\")\n",
    "\n",
    "        grids = self.setup_hyperparameter_grids()\n",
    "        if model_name not in grids:\n",
    "            print(f\"Grid no definido para {model_name}\")\n",
    "            return None\n",
    "\n",
    "        # Seleccionar modelo base\n",
    "        base_models = {\n",
    "            'logistic_regression': LogisticRegression(random_state=self.random_state, solver='saga'),\n",
    "            'random_forest': RandomForestClassifier(random_state=self.random_state),\n",
    "            'gradient_boosting': GradientBoostingClassifier(random_state=self.random_state),\n",
    "            'svm': SVC(random_state=self.random_state, probability=True),\n",
    "            'mlp': MLPClassifier(random_state=self.random_state)\n",
    "        }\n",
    "\n",
    "        model = base_models[model_name]\n",
    "        param_grid = grids[model_name]\n",
    "\n",
    "        # Configurar b√∫squeda\n",
    "        cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=self.random_state)\n",
    "\n",
    "        if use_randomized:\n",
    "            search = RandomizedSearchCV(\n",
    "                model, param_grid, n_iter=n_iter, cv=cv,\n",
    "                scoring='f1', n_jobs=-1, random_state=self.random_state,\n",
    "                return_train_score=True\n",
    "            )\n",
    "        else:\n",
    "            search = GridSearchCV(\n",
    "                model, param_grid, cv=cv, scoring='f1',\n",
    "                n_jobs=-1, return_train_score=True\n",
    "            )\n",
    "\n",
    "        # Ejecutar b√∫squeda\n",
    "        start_time = time.time()\n",
    "        search.fit(X_train, y_train)\n",
    "        tuning_time = time.time() - start_time\n",
    "\n",
    "        # Guardar resultados\n",
    "        self.best_params[model_name] = search.best_params_\n",
    "\n",
    "        # Verificar overfitting\n",
    "        train_score = search.cv_results_['mean_train_score'][search.best_index_]\n",
    "        val_score = search.cv_results_['mean_test_score'][search.best_index_]\n",
    "        overfitting_gap = train_score - val_score\n",
    "\n",
    "        results = {\n",
    "            'best_model': search.best_estimator_,\n",
    "            'best_params': search.best_params_,\n",
    "            'best_score': search.best_score_,\n",
    "            'train_score': train_score,\n",
    "            'val_score': val_score,\n",
    "            'overfitting_gap': overfitting_gap,\n",
    "            'tuning_time': tuning_time,\n",
    "            'cv_results': search.cv_results_\n",
    "        }\n",
    "\n",
    "        print(f\"  ‚úÖ Mejor score: {search.best_score_:.4f}\")\n",
    "        print(f\"  üìä Gap overfitting: {overfitting_gap:.4f}\")\n",
    "        print(f\"  ‚è±Ô∏è Tiempo: {tuning_time:.2f}s\")\n",
    "\n",
    "        if overfitting_gap > self.overfitting_threshold:\n",
    "            print(f\"  ‚ö†Ô∏è Posible overfitting detectado (gap > {self.overfitting_threshold})\")\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6boB1U6oNyN"
   },
   "source": [
    "## Extractor de embaddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cd80r1RtoVFp"
   },
   "outputs": [],
   "source": [
    "class EmbeddingExtractor:\n",
    "    \"\"\"Extractor de embeddings pre-entrenados\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.word2vec_model = None\n",
    "        self.bert_tokenizer = None\n",
    "        self.bert_model = None\n",
    "\n",
    "    def load_word2vec(self, model_name='word2vec-google-news-300'):\n",
    "        \"\"\"Cargar Word2Vec pre-entrenado\"\"\"\n",
    "        print(f\"üìö Cargando Word2Vec: {model_name}...\")\n",
    "        try:\n",
    "            self.word2vec_model = api.load(model_name)\n",
    "            print(\"‚úÖ Word2Vec cargado exitosamente\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error cargando Word2Vec: {e}\")\n",
    "            return False\n",
    "\n",
    "    def load_bert(self, model_name='bert-base-uncased'):\n",
    "        \"\"\"Cargar BERT pre-entrenado\"\"\"\n",
    "        print(f\"üìö Cargando BERT: {model_name}...\")\n",
    "        try:\n",
    "            self.bert_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            self.bert_model = AutoModel.from_pretrained(model_name)\n",
    "            print(\"‚úÖ BERT cargado exitosamente\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error cargando BERT: {e}\")\n",
    "            return False\n",
    "\n",
    "    def get_word2vec_embeddings(self, texts, vector_size=300):\n",
    "        \"\"\"Extraer embeddings Word2Vec\"\"\"\n",
    "        if self.word2vec_model is None:\n",
    "            print(\"‚ùå Word2Vec no est√° cargado\")\n",
    "            return None\n",
    "\n",
    "        embeddings = []\n",
    "        for text in texts:\n",
    "            words = text.split()\n",
    "            word_vectors = []\n",
    "\n",
    "            for word in words:\n",
    "                if word in self.word2vec_model:\n",
    "                    word_vectors.append(self.word2vec_model[word])\n",
    "\n",
    "            if word_vectors:\n",
    "                # Promedio de vectores de palabras\n",
    "                text_embedding = np.mean(word_vectors, axis=0)\n",
    "            else:\n",
    "                # Vector cero si no hay palabras conocidas\n",
    "                text_embedding = np.zeros(vector_size)\n",
    "\n",
    "            embeddings.append(text_embedding)\n",
    "\n",
    "        return np.array(embeddings)\n",
    "\n",
    "    def get_bert_embeddings(self, texts, max_length=128):\n",
    "        \"\"\"Extraer embeddings BERT\"\"\"\n",
    "        if self.bert_model is None or self.bert_tokenizer is None:\n",
    "            print(\"‚ùå BERT no est√° cargado\")\n",
    "            return None\n",
    "\n",
    "        embeddings = []\n",
    "        self.bert_model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for text in texts:\n",
    "                # Tokenizar\n",
    "                inputs = self.bert_tokenizer(\n",
    "                    text, return_tensors='pt',\n",
    "                    max_length=max_length, truncation=True, padding=True\n",
    "                )\n",
    "\n",
    "                # Obtener embeddings\n",
    "                outputs = self.bert_model(**inputs)\n",
    "                # Usar el token [CLS] como representaci√≥n de la oraci√≥n\n",
    "                embedding = outputs.last_hidden_state[:, 0, :].numpy().flatten()\n",
    "                embeddings.append(embedding)\n",
    "\n",
    "        return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "igwQjyyEoWN5"
   },
   "source": [
    "## Clasificador de Deep Learning con regularizaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G65SwYLnodby"
   },
   "outputs": [],
   "source": [
    "class DeepLearningClassifier:\n",
    "    \"\"\"Clasificador de Deep Learning con regularizaci√≥n\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_sizes=[128, 64], dropout=0.3,\n",
    "                 learning_rate=0.001, weight_decay=0.01):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.dropout = dropout\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.model = None\n",
    "        self.history = {}\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"Construir red neuronal con regularizaci√≥n\"\"\"\n",
    "        layers = []\n",
    "        prev_size = self.input_size\n",
    "\n",
    "        for hidden_size in self.hidden_sizes:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_size, hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(hidden_size),\n",
    "                nn.Dropout(self.dropout)\n",
    "            ])\n",
    "            prev_size = hidden_size\n",
    "\n",
    "        # Capa de salida\n",
    "        layers.append(nn.Linear(prev_size, 1))\n",
    "        layers.append(nn.Sigmoid())\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        return self.model\n",
    "\n",
    "    def train(self, X_train, y_train, X_val=None, y_val=None,\n",
    "              epochs=100, batch_size=32, early_stopping_patience=10):\n",
    "        \"\"\"Entrenar modelo con early stopping\"\"\"\n",
    "\n",
    "        if self.model is None:\n",
    "            self.build_model()\n",
    "\n",
    "        # Preparar datos\n",
    "        X_train_tensor = torch.FloatTensor(X_train)\n",
    "        y_train_tensor = torch.FloatTensor(y_train).unsqueeze(1)\n",
    "\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # Configurar optimizador y p√©rdida\n",
    "        optimizer = optim.Adam(self.model.parameters(),\n",
    "                             lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        criterion = nn.BCELoss()\n",
    "\n",
    "        # Variables para early stopping\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        best_model_state = None\n",
    "\n",
    "        # Historia de entrenamiento\n",
    "        self.history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "        print(f\"üìö Iniciando entrenamiento por {epochs} √©pocas...\")\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Entrenamiento\n",
    "            self.model.train()\n",
    "            train_loss = 0\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                predicted = (outputs > 0.5).float()\n",
    "                train_total += batch_y.size(0)\n",
    "                train_correct += (predicted == batch_y).sum().item()\n",
    "\n",
    "            # M√©tricas de entrenamiento\n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            train_accuracy = train_correct / train_total\n",
    "\n",
    "            self.history['train_loss'].append(avg_train_loss)\n",
    "            self.history['train_acc'].append(train_accuracy)\n",
    "\n",
    "            # Validaci√≥n\n",
    "            val_loss, val_accuracy = 0, 0\n",
    "            if X_val is not None and y_val is not None:\n",
    "                self.model.eval()\n",
    "                with torch.no_grad():\n",
    "                    X_val_tensor = torch.FloatTensor(X_val)\n",
    "                    y_val_tensor = torch.FloatTensor(y_val).unsqueeze(1)\n",
    "\n",
    "                    val_outputs = self.model(X_val_tensor)\n",
    "                    val_loss = criterion(val_outputs, y_val_tensor).item()\n",
    "\n",
    "                    val_predicted = (val_outputs > 0.5).float()\n",
    "                    val_accuracy = (val_predicted == y_val_tensor).sum().item() / len(y_val)\n",
    "\n",
    "                self.history['val_loss'].append(val_loss)\n",
    "                self.history['val_acc'].append(val_accuracy)\n",
    "\n",
    "                # Early stopping\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    patience_counter = 0\n",
    "                    best_model_state = self.model.state_dict().copy()\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "\n",
    "                if patience_counter >= early_stopping_patience:\n",
    "                    print(f\"üìö Early stopping en √©poca {epoch+1}\")\n",
    "                    break\n",
    "\n",
    "            # Logging cada 10 √©pocas\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"√âpoca {epoch+1}/{epochs} - \"\n",
    "                      f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f}\")\n",
    "                if X_val is not None:\n",
    "                    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "        # Restaurar mejor modelo\n",
    "        if best_model_state is not None:\n",
    "            self.model.load_state_dict(best_model_state)\n",
    "\n",
    "        print(\"‚úÖ Entrenamiento completado\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Realizar predicciones\"\"\"\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.FloatTensor(X)\n",
    "            outputs = self.model(X_tensor)\n",
    "            return outputs.numpy().flatten()\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Obtener probabilidades\"\"\"\n",
    "        return self.predict(X)\n",
    "\n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Visualizar historia de entrenamiento\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "        # P√©rdida\n",
    "        ax1.plot(self.history['train_loss'], label='Train Loss')\n",
    "        if self.history['val_loss']:\n",
    "            ax1.plot(self.history['val_loss'], label='Validation Loss')\n",
    "        ax1.set_title('P√©rdida durante el entrenamiento')\n",
    "        ax1.set_xlabel('√âpoca')\n",
    "        ax1.set_ylabel('P√©rdida')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "\n",
    "        # Precisi√≥n\n",
    "        ax2.plot(self.history['train_acc'], label='Train Accuracy')\n",
    "        if self.history['val_acc']:\n",
    "            ax2.plot(self.history['val_acc'], label='Validation Accuracy')\n",
    "        ax2.set_title('Precisi√≥n durante el entrenamiento')\n",
    "        ax2.set_xlabel('√âpoca')\n",
    "        ax2.set_ylabel('Precisi√≥n')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUDACnIUoi6b"
   },
   "source": [
    "## Constructor de modelos ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PgUNZWk9ojIw"
   },
   "outputs": [],
   "source": [
    "class EnsembleBuilder:\n",
    "    \"\"\"Constructor de modelos ensemble\"\"\"\n",
    "\n",
    "    def __init__(self, base_models=None):\n",
    "        self.base_models = base_models or {}\n",
    "        self.ensemble_models = {}\n",
    "\n",
    "    def create_voting_ensemble(self, models_dict, voting='hard'):\n",
    "        \"\"\"Crear ensemble por votaci√≥n\"\"\"\n",
    "        estimators = [(name, model) for name, model in models_dict.items()]\n",
    "        ensemble = VotingClassifier(estimators=estimators, voting=voting)\n",
    "        return ensemble\n",
    "\n",
    "    def create_bagging_ensemble(self, base_model, n_estimators=10):\n",
    "        \"\"\"Crear ensemble por bagging\"\"\"\n",
    "        ensemble = BaggingClassifier(\n",
    "            base_estimator=base_model,\n",
    "            n_estimators=n_estimators,\n",
    "            random_state=42\n",
    "        )\n",
    "        return ensemble\n",
    "\n",
    "    def create_stacking_ensemble(self, base_models, meta_model=None):\n",
    "        \"\"\"Crear ensemble por stacking (implementaci√≥n simple)\"\"\"\n",
    "        if meta_model is None:\n",
    "            meta_model = LogisticRegression()\n",
    "\n",
    "        # Esta es una implementaci√≥n simplificada\n",
    "        # En producci√≥n se usar√≠a StackingClassifier de sklearn\n",
    "        return VotingClassifier(\n",
    "            estimators=[(name, model) for name, model in base_models.items()],\n",
    "            voting='soft'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HC4KvF2Koqez"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yvrWMjf9oq42"
   },
   "outputs": [],
   "source": [
    "class OverfittingAnalyzer:\n",
    "    \"\"\"Analizador de overfitting y robustez\"\"\"\n",
    "\n",
    "    def __init__(self, threshold=0.05):\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def analyze_learning_curves(self, model, X, y, cv=5):\n",
    "        \"\"\"Analizar curvas de aprendizaje\"\"\"\n",
    "        train_sizes, train_scores, val_scores = learning_curve(\n",
    "            model, X, y, cv=cv, n_jobs=-1,\n",
    "            train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        train_mean = np.mean(train_scores, axis=1)\n",
    "        train_std = np.std(train_scores, axis=1)\n",
    "        val_mean = np.mean(val_scores, axis=1)\n",
    "        val_std = np.std(val_scores, axis=1)\n",
    "\n",
    "        # Visualizar\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(train_sizes, train_mean, 'o-', label='Training Score')\n",
    "        plt.fill_between(train_sizes, train_mean - train_std,\n",
    "                        train_mean + train_std, alpha=0.1)\n",
    "\n",
    "        plt.plot(train_sizes, val_mean, 'o-', label='Validation Score')\n",
    "        plt.fill_between(train_sizes, val_mean - val_std,\n",
    "                        val_mean + val_std, alpha=0.1)\n",
    "\n",
    "        plt.xlabel('Training Set Size')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title('Learning Curves')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "        # Calcular gap final\n",
    "        final_gap = train_mean[-1] - val_mean[-1]\n",
    "\n",
    "        return {\n",
    "            'train_sizes': train_sizes,\n",
    "            'train_scores': {'mean': train_mean, 'std': train_std},\n",
    "            'val_scores': {'mean': val_mean, 'std': val_std},\n",
    "            'final_gap': final_gap,\n",
    "            'overfitting_detected': final_gap > self.threshold\n",
    "        }\n",
    "\n",
    "    def analyze_validation_curves(self, model, X, y, param_name, param_range):\n",
    "        \"\"\"Analizar curvas de validaci√≥n para un hiperpar√°metro\"\"\"\n",
    "        train_scores, val_scores = validation_curve(\n",
    "            model, X, y, param_name=param_name, param_range=param_range,\n",
    "            cv=5, scoring='f1', n_jobs=-1\n",
    "        )\n",
    "\n",
    "        train_mean = np.mean(train_scores, axis=1)\n",
    "        train_std = np.std(train_scores, axis=1)\n",
    "        val_mean = np.mean(val_scores, axis=1)\n",
    "        val_std = np.std(val_scores, axis=1)\n",
    "\n",
    "        # Visualizar\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.semilogx(param_range, train_mean, 'o-', label='Training Score')\n",
    "        plt.fill_between(param_range, train_mean - train_std,\n",
    "                        train_mean + train_std, alpha=0.1)\n",
    "\n",
    "        plt.semilogx(param_range, val_mean, 'o-', label='Validation Score')\n",
    "        plt.fill_between(param_range, val_mean - val_std,\n",
    "                        val_mean + val_std, alpha=0.1)\n",
    "\n",
    "        plt.xlabel(param_name)\n",
    "        plt.ylabel('Score')\n",
    "        plt.title(f'Validation Curves - {param_name}')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "        # Encontrar valor √≥ptimo\n",
    "        optimal_idx = np.argmax(val_mean)\n",
    "        optimal_value = param_range[optimal_idx]\n",
    "        optimal_gap = train_mean[optimal_idx] - val_mean[optimal_idx]\n",
    "\n",
    "        return {\n",
    "            'param_range': param_range,\n",
    "            'train_scores': {'mean': train_mean, 'std': train_std},\n",
    "            'val_scores': {'mean': val_mean, 'std': val_std},\n",
    "            'optimal_value': optimal_value,\n",
    "            'optimal_gap': optimal_gap\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pss3SssZrEHk"
   },
   "source": [
    "## Implementaci√≥n del pipeline a los datos preprocesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l6ruGV0XrHti",
    "outputId": "0eebcc18-22f4-46c9-df46-42a0bc23ac70"
   },
   "outputs": [],
   "source": [
    "# üì• Cargar datos reales del preprocesamiento\n",
    "print(\"üìö Cargando datos preprocesados reales...\")\n",
    "\n",
    "df_X_train_text = pd.read_csv('text_cleaned/X_train.csv').squeeze()\n",
    "df_X_test_text = pd.read_csv('text_cleaned/X_test.csv').squeeze()\n",
    "\n",
    "df_y_train = pd.read_csv('text_cleaned/y_train.csv')['IsToxic']\n",
    "df_y_test = pd.read_csv('text_cleaned/y_test.csv')['IsToxic']\n",
    "\n",
    "df_X_train_scaled = pd.read_csv('numeric_features/X_train_scaled.csv')\n",
    "df_X_test_scaled = pd.read_csv('numeric_features/X_test_scaled.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "YRkNt1_Nr9bx",
    "outputId": "52826cb5-4aa6-4854-ec60-e772e8e5061f"
   },
   "outputs": [],
   "source": [
    "# üèÉ Ejecutar pipeline completo\n",
    "\n",
    "def run_advanced_pipeline():\n",
    "    print(\"üöÄ INICIANDO PIPELINE AVANZADO\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # 1. Inicializar componentes\n",
    "    pipeline = AdvancedModelPipeline(overfitting_threshold=0.05)\n",
    "    embedder = EmbeddingExtractor()\n",
    "    analyzer = OverfittingAnalyzer(threshold=0.05)\n",
    "\n",
    "    # 2. Confirmaci√≥n de datos cargados\n",
    "    print(\"\\nüìö Datos cargados correctamente\")\n",
    "\n",
    "    # 3. Ajuste de hiperpar√°metros\n",
    "    print(\"\\nüìö FASE 1: AJUSTE DE HIPERPAR√ÅMETROS\")\n",
    "    print(\"-\" * 40)\n",
    "    tuned_results = {}\n",
    "    for model_name in ['logistic_regression', 'random_forest']:\n",
    "        result = pipeline.hyperparameter_tuning(\n",
    "            df_X_train_scaled, df_y_train, model_name=model_name, n_iter=10\n",
    "        )\n",
    "        tuned_results[model_name] = result\n",
    "\n",
    "    # 4. Embeddings\n",
    "    print(\"\\nüìö FASE 2: EXTRACCI√ìN DE EMBEDDINGS\")\n",
    "    print(\"-\" * 40)\n",
    "    if embedder.load_word2vec():\n",
    "        X_train_w2v = embedder.get_word2vec_embeddings(df_X_train_text)\n",
    "        X_test_w2v = embedder.get_word2vec_embeddings(df_X_test_text)\n",
    "\n",
    "    # 5. Deep Learning\n",
    "    print(\"\\nüìö FASE 3: DEEP LEARNING\")\n",
    "    print(\"-\" * 40)\n",
    "    dl_classifier = DeepLearningClassifier(input_size=df_X_train_scaled.shape[1])\n",
    "    dl_classifier.train(df_X_train_scaled.values, df_y_train.values, df_X_test_scaled.values, df_y_test.values,epochs=20)\n",
    "    dl_classifier.plot_training_history()\n",
    "\n",
    "    # 6. Ensemble\n",
    "    print(\"\\nüìö FASE 4: MODELOS ENSEMBLE\")\n",
    "    print(\"-\" * 40)\n",
    "    from sklearn.ensemble import VotingClassifier\n",
    "    best_models = {k: v['best_model'] for k, v in tuned_results.items()}\n",
    "    ensemble_builder = EnsembleBuilder()\n",
    "    voting_ensemble = ensemble_builder.create_voting_ensemble(best_models)\n",
    "    voting_ensemble.fit(df_X_train_scaled, df_y_train)\n",
    "    y_pred = voting_ensemble.predict(df_X_test_scaled)\n",
    "    f1 = f1_score(df_y_test, y_pred)\n",
    "    print(f\"\\n‚úÖ F1-Score Ensemble: {f1:.4f}\")\n",
    "\n",
    "    # 7. An√°lisis de overfitting\n",
    "    print(\"\\nüìö FASE 5: AN√ÅLISIS DE OVERFITTING\")\n",
    "    print(\"-\" * 40)\n",
    "    for name, model in best_models.items():\n",
    "        print(f\"\\nüîç Analizando curvas de aprendizaje para: {name}\")\n",
    "        curves = analyzer.analyze_learning_curves(model, df_X_train_scaled, df_y_train)\n",
    "        print(f\"Gap de overfitting ({name}): {curves['final_gap']:.4f}\")\n",
    "\n",
    "    print(\"\\nüéâ Pipeline completo ejecutado con √©xito\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_advanced_pipeline()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
