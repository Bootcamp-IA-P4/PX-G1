{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# üßº Preprocesamiento y limpieza de texto\n",
    "\n",
    "Este notebook implementa un pipeline completo de preprocesamiento de texto\n",
    "espec√≠ficamente dise√±ado para comentarios de YouTube basado en el EDA realizado.\n",
    "\n",
    "Basado en el an√°lisis previo:\n",
    "- 1,000 comentarios con longitud promedio de 186 caracteres\n",
    "- 33.8 palabras promedio por comentario\n",
    "- 46.2% de comentarios t√≥xicos\n",
    "- Necesidad de limpieza espec√≠fica para comentarios de redes sociales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Funciones principales del preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Librer√≠as para procesamiento de texto\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "# Para manejo de URLs y HTML\n",
    "from urllib.parse import urlparse\n",
    "import html\n",
    "\n",
    "# Para visualizaci√≥n del progreso\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# Configuraci√≥n de pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Descargas necesarias de NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_nltk_resources():\n",
    "    \"\"\"Descarga los recursos necesarios de NLTK - versi√≥n actualizada\"\"\"\n",
    "    nltk_downloads = [\n",
    "        'punkt', 'punkt_tab', 'stopwords', 'wordnet', 'averaged_perceptron_tagger',\n",
    "        'maxent_ne_chunker', 'words', 'omw-1.4', 'averaged_perceptron_tagger_eng'\n",
    "    ]\n",
    "    \n",
    "    print(\"üì• Descargando recursos de NLTK...\")\n",
    "    for resource in nltk_downloads:\n",
    "        try:\n",
    "            nltk.download(resource, quiet=True)\n",
    "            print(f\"‚úÖ {resource} descargado\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è No se pudo descargar {resource}: {str(e)}\")\n",
    "    print(\"‚úÖ Recursos de NLTK listos\")\n",
    "\n",
    "# Ejecutar descargas\n",
    "download_nltk_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Configuraci√≥n inicial\n",
    "\n",
    "Antes de ejecutar la siguiente celda, es importante haber instalado spaCy:\n",
    "```python\n",
    "pip install spacy\n",
    "```\n",
    "\n",
    "Una vez instalado, es necesario descargar el modelo de idioma en ingl√©s que se usa en este pipeline. Para ello, ejecutar: \n",
    "\n",
    "```python\n",
    "python -m spacy download en_core_web_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    SPACY_AVAILABLE = True\n",
    "    print(\"‚úÖ SpaCy modelo cargado correctamente\")\n",
    "except:\n",
    "    SPACY_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è SpaCy no disponible, usando solo NLTK\")\n",
    "\n",
    "# Inicializar herramientas de NLTK\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Cargar stopwords con manejo de errores\n",
    "try:\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    print(\"‚úÖ Stopwords cargadas correctamente\")\n",
    "except:\n",
    "    # Stopwords b√°sicas en caso de error\n",
    "    stop_words = {'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \n",
    "                 \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', \n",
    "                 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", \n",
    "                 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', \n",
    "                 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', \n",
    "                 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \n",
    "                 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', \n",
    "                 'while', 'of', 'at', 'by', 'for', 'with', 'through', 'during', 'before', 'after', \n",
    "                 'above', 'below', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', \n",
    "                 'further', 'then', 'once'}\n",
    "    print(\"‚ö†Ô∏è Usando stopwords b√°sicas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Funciones de limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html_entities(text):\n",
    "    \"\"\"Limpia entidades HTML y caracteres especiales\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Decodificar entidades HTML\n",
    "    text = html.unescape(text)\n",
    "    \n",
    "    # Limpiar entidades HTML comunes manualmente\n",
    "    html_entities = {\n",
    "        '&amp;': '&', '&lt;': '<', '&gt;': '>', '&quot;': '\"',\n",
    "        '&#39;': \"'\", '&nbsp;': ' ', '&copy;': '', '&reg;': ''\n",
    "    }\n",
    "    \n",
    "    for entity, replacement in html_entities.items():\n",
    "        text = text.replace(entity, replacement)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    \"\"\"Elimina URLs y enlaces de texto\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Patrones para URLs\n",
    "    url_patterns = [\n",
    "        r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',\n",
    "        r'www\\.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',\n",
    "        r'(?:[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?\\.)+[a-zA-Z]{2,}',\n",
    "    ]\n",
    "    \n",
    "    for pattern in url_patterns:\n",
    "        text = re.sub(pattern, ' [URL] ', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_social_media_artifacts(text):\n",
    "    \"\"\"Limpia artefactos espec√≠ficos de redes sociales\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Menciones de usuario\n",
    "    text = re.sub(r'@[A-Za-z0-9_]+', ' [USER] ', text)\n",
    "    \n",
    "    # Hashtags (mantener el contenido pero quitar el #)\n",
    "    text = re.sub(r'#([A-Za-z0-9_]+)', r' \\1 ', text)\n",
    "    \n",
    "    # Repeticiones excesivas de caracteres (ej: \"sooooo\" -> \"so\")\n",
    "    text = re.sub(r'(.)\\1{3,}', r'\\1\\1', text)\n",
    "    \n",
    "    # M√∫ltiples signos de puntuaci√≥n\n",
    "    text = re.sub(r'[!]{2,}', '!', text)\n",
    "    text = re.sub(r'[?]{2,}', '?', text)\n",
    "    text = re.sub(r'[.]{3,}', '...', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis_and_special_chars(text):\n",
    "    \"\"\"Elimina emojis y caracteres especiales\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Patr√≥n para emojis (rango Unicode amplio)\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # s√≠mbolos & pictogramas\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # transporte & s√≠mbolos del mapa\n",
    "        \"\\U0001F1E0-\\U0001F1FF\"  # banderas (iOS)\n",
    "        \"\\U00002702-\\U000027B0\"  # dingbats\n",
    "        \"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\", flags=re.UNICODE\n",
    "    )\n",
    "    \n",
    "    text = emoji_pattern.sub(' [EMOJI] ', text)\n",
    "    \n",
    "    # Eliminar caracteres de control y no ASCII (excepto espacios y algunos signos)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    \n",
    "    # Limpiar caracteres especiales pero mantener puntuaci√≥n b√°sica\n",
    "    text = re.sub(r'[^\\w\\s\\.\\!\\?\\,\\;\\:\\'\\\"\\-\\(\\)]', ' ', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_contractions(text):\n",
    "    \"\"\"Expande contracciones comunes en ingl√©s\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    contractions = {\n",
    "        \"ain't\": \"is not\", \"aren't\": \"are not\", \"can't\": \"cannot\",\n",
    "        \"couldn't\": \"could not\", \"didn't\": \"did not\", \"doesn't\": \"does not\",\n",
    "        \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\",\n",
    "        \"haven't\": \"have not\", \"he'd\": \"he would\", \"he'll\": \"he will\",\n",
    "        \"he's\": \"he is\", \"i'd\": \"i would\", \"i'll\": \"i will\",\n",
    "        \"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\",\n",
    "        \"it'd\": \"it would\", \"it'll\": \"it will\", \"it's\": \"it is\",\n",
    "        \"let's\": \"let us\", \"shouldn't\": \"should not\", \"that's\": \"that is\",\n",
    "        \"there's\": \"there is\", \"they'd\": \"they would\", \"they'll\": \"they will\",\n",
    "        \"they're\": \"they are\", \"they've\": \"they have\", \"we'd\": \"we would\",\n",
    "        \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\",\n",
    "        \"what's\": \"what is\", \"where's\": \"where is\", \"who's\": \"who is\",\n",
    "        \"won't\": \"will not\", \"wouldn't\": \"would not\", \"you'd\": \"you would\",\n",
    "        \"you'll\": \"you will\", \"you're\": \"you are\", \"you've\": \"you have\"\n",
    "    }\n",
    "    \n",
    "    # Aplicar contracciones (case-insensitive)\n",
    "    for contraction, expansion in contractions.items():\n",
    "        text = re.sub(re.escape(contraction), expansion, text, flags=re.IGNORECASE)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_whitespace(text):\n",
    "    \"\"\"Limpia espacios en blanco excesivos\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Eliminar espacios m√∫ltiples\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Eliminar espacios al inicio y final\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_basic_cleaning(text):\n",
    "    \"\"\"Pipeline b√°sico de limpieza\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convertir a min√∫sculas\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Aplicar todas las funciones de limpieza\n",
    "    text = clean_html_entities(text)\n",
    "    text = remove_urls(text)\n",
    "    text = clean_social_media_artifacts(text)\n",
    "    text = remove_emojis_and_special_chars(text)\n",
    "    text = standardize_contractions(text)\n",
    "    text = clean_whitespace(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Tokenizaci√≥n y filtrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_tokenize(text, remove_stopwords=True, min_word_length=2):\n",
    "    \"\"\"Tokenizaci√≥n avanzada con filtrado\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return []\n",
    "    \n",
    "    # Tokenizar\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Filtrar tokens\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        # Saltar si es stopword (opcional)\n",
    "        if remove_stopwords and token in stop_words:\n",
    "            continue\n",
    "        \n",
    "        # Saltar si es muy corto\n",
    "        if len(token) < min_word_length:\n",
    "            continue\n",
    "        \n",
    "        # Saltar si es solo puntuaci√≥n\n",
    "        if token in string.punctuation:\n",
    "            continue\n",
    "        \n",
    "        # Saltar si es solo n√∫meros\n",
    "        if token.isdigit():\n",
    "            continue\n",
    "        \n",
    "        filtered_tokens.append(token)\n",
    "    \n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_tokens(tokens):\n",
    "    \"\"\"Aplica lematizaci√≥n a los tokens\"\"\"\n",
    "    if not tokens:\n",
    "        return []\n",
    "    \n",
    "    lemmatized = []\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            lemmatized.append(lemmatizer.lemmatize(token))\n",
    "        except:\n",
    "            lemmatized.append(token)\n",
    "    \n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_tokens(tokens):\n",
    "    \"\"\"Aplica stemming a los tokens\"\"\"\n",
    "    if not tokens:\n",
    "        return []\n",
    "    \n",
    "    stemmed = []\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            stemmed.append(stemmer.stem(token))\n",
    "        except:\n",
    "            stemmed.append(token)\n",
    "    \n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_process(text):\n",
    "    \"\"\"Procesamiento con SpaCy (si est√° disponible)\"\"\"\n",
    "    if not SPACY_AVAILABLE or pd.isna(text):\n",
    "        return {'tokens': [], 'lemmas': [], 'pos_tags': [], 'entities': []}\n",
    "    \n",
    "    try:\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # Extraer informaci√≥n\n",
    "        tokens = [token.text for token in doc if not token.is_space]\n",
    "        lemmas = [token.lemma_ for token in doc if not token.is_space]\n",
    "        pos_tags = [(token.text, token.pos_) for token in doc if not token.is_space]\n",
    "        entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "        \n",
    "        return {\n",
    "            'tokens': tokens,\n",
    "            'lemmas': lemmas,\n",
    "            'pos_tags': pos_tags,\n",
    "            'entities': entities\n",
    "        }\n",
    "    except:\n",
    "        return {'tokens': [], 'lemmas': [], 'pos_tags': [], 'entities': []}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Detecci√≥n de patrones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_toxic_patterns(text):\n",
    "    \"\"\"Detecta patrones espec√≠ficos de toxicidad\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return {}\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    patterns = {\n",
    "        'profanity_count': len(re.findall(r'\\b(fuck|shit|damn|bitch|ass|crap)\\b', text_lower)),\n",
    "        'caps_ratio': sum(1 for c in text if c.isupper()) / max(len(text), 1),\n",
    "        'exclamation_count': text.count('!'),\n",
    "        'question_count': text.count('?'),\n",
    "        'racial_keywords': len(re.findall(r'\\b(black|white|race|racist|racial)\\b', text_lower)),\n",
    "        'threat_keywords': len(re.findall(r'\\b(kill|die|death|hurt|violence|attack)\\b', text_lower)),\n",
    "        'has_slurs': bool(re.search(r'\\b(nigger|faggot|retard|whore|slut)\\b', text_lower)),\n",
    "        'repeated_chars': len(re.findall(r'(.)\\1{2,}', text_lower)),\n",
    "        'multiple_punctuation': len(re.findall(r'[!?]{2,}', text))\n",
    "    }\n",
    "    \n",
    "    return patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_features(text):\n",
    "    \"\"\"Extrae caracter√≠sticas num√©ricas del texto\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return {}\n",
    "    \n",
    "    features = {\n",
    "        'char_count': len(text),\n",
    "        'word_count': len(text.split()),\n",
    "        'sentence_count': len(sent_tokenize(text)),\n",
    "        'avg_word_length': np.mean([len(word) for word in text.split()]) if text.split() else 0,\n",
    "        'unique_words': len(set(text.lower().split())),\n",
    "        'lexical_diversity': len(set(text.lower().split())) / max(len(text.split()), 1)\n",
    "    }\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Pipeline principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    \"\"\"Clase principal para preprocesamiento de texto\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 apply_stemming=False,\n",
    "                 apply_lemmatization=True,\n",
    "                 remove_stopwords=True,\n",
    "                 min_word_length=2,\n",
    "                 extract_features=True):\n",
    "        \n",
    "        self.apply_stemming = apply_stemming\n",
    "        self.apply_lemmatization = apply_lemmatization\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.min_word_length = min_word_length\n",
    "        self.extract_features = extract_features\n",
    "        \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Pipeline completo de preprocesamiento\"\"\"\n",
    "        \n",
    "        # Limpieza b√°sica\n",
    "        cleaned_text = apply_basic_cleaning(text)\n",
    "        \n",
    "        # Tokenizaci√≥n\n",
    "        tokens = advanced_tokenize(\n",
    "            cleaned_text, \n",
    "            remove_stopwords=self.remove_stopwords,\n",
    "            min_word_length=self.min_word_length\n",
    "        )\n",
    "        \n",
    "        # Lematizaci√≥n\n",
    "        if self.apply_lemmatization and tokens:\n",
    "            tokens = lemmatize_tokens(tokens)\n",
    "        \n",
    "        # Stemming\n",
    "        if self.apply_stemming and tokens:\n",
    "            tokens = stem_tokens(tokens)\n",
    "        \n",
    "        # Resultado\n",
    "        result = {\n",
    "            'original_text': text,\n",
    "            'cleaned_text': cleaned_text,\n",
    "            'tokens': tokens,\n",
    "            'processed_text': ' '.join(tokens) if tokens else ''\n",
    "        }\n",
    "        \n",
    "        # Caracter√≠sticas adicionales\n",
    "        if self.extract_features:\n",
    "            result.update({\n",
    "                'toxic_patterns': detect_toxic_patterns(text),\n",
    "                'text_features': extract_text_features(text),\n",
    "                'spacy_analysis': spacy_process(cleaned_text) if SPACY_AVAILABLE else None\n",
    "            })\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def process_dataframe(self, df, text_column='Text'):\n",
    "        \"\"\"Procesa un DataFrame completo\"\"\"\n",
    "        \n",
    "        print(f\"üîÑ Procesando {len(df)} comentarios...\")\n",
    "        \n",
    "        # Aplicar preprocesamiento\n",
    "        processed_results = df[text_column].progress_apply(self.preprocess_text)\n",
    "        \n",
    "        # Extraer resultados en columnas separadas\n",
    "        df_processed = df.copy()\n",
    "        df_processed['text_cleaned'] = [result['cleaned_text'] for result in processed_results]\n",
    "        df_processed['text_tokens'] = [result['tokens'] for result in processed_results]\n",
    "        df_processed['text_processed'] = [result['processed_text'] for result in processed_results]\n",
    "        \n",
    "        # Agregar caracter√≠sticas de toxicidad\n",
    "        if self.extract_features:\n",
    "            toxic_features = pd.DataFrame([result['toxic_patterns'] for result in processed_results])\n",
    "            text_features = pd.DataFrame([result['text_features'] for result in processed_results])\n",
    "            \n",
    "            # Agregar prefijos para evitar conflictos\n",
    "            toxic_features.columns = ['toxic_' + col for col in toxic_features.columns]\n",
    "            text_features.columns = ['feature_' + col for col in text_features.columns]\n",
    "            \n",
    "            df_processed = pd.concat([df_processed, toxic_features, text_features], axis=1)\n",
    "        \n",
    "        print(\"‚úÖ Preprocesamiento completado!\")\n",
    "        return df_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## Aplicaci√≥n del preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/youtoxic_english_1000.csv')\n",
    "\n",
    "print(f\"‚úÖ Dataset cargado: {df.shape[0]} comentarios, {df.shape[1]} columnas\")\n",
    "\n",
    "# Identificar etiquetas v√°lidas (basado en el EDA)\n",
    "valid_labels = ['IsToxic', 'IsAbusive', 'IsThreat', 'IsProvocative', \n",
    "                'IsObscene', 'IsHatespeech', 'IsRacist', 'IsNationalist', \n",
    "                'IsReligiousHate']\n",
    "\n",
    "excluded_labels = ['IsSexist', 'IsHomophobic', 'IsRadicalism']\n",
    "\n",
    "print(f\"üéØ Etiquetas v√°lidas para modelado: {len(valid_labels)}\")\n",
    "print(f\"‚ö†Ô∏è Etiquetas excluidas: {len(excluded_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### Configuraci√≥n del preprocesador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_configs = {\n",
    "    'basic': {\n",
    "        'apply_stemming': False,\n",
    "        'apply_lemmatization': True,\n",
    "        'remove_stopwords': True,\n",
    "        'min_word_length': 2,\n",
    "        'extract_features': True\n",
    "    },\n",
    "    \n",
    "    'aggressive': {\n",
    "        'apply_stemming': True,\n",
    "        'apply_lemmatization': True,\n",
    "        'remove_stopwords': True,\n",
    "        'min_word_length': 3,\n",
    "        'extract_features': True\n",
    "    },\n",
    "    \n",
    "    'conservative': {\n",
    "        'apply_stemming': False,\n",
    "        'apply_lemmatization': False,\n",
    "        'remove_stopwords': False,\n",
    "        'min_word_length': 1,\n",
    "        'extract_features': True\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### Aplicar preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_preprocessing_config(df, config_name, config_params):\n",
    "    \"\"\"Aplica una configuraci√≥n espec√≠fica de preprocesamiento\"\"\"\n",
    "    \n",
    "    print(f\"\\nüîÑ Aplicando configuraci√≥n: {config_name.upper()}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Crear preprocessor\n",
    "    preprocessor = TextPreprocessor(**config_params)\n",
    "    \n",
    "    # Procesar dataset\n",
    "    df_processed = preprocessor.process_dataframe(df, text_column='Text')\n",
    "    \n",
    "    # Mostrar estad√≠sticas\n",
    "    print(f\"‚úÖ Procesamiento completado\")\n",
    "    print(f\"üìä Columnas agregadas: {df_processed.shape[1] - df.shape[1]}\")\n",
    "    \n",
    "    # Estad√≠sticas b√°sicas del texto procesado\n",
    "    df_processed['processed_word_count'] = df_processed['text_processed'].str.split().str.len()\n",
    "    \n",
    "    print(f\"üìà Estad√≠sticas del texto procesado:\")\n",
    "    print(f\"   ‚Ä¢ Palabras promedio: {df_processed['processed_word_count'].mean():.1f}\")\n",
    "    print(f\"   ‚Ä¢ Mediana de palabras: {df_processed['processed_word_count'].median():.1f}\")\n",
    "    print(f\"   ‚Ä¢ Comentarios vac√≠os despu√©s del procesamiento: {(df_processed['processed_word_count'] == 0).sum()}\")\n",
    "    \n",
    "    return df_processed\n",
    "\n",
    "# Aplicar configuraci√≥n b√°sica (recomendada para el proyecto)\n",
    "df_processed = apply_preprocessing_config(df, 'basic', preprocessing_configs['basic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "## An√°lisis Post-Procesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "### Librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "### Visualizaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaci√≥n de longitudes antes y despu√©s\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('An√°lisis del Impacto del Preprocesamiento', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Longitud de caracteres\n",
    "axes[0,0].hist(df['Text'].str.len(), bins=50, alpha=0.7, label='Original', color='skyblue')\n",
    "axes[0,0].hist(df_processed['text_cleaned'].str.len(), bins=50, alpha=0.7, label='Limpio', color='orange')\n",
    "axes[0,0].set_title('Distribuci√≥n de Longitud (Caracteres)')\n",
    "axes[0,0].set_xlabel('N√∫mero de Caracteres')\n",
    "axes[0,0].set_ylabel('Frecuencia')\n",
    "axes[0,0].legend()\n",
    "\n",
    "# Longitud de palabras\n",
    "df['original_word_count'] = df['Text'].str.split().str.len()\n",
    "axes[0,1].hist(df['original_word_count'], bins=30, alpha=0.7, label='Original', color='skyblue')\n",
    "axes[0,1].hist(df_processed['processed_word_count'], bins=30, alpha=0.7, label='Procesado', color='lightgreen')\n",
    "axes[0,1].set_title('Distribuci√≥n de Longitud (Palabras)')\n",
    "axes[0,1].set_xlabel('N√∫mero de Palabras')\n",
    "axes[0,1].set_ylabel('Frecuencia')\n",
    "axes[0,1].legend()\n",
    "\n",
    "# Caracter√≠sticas de toxicidad por clase\n",
    "toxic_features = df_processed[df_processed.columns[df_processed.columns.str.startswith('toxic_')]]\n",
    "mean_features_toxic = toxic_features[df_processed['IsToxic'] == 1].mean()\n",
    "mean_features_non_toxic = toxic_features[df_processed['IsToxic'] == 0].mean()\n",
    "\n",
    "x = np.arange(len(mean_features_toxic))\n",
    "width = 0.35\n",
    "\n",
    "axes[1,0].bar(x - width/2, mean_features_toxic, width, label='T√≥xicos', color='red', alpha=0.7)\n",
    "axes[1,0].bar(x + width/2, mean_features_non_toxic, width, label='No T√≥xicos', color='green', alpha=0.7)\n",
    "axes[1,0].set_title('Caracter√≠sticas de Toxicidad por Clase')\n",
    "axes[1,0].set_xlabel('Caracter√≠sticas')\n",
    "axes[1,0].set_ylabel('Valor Promedio')\n",
    "axes[1,0].set_xticks(x)\n",
    "axes[1,0].set_xticklabels([col.replace('toxic_', '') for col in toxic_features.columns], rotation=45)\n",
    "axes[1,0].legend()\n",
    "\n",
    "# Distribuci√≥n de caracter√≠sticas textuales\n",
    "text_features = df_processed[df_processed.columns[df_processed.columns.str.startswith('feature_')]]\n",
    "correlation_matrix = text_features.corr()\n",
    "\n",
    "im = axes[1,1].imshow(correlation_matrix, cmap='coolwarm', aspect='auto', vmin=-1, vmax=1)\n",
    "axes[1,1].set_title('Correlaci√≥n entre Caracter√≠sticas Textuales')\n",
    "axes[1,1].set_xticks(range(len(correlation_matrix.columns)))\n",
    "axes[1,1].set_yticks(range(len(correlation_matrix.columns)))\n",
    "axes[1,1].set_xticklabels([col.replace('feature_', '') for col in correlation_matrix.columns], rotation=45)\n",
    "axes[1,1].set_yticklabels([col.replace('feature_', '') for col in correlation_matrix.columns])\n",
    "\n",
    "# Agregar colorbar\n",
    "plt.colorbar(im, ax=axes[1,1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### An√°lisis del vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear vocabulario completo\n",
    "all_tokens = []\n",
    "for tokens in df_processed['text_tokens']:\n",
    "    if isinstance(tokens, list):\n",
    "        all_tokens.extend(tokens)\n",
    "\n",
    "vocab_counter = Counter(all_tokens)\n",
    "vocab_size = len(vocab_counter)\n",
    "\n",
    "print(f\"‚úÖ Tama√±o del vocabulario: {vocab_size:,} palabras √∫nicas\")\n",
    "print(f\"‚úÖ Total de tokens: {len(all_tokens):,}\")\n",
    "\n",
    "# Top palabras m√°s comunes\n",
    "print(f\"\\nüîù Top 20 palabras m√°s frecuentes:\")\n",
    "for word, count in vocab_counter.most_common(20):\n",
    "    print(f\"   {word}: {count}\")\n",
    "\n",
    "# An√°lisis por toxicidad\n",
    "toxic_tokens = []\n",
    "non_toxic_tokens = []\n",
    "\n",
    "for idx, tokens in enumerate(df_processed['text_tokens']):\n",
    "    if isinstance(tokens, list):\n",
    "        if df_processed.iloc[idx]['IsToxic'] == 1:\n",
    "            toxic_tokens.extend(tokens)\n",
    "        else:\n",
    "            non_toxic_tokens.extend(tokens)\n",
    "\n",
    "toxic_vocab = Counter(toxic_tokens)\n",
    "non_toxic_vocab = Counter(non_toxic_tokens)\n",
    "\n",
    "print(f\"\\nüî• Palabras m√°s comunes en comentarios T√ìXICOS:\")\n",
    "for word, count in toxic_vocab.most_common(15):\n",
    "    print(f\"   {word}: {count}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Palabras m√°s comunes en comentarios NO T√ìXICOS:\")\n",
    "for word, count in non_toxic_vocab.most_common(15):\n",
    "    print(f\"   {word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "## Preparaci√≥n para modelado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar caracter√≠sticas para el modelado\n",
    "feature_columns = [col for col in df_processed.columns if col.startswith(('toxic_', 'feature_'))]\n",
    "text_columns = ['text_cleaned', 'text_processed', 'text_tokens']\n",
    "label_columns = [col for col in valid_labels if col in df_processed.columns]\n",
    "\n",
    "print(f\"‚úÖ Caracter√≠sticas num√©ricas: {len(feature_columns)}\")\n",
    "print(f\"‚úÖ Columnas de texto: {len(text_columns)}\")\n",
    "print(f\"‚úÖ Etiquetas disponibles: {len(label_columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear diferentes versiones del dataset para experimentaci√≥n\n",
    "datasets = {}\n",
    "\n",
    "# 1. Dataset con caracter√≠sticas num√©ricas solamente\n",
    "datasets['numeric_features'] = {\n",
    "    'X': df_processed[feature_columns],\n",
    "    'y': df_processed[label_columns],\n",
    "    'description': 'Solo caracter√≠sticas num√©ricas extra√≠das'\n",
    "}\n",
    "\n",
    "# 2. Dataset con texto procesado para vectorizaci√≥n\n",
    "datasets['text_processed'] = {\n",
    "    'X': df_processed['text_processed'],\n",
    "    'y': df_processed[label_columns],\n",
    "    'description': 'Texto procesado para TF-IDF/Count Vectorizer'\n",
    "}\n",
    "\n",
    "# 3. Dataset con texto limpio para embeddings\n",
    "datasets['text_cleaned'] = {\n",
    "    'X': df_processed['text_cleaned'],\n",
    "    'y': df_processed[label_columns],\n",
    "    'description': 'Texto limpio para word embeddings'\n",
    "}\n",
    "\n",
    "# 4. Dataset combinado (caracter√≠sticas + texto)\n",
    "datasets['combined'] = {\n",
    "    'X_numeric': df_processed[feature_columns],\n",
    "    'X_text': df_processed['text_processed'],\n",
    "    'y': df_processed[label_columns],\n",
    "    'description': 'Caracter√≠sticas num√©ricas + texto procesado'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir en train/test para cada configuraci√≥n\n",
    "print(f\"\\nüîÑ Dividiendo datasets en train/test (80/20)...\")\n",
    "\n",
    "for name, dataset in datasets.items():\n",
    "    if name != 'combined':\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            dataset['X'], dataset['y'], \n",
    "            test_size=0.2, \n",
    "            random_state=42, \n",
    "            stratify=dataset['y']['IsToxic']\n",
    "        )\n",
    "        \n",
    "        datasets[name].update({\n",
    "            'X_train': X_train,\n",
    "            'X_test': X_test,\n",
    "            'y_train': y_train,\n",
    "            'y_test': y_test\n",
    "        })\n",
    "    else:\n",
    "        # Para el dataset combinado, dividir por separado\n",
    "        X_num_train, X_num_test, X_text_train, X_text_test, y_train, y_test = train_test_split(\n",
    "            dataset['X_numeric'], dataset['X_text'], dataset['y'],\n",
    "            test_size=0.2,\n",
    "            random_state=42,\n",
    "            stratify=dataset['y']['IsToxic']\n",
    "        )\n",
    "        \n",
    "        datasets[name].update({\n",
    "            'X_numeric_train': X_num_train,\n",
    "            'X_numeric_test': X_num_test,\n",
    "            'X_text_train': X_text_train,\n",
    "            'X_text_test': X_text_test,\n",
    "            'y_train': y_train,\n",
    "            'y_test': y_test\n",
    "        })\n",
    "\n",
    "print(\"‚úÖ Divisi√≥n completada para todos los datasets\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "### Escalado de caracter√≠sticas num√©ricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalar caracter√≠sticas num√©ricas\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Para datasets con caracter√≠sticas num√©ricas\n",
    "for name in ['numeric_features', 'combined']:\n",
    "    if name in datasets:\n",
    "        if name == 'numeric_features':\n",
    "            X_train_scaled = scaler.fit_transform(datasets[name]['X_train'])\n",
    "            X_test_scaled = scaler.transform(datasets[name]['X_test'])\n",
    "            \n",
    "            datasets[name]['X_train_scaled'] = pd.DataFrame(\n",
    "                X_train_scaled, \n",
    "                columns=datasets[name]['X_train'].columns,\n",
    "                index=datasets[name]['X_train'].index\n",
    "            )\n",
    "            datasets[name]['X_test_scaled'] = pd.DataFrame(\n",
    "                X_test_scaled, \n",
    "                columns=datasets[name]['X_test'].columns,\n",
    "                index=datasets[name]['X_test'].index\n",
    "            )\n",
    "        \n",
    "        elif name == 'combined':\n",
    "            X_num_train_scaled = scaler.fit_transform(datasets[name]['X_numeric_train'])\n",
    "            X_num_test_scaled = scaler.transform(datasets[name]['X_numeric_test'])\n",
    "            \n",
    "            datasets[name]['X_numeric_train_scaled'] = pd.DataFrame(\n",
    "                X_num_train_scaled,\n",
    "                columns=datasets[name]['X_numeric_train'].columns,\n",
    "                index=datasets[name]['X_numeric_train'].index\n",
    "            )\n",
    "            datasets[name]['X_numeric_test_scaled'] = pd.DataFrame(\n",
    "                X_num_test_scaled,\n",
    "                columns=datasets[name]['X_numeric_test'].columns,\n",
    "                index=datasets[name]['X_numeric_test'].index\n",
    "            )\n",
    "\n",
    "print(\"‚úÖ Escalado completado\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "### Guardado de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear directorio de salida\n",
    "output_dir = '../processed_data'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Guardar dataset completo procesado\n",
    "df_processed.to_csv(f'{output_dir}/dataset_processed_complete.csv', index=False)\n",
    "print(f\"‚úÖ Dataset completo guardado: {output_dir}/dataset_processed_complete.csv\")\n",
    "\n",
    "# Guardar datasets divididos\n",
    "for name, dataset in datasets.items():\n",
    "    dataset_dir = f'{output_dir}/{name}'\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "    \n",
    "    # Guardar seg√∫n el tipo de dataset\n",
    "    if name != 'combined':\n",
    "        dataset['X_train'].to_csv(f'{dataset_dir}/X_train.csv', index=False)\n",
    "        dataset['X_test'].to_csv(f'{dataset_dir}/X_test.csv', index=False)\n",
    "        dataset['y_train'].to_csv(f'{dataset_dir}/y_train.csv', index=False)\n",
    "        dataset['y_test'].to_csv(f'{dataset_dir}/y_test.csv', index=False)\n",
    "        \n",
    "        # Guardar versiones escaladas si existen\n",
    "        if 'X_train_scaled' in dataset:\n",
    "            dataset['X_train_scaled'].to_csv(f'{dataset_dir}/X_train_scaled.csv', index=False)\n",
    "            dataset['X_test_scaled'].to_csv(f'{dataset_dir}/X_test_scaled.csv', index=False)\n",
    "    \n",
    "    else:\n",
    "        # Dataset combinado\n",
    "        dataset['X_numeric_train'].to_csv(f'{dataset_dir}/X_numeric_train.csv', index=False)\n",
    "        dataset['X_numeric_test'].to_csv(f'{dataset_dir}/X_numeric_test.csv', index=False)\n",
    "        dataset['X_text_train'].to_csv(f'{dataset_dir}/X_text_train.csv', index=False)\n",
    "        dataset['X_text_test'].to_csv(f'{dataset_dir}/X_text_test.csv', index=False)\n",
    "        dataset['y_train'].to_csv(f'{dataset_dir}/y_train.csv', index=False)\n",
    "        dataset['y_test'].to_csv(f'{dataset_dir}/y_test.csv', index=False)\n",
    "        \n",
    "        # Versiones escaladas\n",
    "        dataset['X_numeric_train_scaled'].to_csv(f'{dataset_dir}/X_numeric_train_scaled.csv', index=False)\n",
    "        dataset['X_numeric_test_scaled'].to_csv(f'{dataset_dir}/X_numeric_test_scaled.csv', index=False)\n",
    "\n",
    "print(f\"‚úÖ Todos los datasets guardados en: {output_dir}/\")\n",
    "\n",
    "# Guardar metadatos\n",
    "metadata = {\n",
    "    'original_shape': df.shape,\n",
    "    'processed_shape': df_processed.shape,\n",
    "    'vocab_size': vocab_size,\n",
    "    'total_tokens': len(all_tokens),\n",
    "    'feature_columns': feature_columns,\n",
    "    'text_columns': text_columns,\n",
    "    'label_columns': label_columns,\n",
    "    'valid_labels': valid_labels,\n",
    "    'excluded_labels': excluded_labels,\n",
    "    'datasets_info': {name: dataset['description'] for name, dataset in datasets.items()},\n",
    "    'preprocessing_config': preprocessing_configs['basic']\n",
    "}\n",
    "\n",
    "with open(f'{output_dir}/metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Metadatos guardados: {output_dir}/metadata.json\")\n",
    "\n",
    "# Guardar el scaler\n",
    "with open(f'{output_dir}/scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(f\"‚úÖ Scaler guardado: {output_dir}/scaler.pkl\")\n",
    "\n",
    "# Guardar vocabulario\n",
    "vocab_info = {\n",
    "    'vocab_counter': dict(vocab_counter.most_common(1000)),  # Top 1000 palabras\n",
    "    'toxic_vocab': dict(toxic_vocab.most_common(500)),      # Top 500 t√≥xicas\n",
    "    'non_toxic_vocab': dict(non_toxic_vocab.most_common(500))  # Top 500 no t√≥xicas\n",
    "}\n",
    "\n",
    "with open(f'{output_dir}/vocabulary.json', 'w') as f:\n",
    "    json.dump(vocab_info, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Informaci√≥n de vocabulario guardada: {output_dir}/vocabulary.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
