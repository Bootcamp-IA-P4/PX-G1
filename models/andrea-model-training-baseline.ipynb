{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Modelos Baseline y Evaluaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "Este notebook implementa modelos baseline para clasificaci√≥n de toxicidad\n",
    "basado en los datos preprocesados del pipeline del notebook `andrea-mode-preprocessing.ipynb`.\n",
    "\n",
    "**Objetivos**:\n",
    "- Cargar datos preprocesados y divididos\n",
    "- Implementar modelos baseline simples\n",
    "- Sistema de evaluaci√≥n comprehensivo\n",
    "- Comparaci√≥n inicial de enfoques\n",
    "- An√°lisis de resultados y pr√≥ximos pasos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Librer√≠as para modeling\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score, \n",
    "    roc_curve, precision_recall_curve, f1_score, accuracy_score,\n",
    "    multilabel_confusion_matrix, average_precision_score\n",
    ")\n",
    "\n",
    "# Modelos baseline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# Vectorizaci√≥n de texto\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Utilidades\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Configurar visualizaciones\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "print(\"üìö Librer√≠as cargadas exitosamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Carga de datos preprocesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \"\"\"Clase para cargar y gestionar datos preprocesados\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir='../processed_data'):\n",
    "        self.data_dir = data_dir\n",
    "        self.metadata = None\n",
    "        self.datasets = {}\n",
    "        self.scaler = None\n",
    "        \n",
    "    def load_metadata(self):\n",
    "        \"\"\"Carga metadatos del preprocesamiento\"\"\"\n",
    "        try:\n",
    "            with open(f'{self.data_dir}/metadata.json', 'r') as f:\n",
    "                self.metadata = json.load(f)\n",
    "            print(\"üì• Metadatos cargados exitosamente\")\n",
    "            return self.metadata\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error cargando metadatos: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def load_scaler(self):\n",
    "        \"\"\"Carga el scaler entrenado\"\"\"\n",
    "        try:\n",
    "            with open(f'{self.data_dir}/scaler.pkl', 'rb') as f:\n",
    "                self.scaler = pickle.load(f)\n",
    "            print(\"üì• Scaler cargado exitosamente\")\n",
    "            return self.scaler\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error cargando scaler: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def load_dataset(self, dataset_name):\n",
    "        \"\"\"Carga un dataset espec√≠fico\"\"\"\n",
    "        dataset_path = f'{self.data_dir}/{dataset_name}'\n",
    "        \n",
    "        if not os.path.exists(dataset_path):\n",
    "            print(f\"‚ùå Dataset {dataset_name} no encontrado\")\n",
    "            return None\n",
    "        \n",
    "        dataset = {}\n",
    "        \n",
    "        try:\n",
    "            # Detectar tipo de dataset\n",
    "            files = os.listdir(dataset_path)\n",
    "            \n",
    "            if 'X_numeric_train.csv' in files:  # Dataset combinado\n",
    "                dataset['X_numeric_train'] = pd.read_csv(f'{dataset_path}/X_numeric_train.csv')\n",
    "                dataset['X_numeric_test'] = pd.read_csv(f'{dataset_path}/X_numeric_test.csv')\n",
    "                dataset['X_text_train'] = pd.read_csv(f'{dataset_path}/X_text_train.csv')\n",
    "                dataset['X_text_test'] = pd.read_csv(f'{dataset_path}/X_text_test.csv')\n",
    "                \n",
    "                # Cargar versiones escaladas si existen\n",
    "                if 'X_numeric_train_scaled.csv' in files:\n",
    "                    dataset['X_numeric_train_scaled'] = pd.read_csv(f'{dataset_path}/X_numeric_train_scaled.csv')\n",
    "                    dataset['X_numeric_test_scaled'] = pd.read_csv(f'{dataset_path}/X_numeric_test_scaled.csv')\n",
    "                    \n",
    "            else:  # Dataset regular\n",
    "                dataset['X_train'] = pd.read_csv(f'{dataset_path}/X_train.csv')\n",
    "                dataset['X_test'] = pd.read_csv(f'{dataset_path}/X_test.csv')\n",
    "                \n",
    "                # Cargar versiones escaladas si existen\n",
    "                if 'X_train_scaled.csv' in files:\n",
    "                    dataset['X_train_scaled'] = pd.read_csv(f'{dataset_path}/X_train_scaled.csv')\n",
    "                    dataset['X_test_scaled'] = pd.read_csv(f'{dataset_path}/X_test_scaled.csv')\n",
    "            \n",
    "            # Cargar etiquetas (com√∫n para todos)\n",
    "            dataset['y_train'] = pd.read_csv(f'{dataset_path}/y_train.csv')\n",
    "            dataset['y_test'] = pd.read_csv(f'{dataset_path}/y_test.csv')\n",
    "            \n",
    "            self.datasets[dataset_name] = dataset\n",
    "            print(f\"üì• Dataset '{dataset_name}' cargado exitosamente\")\n",
    "            return dataset\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error cargando dataset {dataset_name}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def load_all_datasets(self):\n",
    "        \"\"\"Carga todos los datasets disponibles\"\"\"\n",
    "        dataset_names = ['numeric_features', 'text_processed', 'text_cleaned', 'combined']\n",
    "        \n",
    "        for name in dataset_names:\n",
    "            self.load_dataset(name)\n",
    "        \n",
    "        print(f\"üì• {len(self.datasets)} datasets cargados\")\n",
    "        return self.datasets\n",
    "    \n",
    "    def get_dataset_info(self):\n",
    "        \"\"\"Muestra informaci√≥n de los datasets cargados\"\"\"\n",
    "        if not self.metadata:\n",
    "            self.load_metadata()\n",
    "        \n",
    "        print(\"üìä INFORMACI√ìN DE DATASETS:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for name, dataset in self.datasets.items():\n",
    "            print(f\"\\nüîπ {name.upper()}:\")\n",
    "            if 'X_train' in dataset:\n",
    "                print(f\"   ‚Ä¢ X_train: {dataset['X_train'].shape}\")\n",
    "                print(f\"   ‚Ä¢ X_test: {dataset['X_test'].shape}\")\n",
    "            else:\n",
    "                print(f\"   ‚Ä¢ X_numeric_train: {dataset['X_numeric_train'].shape}\")\n",
    "                print(f\"   ‚Ä¢ X_text_train: {dataset['X_text_train'].shape}\")\n",
    "            \n",
    "            print(f\"   ‚Ä¢ y_train: {dataset['y_train'].shape}\")\n",
    "            print(f\"   ‚Ä¢ Etiquetas: {list(dataset['y_train'].columns)}\")\n",
    "\n",
    "# Inicializar y cargar datos\n",
    "loader = DataLoader()\n",
    "metadata = loader.load_metadata()\n",
    "scaler = loader.load_scaler()\n",
    "datasets = loader.load_all_datasets()\n",
    "loader.get_dataset_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Sistema de evaluaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    \"\"\"Sistema comprehensivo de evaluaci√≥n de modelos\"\"\"\n",
    "    \n",
    "    def __init__(self, target_labels=None):\n",
    "        self.target_labels = target_labels or ['IsToxic']\n",
    "        self.results = {}\n",
    "        \n",
    "    def evaluate_binary_classification(self, y_true, y_pred, y_prob=None, model_name=\"Model\"):\n",
    "        \"\"\"Evaluaci√≥n para clasificaci√≥n binaria\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # M√©tricas b√°sicas\n",
    "        results['accuracy'] = accuracy_score(y_true, y_pred)\n",
    "        results['f1'] = f1_score(y_true, y_pred)\n",
    "        results['f1_macro'] = f1_score(y_true, y_pred, average='macro')\n",
    "        results['f1_weighted'] = f1_score(y_true, y_pred, average='weighted')\n",
    "        \n",
    "        # AUC si hay probabilidades\n",
    "        if y_prob is not None:\n",
    "            try:\n",
    "                results['roc_auc'] = roc_auc_score(y_true, y_prob)\n",
    "                results['avg_precision'] = average_precision_score(y_true, y_prob)\n",
    "            except:\n",
    "                results['roc_auc'] = np.nan\n",
    "                results['avg_precision'] = np.nan\n",
    "        \n",
    "        # Reporte de clasificaci√≥n\n",
    "        results['classification_report'] = classification_report(\n",
    "            y_true, y_pred, output_dict=True\n",
    "        )\n",
    "        \n",
    "        # Matriz de confusi√≥n\n",
    "        results['confusion_matrix'] = confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def evaluate_multilabel_classification(self, y_true, y_pred, y_prob=None, model_name=\"Model\"):\n",
    "        \"\"\"Evaluaci√≥n para clasificaci√≥n multilabel\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # M√©tricas por etiqueta\n",
    "        label_results = {}\n",
    "        for i, label in enumerate(self.target_labels):\n",
    "            if label in y_true.columns:\n",
    "                label_results[label] = self.evaluate_binary_classification(\n",
    "                    y_true[label], \n",
    "                    y_pred[:, i] if len(y_pred.shape) > 1 else y_pred,\n",
    "                    y_prob[:, i] if y_prob is not None and len(y_prob.shape) > 1 else y_prob,\n",
    "                    f\"{model_name}_{label}\"\n",
    "                )\n",
    "        \n",
    "        results['label_results'] = label_results\n",
    "        \n",
    "        # M√©tricas agregadas\n",
    "        if len(y_pred.shape) > 1:\n",
    "            # Micro y macro promedios\n",
    "            results['f1_micro'] = f1_score(y_true, y_pred, average='micro')\n",
    "            results['f1_macro'] = f1_score(y_true, y_pred, average='macro')\n",
    "            results['f1_weighted'] = f1_score(y_true, y_pred, average='weighted')\n",
    "            \n",
    "            if y_prob is not None:\n",
    "                try:\n",
    "                    results['roc_auc_micro'] = roc_auc_score(y_true, y_prob, average='micro')\n",
    "                    results['roc_auc_macro'] = roc_auc_score(y_true, y_prob, average='macro')\n",
    "                except:\n",
    "                    results['roc_auc_micro'] = np.nan\n",
    "                    results['roc_auc_macro'] = np.nan\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def cross_validate_model(self, model, X, y, cv_folds=5, scoring='f1'):\n",
    "        \"\"\"Cross-validation para un modelo\"\"\"\n",
    "        if len(y.shape) > 1 and y.shape[1] > 1:\n",
    "            # Multilabel - usar solo la primera etiqueta para CV\n",
    "            y_cv = y.iloc[:, 0] if hasattr(y, 'iloc') else y[:, 0]\n",
    "        else:\n",
    "            y_cv = y\n",
    "        \n",
    "        cv_scores = cross_val_score(\n",
    "            model, X, y_cv, \n",
    "            cv=StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42),\n",
    "            scoring=scoring\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'cv_mean': cv_scores.mean(),\n",
    "            'cv_std': cv_scores.std(),\n",
    "            'cv_scores': cv_scores\n",
    "        }\n",
    "    \n",
    "    def plot_confusion_matrix(self, cm, title=\"Confusion Matrix\", labels=None):\n",
    "        \"\"\"Visualizar matriz de confusi√≥n\"\"\"\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                   xticklabels=labels or ['No T√≥xico', 'T√≥xico'],\n",
    "                   yticklabels=labels or ['No T√≥xico', 'T√≥xico'])\n",
    "        plt.title(title)\n",
    "        plt.ylabel('Etiqueta Real')\n",
    "        plt.xlabel('Predicci√≥n')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_roc_curves(self, results_dict, target_label='IsToxic'):\n",
    "        \"\"\"Plotear curvas ROC para m√∫ltiples modelos\"\"\"\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        for model_name, result in results_dict.items():\n",
    "            if 'fpr' in result and 'tpr' in result:\n",
    "                auc_score = result.get('roc_auc', 0)\n",
    "                plt.plot(result['fpr'], result['tpr'], \n",
    "                        label=f'{model_name} (AUC = {auc_score:.3f})', \n",
    "                        linewidth=2)\n",
    "        \n",
    "        plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'ROC Curves - {target_label}')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Inicializar evaluador\n",
    "evaluator = ModelEvaluator(target_labels=metadata['valid_labels'] if metadata else ['IsToxic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Modelos baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineModels:\n",
    "    \"\"\"Colecci√≥n de modelos baseline para clasificaci√≥n de toxicidad\"\"\"\n",
    "    \n",
    "    def __init__(self, random_state=42):\n",
    "        self.random_state = random_state\n",
    "        self.models = {}\n",
    "        self.vectorizers = {}\n",
    "        self.results = {}\n",
    "        \n",
    "    def get_dummy_models(self):\n",
    "        \"\"\"Modelos dummy para comparaci√≥n\"\"\"\n",
    "        return {\n",
    "            'dummy_most_frequent': DummyClassifier(strategy='most_frequent', random_state=self.random_state),\n",
    "            'dummy_stratified': DummyClassifier(strategy='stratified', random_state=self.random_state),\n",
    "            'dummy_uniform': DummyClassifier(strategy='uniform', random_state=self.random_state)\n",
    "        }\n",
    "    \n",
    "    def get_text_vectorizers(self, max_features=5000):\n",
    "        \"\"\"Vectorizadores de texto\"\"\"\n",
    "        return {\n",
    "            'tfidf': TfidfVectorizer(\n",
    "                max_features=max_features,\n",
    "                ngram_range=(1, 2),\n",
    "                min_df=2,\n",
    "                max_df=0.95,\n",
    "                stop_words='english'\n",
    "            ),\n",
    "            'count': CountVectorizer(\n",
    "                max_features=max_features,\n",
    "                ngram_range=(1, 2),\n",
    "                min_df=2,\n",
    "                max_df=0.95,\n",
    "                stop_words='english'\n",
    "            )\n",
    "        }\n",
    "    \n",
    "    def get_numeric_models(self):\n",
    "        \"\"\"Modelos para caracter√≠sticas num√©ricas\"\"\"\n",
    "        return {\n",
    "            'logistic_numeric': LogisticRegression(\n",
    "                random_state=self.random_state, \n",
    "                max_iter=1000,\n",
    "                class_weight='balanced'\n",
    "            ),\n",
    "            'rf_numeric': RandomForestClassifier(\n",
    "                n_estimators=100,\n",
    "                random_state=self.random_state,\n",
    "                class_weight='balanced'\n",
    "            ),\n",
    "            'nb_gaussian': GaussianNB()\n",
    "        }\n",
    "    \n",
    "    def get_text_models(self):\n",
    "        \"\"\"Modelos para caracter√≠sticas de texto\"\"\"\n",
    "        return {\n",
    "            'logistic_text': LogisticRegression(\n",
    "                random_state=self.random_state,\n",
    "                max_iter=1000,\n",
    "                class_weight='balanced'\n",
    "            ),\n",
    "            'rf_text': RandomForestClassifier(\n",
    "                n_estimators=100,\n",
    "                random_state=self.random_state,\n",
    "                class_weight='balanced'\n",
    "            ),\n",
    "            'nb_multinomial': MultinomialNB(alpha=1.0),\n",
    "            'svm_text': SVC(\n",
    "                kernel='linear',\n",
    "                random_state=self.random_state,\n",
    "                class_weight='balanced',\n",
    "                probability=True\n",
    "            )\n",
    "        }\n",
    "    \n",
    "    def train_numeric_models(self, X_train, y_train, X_test, y_test, \n",
    "                           use_scaled=True, target_label='IsToxic'):\n",
    "        \"\"\"Entrenar modelos con caracter√≠sticas num√©ricas\"\"\"\n",
    "        print(f\"üî¢ Entrenando modelos num√©ricos para {target_label}...\")\n",
    "        \n",
    "        # Seleccionar datos\n",
    "        if use_scaled and 'X_train_scaled' in datasets['numeric_features']:\n",
    "            X_tr = datasets['numeric_features']['X_train_scaled']\n",
    "            X_te = datasets['numeric_features']['X_test_scaled']\n",
    "        else:\n",
    "            X_tr = X_train\n",
    "            X_te = X_test\n",
    "        \n",
    "        y_tr = y_train[target_label]\n",
    "        y_te = y_test[target_label]\n",
    "        \n",
    "        models = self.get_numeric_models()\n",
    "        models.update(self.get_dummy_models())\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for name, model in models.items():\n",
    "            print(f\"   üìä Entrenando {name}...\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                # Entrenar modelo\n",
    "                model.fit(X_tr, y_tr)\n",
    "                \n",
    "                # Predicciones\n",
    "                y_pred = model.predict(X_te)\n",
    "                y_prob = None\n",
    "                \n",
    "                if hasattr(model, 'predict_proba'):\n",
    "                    y_prob = model.predict_proba(X_te)[:, 1]\n",
    "                elif hasattr(model, 'decision_function'):\n",
    "                    y_prob = model.decision_function(X_te)\n",
    "                \n",
    "                # Evaluaci√≥n\n",
    "                result = evaluator.evaluate_binary_classification(\n",
    "                    y_te, y_pred, y_prob, name\n",
    "                )\n",
    "                \n",
    "                # Cross-validation\n",
    "                cv_result = evaluator.cross_validate_model(model, X_tr, y_tr)\n",
    "                result.update(cv_result)\n",
    "                \n",
    "                # Tiempo de entrenamiento\n",
    "                result['training_time'] = time.time() - start_time\n",
    "                \n",
    "                # Curva ROC\n",
    "                if y_prob is not None:\n",
    "                    fpr, tpr, _ = roc_curve(y_te, y_prob)\n",
    "                    result['fpr'] = fpr\n",
    "                    result['tpr'] = tpr\n",
    "                \n",
    "                results[name] = result\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Error en {name}: {e}\")\n",
    "                results[name] = {'error': str(e)}\n",
    "        \n",
    "        self.results[f'numeric_{target_label}'] = results\n",
    "        return results\n",
    "    \n",
    "    def train_text_models(self, X_train_text, y_train, X_test_text, y_test,\n",
    "                         target_label='IsToxic', max_features=5000):\n",
    "        \"\"\"Entrenar modelos con caracter√≠sticas de texto\"\"\"\n",
    "        print(f\"üìù Entrenando modelos de texto para {target_label}...\")\n",
    "        \n",
    "        y_tr = y_train[target_label]\n",
    "        y_te = y_test[target_label]\n",
    "        \n",
    "        vectorizers = self.get_text_vectorizers(max_features)\n",
    "        text_models = self.get_text_models()\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for vec_name, vectorizer in vectorizers.items():\n",
    "            print(f\"   üî§ Usando vectorizador: {vec_name}\")\n",
    "            \n",
    "            # Vectorizar texto\n",
    "            X_tr_vec = vectorizer.fit_transform(X_train_text)\n",
    "            X_te_vec = vectorizer.transform(X_test_text)\n",
    "            \n",
    "            # Guardar vectorizador\n",
    "            self.vectorizers[f'{vec_name}_{target_label}'] = vectorizer\n",
    "            \n",
    "            for model_name, model in text_models.items():\n",
    "                full_name = f\"{model_name}_{vec_name}\"\n",
    "                print(f\"      üìä Entrenando {full_name}...\")\n",
    "                start_time = time.time()\n",
    "                \n",
    "                try:\n",
    "                    # Entrenar modelo\n",
    "                    model.fit(X_tr_vec, y_tr)\n",
    "                    \n",
    "                    # Predicciones\n",
    "                    y_pred = model.predict(X_te_vec)\n",
    "                    y_prob = None\n",
    "                    \n",
    "                    if hasattr(model, 'predict_proba'):\n",
    "                        y_prob = model.predict_proba(X_te_vec)[:, 1]\n",
    "                    elif hasattr(model, 'decision_function'):\n",
    "                        y_prob = model.decision_function(X_te_vec)\n",
    "                    \n",
    "                    # Evaluaci√≥n\n",
    "                    result = evaluator.evaluate_binary_classification(\n",
    "                        y_te, y_pred, y_prob, full_name\n",
    "                    )\n",
    "                    \n",
    "                    # Cross-validation\n",
    "                    cv_result = evaluator.cross_validate_model(model, X_tr_vec, y_tr)\n",
    "                    result.update(cv_result)\n",
    "                    \n",
    "                    # Tiempo de entrenamiento\n",
    "                    result['training_time'] = time.time() - start_time\n",
    "                    \n",
    "                    # Informaci√≥n del vectorizador\n",
    "                    result['vocab_size'] = X_tr_vec.shape[1]\n",
    "                    \n",
    "                    # Curva ROC\n",
    "                    if y_prob is not None:\n",
    "                        fpr, tpr, _ = roc_curve(y_te, y_prob)\n",
    "                        result['fpr'] = fpr\n",
    "                        result['tpr'] = tpr\n",
    "                    \n",
    "                    results[full_name] = result\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"      ‚ùå Error en {full_name}: {e}\")\n",
    "                    results[full_name] = {'error': str(e)}\n",
    "        \n",
    "        self.results[f'text_{target_label}'] = results\n",
    "        return results\n",
    "    \n",
    "    def get_results_summary(self, results_dict, top_n=10):\n",
    "        \"\"\"Resumen de resultados ordenados por F1-score\"\"\"\n",
    "        summary = []\n",
    "        \n",
    "        for model_name, result in results_dict.items():\n",
    "            if 'error' not in result:\n",
    "                summary.append({\n",
    "                    'Model': model_name,\n",
    "                    'Accuracy': result.get('accuracy', 0),\n",
    "                    'F1': result.get('f1', 0),\n",
    "                    'F1_Macro': result.get('f1_macro', 0),\n",
    "                    'ROC_AUC': result.get('roc_auc', 0),\n",
    "                    'CV_Mean': result.get('cv_mean', 0),\n",
    "                    'CV_Std': result.get('cv_std', 0),\n",
    "                    'Training_Time': result.get('training_time', 0)\n",
    "                })\n",
    "        \n",
    "        df_summary = pd.DataFrame(summary)\n",
    "        if not df_summary.empty:\n",
    "            df_summary = df_summary.sort_values('F1', ascending=False).head(top_n)\n",
    "        \n",
    "        return df_summary\n",
    "\n",
    "# Inicializar modelos baseline\n",
    "baseline = BaselineModels()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Experimentos con modelos baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimento 1: Modelos con caracter√≠sticas num√©ricas\n",
    "print(\"\\nüìä EXPERIMENTO 1: CARACTER√çSTICAS NUM√âRICAS\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "if 'numeric_features' in datasets:\n",
    "    numeric_results = baseline.train_numeric_models(\n",
    "        datasets['numeric_features']['X_train'],\n",
    "        datasets['numeric_features']['y_train'],\n",
    "        datasets['numeric_features']['X_test'],\n",
    "        datasets['numeric_features']['y_test'],\n",
    "        use_scaled=True,\n",
    "        target_label='IsToxic'\n",
    "    )\n",
    "    \n",
    "    # Mostrar resumen\n",
    "    numeric_summary = baseline.get_results_summary(numeric_results)\n",
    "    print(\"\\nüìà TOP MODELOS NUM√âRICOS:\")\n",
    "    print(numeric_summary.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimento 2: Modelos con texto procesado\n",
    "print(\"\\nüìù EXPERIMENTO 2: CARACTER√çSTICAS DE TEXTO\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "if 'text_processed' in datasets:\n",
    "    text_results = baseline.train_text_models(\n",
    "        datasets['text_processed']['X_train'].iloc[:, 0],  # Primera columna (texto)\n",
    "        datasets['text_processed']['y_train'],\n",
    "        datasets['text_processed']['X_test'].iloc[:, 0],\n",
    "        datasets['text_processed']['y_test'],\n",
    "        target_label='IsToxic',\n",
    "        max_features=5000\n",
    "    )\n",
    "    \n",
    "    # Mostrar resumen\n",
    "    text_summary = baseline.get_results_summary(text_results)\n",
    "    print(\"\\nüìà TOP MODELOS DE TEXTO:\")\n",
    "    print(text_summary.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## An√°lisis y visualizaci√≥n de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_comparison(results_dict, metric='f1', title=\"Comparaci√≥n de Modelos\"):\n",
    "    \"\"\"Gr√°fico de barras para comparar modelos\"\"\"\n",
    "    models = []\n",
    "    scores = []\n",
    "    \n",
    "    for model_name, result in results_dict.items():\n",
    "        if 'error' not in result and metric in result:\n",
    "            models.append(model_name)\n",
    "            scores.append(result[metric])\n",
    "    \n",
    "    if not models:\n",
    "        print(f\"No hay datos para la m√©trica {metric}\")\n",
    "        return\n",
    "    \n",
    "    # Ordenar por score\n",
    "    sorted_data = sorted(zip(models, scores), key=lambda x: x[1], reverse=True)\n",
    "    models, scores = zip(*sorted_data)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(models)))\n",
    "    bars = plt.barh(range(len(models)), scores, color=colors)\n",
    "    \n",
    "    plt.yticks(range(len(models)), models)\n",
    "    plt.xlabel(metric.upper())\n",
    "    plt.title(title)\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Agregar valores en las barras\n",
    "    for i, (bar, score) in enumerate(zip(bars, scores)):\n",
    "        plt.text(score + 0.01, i, f'{score:.3f}', \n",
    "                va='center', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_time_vs_performance(results_dict):\n",
    "    \"\"\"Gr√°fico de tiempo de entrenamiento vs performance\"\"\"\n",
    "    models = []\n",
    "    times = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    for model_name, result in results_dict.items():\n",
    "        if 'error' not in result and 'training_time' in result and 'f1' in result:\n",
    "            models.append(model_name)\n",
    "            times.append(result['training_time'])\n",
    "            f1_scores.append(result['f1'])\n",
    "    \n",
    "    if not models:\n",
    "        print(\"No hay datos de tiempo de entrenamiento\")\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(times, f1_scores, s=100, alpha=0.7, c=range(len(models)), cmap='viridis')\n",
    "    \n",
    "    # anotar puntos\n",
    "    for i, model in enumerate(models):\n",
    "        plt.annotate(model, (times[i], f1_scores[i]), \n",
    "                    xytext=(5, 5), textcoords='offset points', \n",
    "                    fontsize=8, rotation=15)\n",
    "    \n",
    "    plt.xlabel('Tiempo de Entrenamiento (segundos)')\n",
    "    plt.ylabel('F1-Score')\n",
    "    plt.title('Eficiencia vs Performance de Modelos')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaciones\n",
    "print(\"\\nüìä VISUALIZACI√ìN DE RESULTADOS\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Comparar modelos num√©ricos\n",
    "if 'numeric_IsToxic' in baseline.results:\n",
    "    plot_model_comparison(\n",
    "        baseline.results['numeric_IsToxic'], \n",
    "        metric='f1',\n",
    "        title=\"Comparaci√≥n Modelos Num√©ricos - F1 Score\"\n",
    "    )\n",
    "\n",
    "# Comparar modelos de texto\n",
    "if 'text_IsToxic' in baseline.results:\n",
    "    plot_model_comparison(\n",
    "        baseline.results['text_IsToxic'], \n",
    "        metric='f1',\n",
    "        title=\"Comparaci√≥n Modelos de Texto - F1 Score\"\n",
    "    )\n",
    "\n",
    "# Curvas ROC\n",
    "print(\"\\nüìà CURVAS ROC\")\n",
    "if 'numeric_IsToxic' in baseline.results:\n",
    "    evaluator.plot_roc_curves(baseline.results['numeric_IsToxic'], 'IsToxic - Modelos Num√©ricos')\n",
    "\n",
    "if 'text_IsToxic' in baseline.results:\n",
    "    # Seleccionar top 5 modelos para ROC\n",
    "    text_results = baseline.results['text_IsToxic']\n",
    "    top_models = {}\n",
    "    for model_name, result in text_results.items():\n",
    "        if 'error' not in result and 'fpr' in result:\n",
    "            top_models[model_name] = result\n",
    "    \n",
    "    # Tomar solo los primeros 5 para no saturar el gr√°fico\n",
    "    if len(top_models) > 5:\n",
    "        sorted_models = sorted(top_models.items(), \n",
    "                             key=lambda x: x[1].get('f1', 0), reverse=True)\n",
    "        top_models = dict(sorted_models[:5])\n",
    "    \n",
    "    evaluator.plot_roc_curves(top_models, 'IsToxic - Modelos de Texto (Top 5)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## An√°lisis detallado del mejor modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_best_model(results_dict, model_name=None):\n",
    "    \"\"\"An√°lisis detallado del mejor modelo\"\"\"\n",
    "    \n",
    "    if model_name is None:\n",
    "        # Encontrar el mejor modelo por F1-score\n",
    "        best_f1 = 0\n",
    "        best_model = None\n",
    "        for name, result in results_dict.items():\n",
    "            if 'error' not in result and result.get('f1', 0) > best_f1:\n",
    "                best_f1 = result['f1']\n",
    "                best_model = name\n",
    "        model_name = best_model\n",
    "    \n",
    "    if model_name not in results_dict:\n",
    "        print(f\"Modelo {model_name} no encontrado\")\n",
    "        return\n",
    "    \n",
    "    result = results_dict[model_name]\n",
    "    \n",
    "    print(f\"\\nüèÜ AN√ÅLISIS DETALLADO: {model_name.upper()}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # M√©tricas principales\n",
    "    print(\"üìä M√âTRICAS PRINCIPALES:\")\n",
    "    print(f\"  ‚Ä¢ Accuracy: {result.get('accuracy', 'N/A'):.4f}\")\n",
    "    print(f\"  ‚Ä¢ F1-score: {result.get('f1', 'N/A'):.4f}\")\n",
    "    print(f\"  ‚Ä¢ F1 Macro: {result.get('f1_macro', 'N/A'):.4f}\")\n",
    "    print(f\"  ‚Ä¢ ROC AUC: {result.get('roc_auc', 'N/A'):.4f}\")\n",
    "    print(f\"  ‚Ä¢ CV Mean: {result.get('cv_mean', 'N/A'):.4f}\")\n",
    "    print(f\"  ‚Ä¢ Tiempo de entrenamiento: {result.get('training_time', 0):.2f} seg\")\n",
    "    \n",
    "    # Matriz de confusi√≥n\n",
    "    print(\"\\nüìâ MATRIZ DE CONFUSI√ìN:\")\n",
    "    cm = result.get('confusion_matrix')\n",
    "    if cm is not None:\n",
    "        evaluator.plot_confusion_matrix(cm, title=f\"Matriz de Confusi√≥n - {model_name}\")\n",
    "\n",
    "    # Reporte de clasificaci√≥n\n",
    "    print(\"\\nüìÑ REPORTE DE CLASIFICACI√ìN:\")\n",
    "    report = result.get('classification_report')\n",
    "    if report:\n",
    "        df_report = pd.DataFrame(report).transpose()\n",
    "        display(df_report.style.background_gradient(cmap='RdYlGn', subset=['precision', 'recall', 'f1-score']))\n",
    "    \n",
    "    # Curva ROC individual\n",
    "    print(\"\\nüìà CURVA ROC:\")\n",
    "    if 'fpr' in result and 'tpr' in result:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(result['fpr'], result['tpr'], label=f'{model_name} (AUC = {result[\"roc_auc\"]:.3f})')\n",
    "        plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'Curva ROC - {model_name}')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No se encontr√≥ informaci√≥n de curva ROC.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar an√°lisis del mejor modelo de texto\n",
    "print(\"\\nüîç AN√ÅLISIS DETALLADO DEL MEJOR MODELO DE TEXTO\")\n",
    "if 'text_IsToxic' in baseline.results:\n",
    "    analyze_best_model(baseline.results['text_IsToxic'])\n",
    "\n",
    "# Ejecutar an√°lisis del mejor modelo num√©rico\n",
    "print(\"\\nüîç AN√ÅLISIS DETALLADO DEL MEJOR MODELO NUM√âRICO\")\n",
    "if 'numeric_IsToxic' in baseline.results:\n",
    "    analyze_best_model(baseline.results['numeric_IsToxic'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
