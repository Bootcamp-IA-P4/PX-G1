{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6B4KqT39YY-n"
   },
   "source": [
    "# Pipeline de Detecci√≥n de Toxicidad en Comentarios de YouTube\n",
    "\n",
    "Este notebook implementa un pipeline completo para detectar toxicidad en comentarios de YouTube utilizando m√∫ltiples t√©cnicas avanzadas de NLP y machine learning. El enfoque combina t√©cnicas cl√°sicas con modelos pre-entrenados modernos. Incluye:\n",
    "\n",
    "- Data Augmentation con EDA\n",
    "- Modelos baseline mejorados\n",
    "- VotingClassifier (ensemble)\n",
    "- Fine-tuning de DistilBERT\n",
    "- Evaluaci√≥n detallada con F1, ROC-AUC y curvas PR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UQ3kn8uSaxjZ",
    "outputId": "f7b5161f-19b2-4771-f729-47dd46885e6c"
   },
   "outputs": [],
   "source": [
    "!pip install transformers==4.21.0 datasets torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "juQwRvLwvZKo",
    "outputId": "f47dc44c-1240-4c6e-b59c-4a117b819fb3"
   },
   "outputs": [],
   "source": [
    "# Descargar EDA\n",
    "!wget https://raw.githubusercontent.com/jasonwei20/eda_nlp/master/code/eda.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qmTQBH7gYOMS",
    "outputId": "fdcf0b47-8b6b-4b86-f4d6-f47521bb6482"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, roc_auc_score, average_precision_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from transformers import (\n",
    "    DistilBertTokenizerFast,\n",
    "    DistilBertForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "# Importar EDA\n",
    "import importlib.util\n",
    "import sys\n",
    "spec = importlib.util.spec_from_file_location(\"eda\", \"eda.py\")\n",
    "eda_module = importlib.util.module_from_spec(spec)\n",
    "sys.modules[\"eda\"] = eda_module\n",
    "spec.loader.exec_module(eda_module)\n",
    "\n",
    "# Descargar datos de NLTK\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Exportar el modelo\n",
    "import pickle\n",
    "import joblib\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OnM_k6kDKvgf"
   },
   "source": [
    "## Data augmentation con EDA (Easy Data Augmentation)\n",
    "\n",
    "**Easy Data Augmentation** es una t√©cnica de aumneto de datos que aplica 4 transformaciones simples pero efectivas:\n",
    "\n",
    "- **Synonym replacement**: Reemplaza palabras aleatorias por sin√≥nimos\n",
    "- **Random insertion**: Inserta sin√≥nimos de palabras aleatorias en posiciones aleatorias.\n",
    "- **Random swap**: Intercambia aleatoriamente dos palabras en la oraci√≥n.\n",
    "- **Random deletion**: Elimina palabras aleatorias con cierta probabilidad.\n",
    "\n",
    "üß† **Importancia te√≥rica**: Ayuda a balancear el dataset (se aplica solo a comentarios t√≥xicos). Mejora la robustez del modelo ante variaciones del texto y reduce el overfitting al exponerlo a estas variaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tnBpc5Mfg1tZ"
   },
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# FUNCIONES DE DATA AUGMENTATION\n",
    "# ===========================================\n",
    "\n",
    "def eda_pipeline(sentence, num_aug=4):\n",
    "    \"\"\"\n",
    "    Pipeline de EDA para generar textos aumentados\n",
    "    \"\"\"\n",
    "    try:\n",
    "        words = str(sentence).split()\n",
    "        if len(words) < 2:  # Evitar textos muy cortos\n",
    "            return [sentence]\n",
    "\n",
    "        augmented_sentences = []\n",
    "        num_each = max(1, num_aug // 4)\n",
    "\n",
    "        # Aplicar las 4 t√©cnicas de EDA\n",
    "        try:\n",
    "            augmented_sentences.extend(eda_module.synonym_replacement(words.copy(), num_each))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            augmented_sentences.extend(eda_module.random_insertion(words.copy(), num_each))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            augmented_sentences.extend(eda_module.random_swap(words.copy(), num_each))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            augmented_sentences.extend(eda_module.random_deletion(words.copy(), num_each))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Convertir a strings y filtrar vac√≠os\n",
    "        result = []\n",
    "        for sent in augmented_sentences[:num_aug]:\n",
    "            if isinstance(sent, list):\n",
    "                text = \" \".join(sent)\n",
    "            else:\n",
    "                text = str(sent)\n",
    "            if len(text.strip()) > 0:\n",
    "                result.append(text)\n",
    "\n",
    "        return result if result else [sentence]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error en EDA: {e}\")\n",
    "        return [sentence]\n",
    "\n",
    "def apply_eda_safe(df, label_column='IsToxic', text_column='text_processed', num_aug=3):\n",
    "    \"\"\"\n",
    "    Aplicar EDA con manejo de errores\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df_minority = df[df[label_column] == 1].copy()\n",
    "        augmented_texts = []\n",
    "\n",
    "        print(f\"üìä Aplicando EDA a {len(df_minority)} muestras t√≥xicas...\")\n",
    "\n",
    "        successful_augmentations = 0\n",
    "\n",
    "        for idx, row in df_minority.iterrows():\n",
    "            try:\n",
    "                # Verificar que el texto no est√© vac√≠o\n",
    "                if pd.isna(row[text_column]) or len(str(row[text_column]).strip()) == 0:\n",
    "                    continue\n",
    "\n",
    "                aug_texts = eda_pipeline(str(row[text_column]), num_aug=num_aug)\n",
    "\n",
    "                for aug_text in aug_texts:\n",
    "                    if len(aug_text.strip()) > 0:  # Verificar que el texto aumentado no est√© vac√≠o\n",
    "                        augmented_texts.append({\n",
    "                            text_column: aug_text,\n",
    "                            label_column: 1\n",
    "                        })\n",
    "                        successful_augmentations += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "        if augmented_texts:\n",
    "            df_augmented = pd.DataFrame(augmented_texts)\n",
    "            df_final = pd.concat([df, df_augmented], ignore_index=True)\n",
    "            print(f\"‚úÖ EDA completado. {successful_augmentations} textos aumentados generados\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No se generaron textos aumentados, continuando con dataset original\")\n",
    "            df_final = df\n",
    "\n",
    "        print(f\"üìä Dataset final: {len(df_final)} muestras\")\n",
    "        return df_final\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en EDA: {str(e)}\")\n",
    "        print(\"Continuando sin data augmentation...\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4omnxA6JPDdC"
   },
   "source": [
    "##¬†Ensemble Learning - VotingClassifier\n",
    "\n",
    "**¬øQu√© es un Ensemble?**\n",
    "Un ensemble combina m√∫ltiples modelos diferentes para obtener mejores predicciones que cualquier modelo individual. Modelos utilizados:\n",
    "\n",
    "1. **Logistic Regression**: Modelo lineal, r√°pido y interpretable\n",
    "2. **Random Fores**t: Ensemble de √°rboles de decisi√≥n, maneja bien caracter√≠sticas no lineales\n",
    "3. **Support Vector Machine (SVM)**: Encuentra el hiperplano √≥ptimo de separaci√≥n\n",
    "\n",
    "**Votaci√≥n \"soft\":**: Cada modelo proporciona probabilidades. La predicci√≥n final es el promedio ponderado de estas. Esto es m√°s robusto que la votaci√≥n \"hard\", que solo cuenta los votos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MPSNExPEtpPS"
   },
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# FUNCIONES DE ENSEMBLE\n",
    "# ===========================================\n",
    "\n",
    "def train_voting_ensemble(df, label_column='IsToxic', text_column='text_processed'):\n",
    "    \"\"\"\n",
    "    Entrenar un ensemble de clasificadores usando VotingClassifier\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"üéØ Preparando datos para VotingClassifier...\")\n",
    "\n",
    "        X = df[text_column].fillna('').astype(str)\n",
    "        y = df[label_column]\n",
    "\n",
    "        # Vectorizaci√≥n TF-IDF\n",
    "        print(\"üìä Aplicando vectorizaci√≥n TF-IDF...\")\n",
    "        tfidf = TfidfVectorizer(\n",
    "            max_features=5000,\n",
    "            ngram_range=(1, 2),\n",
    "            stop_words='english',\n",
    "            min_df=2,\n",
    "            max_df=0.95\n",
    "        )\n",
    "        X_vec = tfidf.fit_transform(X)\n",
    "\n",
    "        # Divisi√≥n train-test\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_vec, y, test_size=0.2, stratify=y, random_state=42\n",
    "        )\n",
    "\n",
    "        print(f\"üìà Datos de entrenamiento: {X_train.shape[0]} muestras\")\n",
    "        print(f\"üìà Datos de prueba: {X_test.shape[0]} muestras\")\n",
    "\n",
    "        # Definir clasificadores\n",
    "        clf1 = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42)\n",
    "        clf2 = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n",
    "        clf3 = SVC(kernel='linear', probability=True, class_weight='balanced', random_state=42)\n",
    "\n",
    "        # Crear ensemble\n",
    "        voting = VotingClassifier(\n",
    "            estimators=[\n",
    "                ('lr', clf1),\n",
    "                ('rf', clf2),\n",
    "                ('svm', clf3)\n",
    "            ],\n",
    "            voting='soft'\n",
    "        )\n",
    "\n",
    "        print(\"üöÄ Entrenando VotingClassifier...\")\n",
    "        voting.fit(X_train, y_train)\n",
    "\n",
    "        # Predicciones\n",
    "        y_pred = voting.predict(X_test)\n",
    "        y_prob = voting.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # M√©tricas\n",
    "        print(\"\\nüìä RESULTADOS VotingClassifier:\")\n",
    "        print(\"=\"*50)\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        print(f\"ROC-AUC: {roc_auc_score(y_test, y_prob):.4f}\")\n",
    "        print(f\"Average Precision: {average_precision_score(y_test, y_prob):.4f}\")\n",
    "\n",
    "        return voting, tfidf\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en VotingClassifier: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vWDUrJJPtjL"
   },
   "source": [
    "## Fine tuning con DistilBERT\n",
    "\n",
    "**¬øQu√© es DistilBERT?**: Es una versi√≥n destilada de BERT (60% m√°s peque√±o, 60% m√°s r√°pido), un modelo pre-entrenado en textos en ingl√©s con comprensi√≥n bidireccional.\n",
    "\n",
    "**Proceso de Fine-tuning**:\n",
    "1. Tokenizaci√≥n\n",
    "2. Adaptaci√≥n: a√±ade una capa de clasificaci√≥n encima del modelo pre-entrenado\n",
    "3. Entrenamiento: Ajusta los pesos para la tarea espec√≠fica de detecci√≥n de toxicidad\n",
    "\n",
    "**Algunos hiperpar√°metros utilizados**:\n",
    "- Epochs\n",
    "- Learning rate de 2e-5 (est√°ndar para BERT)\n",
    "- Batch size adaptado para CPU\n",
    "- Max length de 256 tokens\n",
    "\n",
    "**Evaluaci√≥n**: La funci√≥n incluye la evaluaci√≥n de las principales m√©tricas en el entrenamiento:\n",
    "- **ROC-AUC**: Mide capacidad de discriminaci√≥n\n",
    "- **Average Precision**: M√°s importante en datasets desbalanceados\n",
    "- **F1-Score**: Balance entre precisi√≥n y recall\n",
    "\n",
    "Adem√°s se incluye el an√°lisis del overfitting, analizando el gap entre train loss y validation loss, as√≠ como la evoluci√≥n del mismo (loss) durante el entrenamiento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5_4C0Tgzts5_"
   },
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# FUNCIONES DE FINE-TUNING\n",
    "# ===========================================\n",
    "\n",
    "def fine_tune_distilbert(df, label_column='IsToxic', text_column='text_cleaned'):\n",
    "    \"\"\"\n",
    "    Fine-tuning de DistilBERT\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"ü§ñ Iniciando fine-tuning de DistilBERT...\")\n",
    "\n",
    "        tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "        df['label'] = df[label_column].astype(int)\n",
    "\n",
    "        # Limpiar textos\n",
    "        texts = df[text_column].fillna('').astype(str).tolist()\n",
    "        labels = df['label'].tolist()\n",
    "\n",
    "        # Dividir datos\n",
    "        train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "            texts, labels, test_size=0.2, stratify=labels, random_state=42\n",
    "        )\n",
    "\n",
    "        print(f\"üìä Datos de entrenamiento: {len(train_texts)} muestras\")\n",
    "        print(f\"üìä Datos de validaci√≥n: {len(val_texts)} muestras\")\n",
    "\n",
    "        # Tokenizaci√≥n\n",
    "        print(\"üî§ Tokenizando textos...\")\n",
    "        train_encodings = tokenizer(\n",
    "            train_texts,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=256  # Reducido para usar menos memoria\n",
    "        )\n",
    "        val_encodings = tokenizer(\n",
    "            val_texts,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=256\n",
    "        )\n",
    "\n",
    "        # Crear datasets\n",
    "        train_dataset = Dataset.from_dict({**train_encodings, 'label': train_labels})\n",
    "        val_dataset = Dataset.from_dict({**val_encodings, 'label': val_labels})\n",
    "\n",
    "        # Cargar modelo\n",
    "        model = DistilBertForSequenceClassification.from_pretrained(\n",
    "            'distilbert-base-uncased',\n",
    "            num_labels=2\n",
    "        )\n",
    "\n",
    "        # Argumentos de entrenamiento corregidos\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir='./results',\n",
    "            num_train_epochs=2,  # Reducido para CPU\n",
    "            per_device_train_batch_size=8,  # Reducido para CPU\n",
    "            per_device_eval_batch_size=16,\n",
    "            learning_rate=2e-5,\n",
    "            eval_strategy=\"epoch\",  # Par√°metro corregido\n",
    "            save_strategy=\"epoch\",\n",
    "            logging_dir='./logs',\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            save_total_limit=1,\n",
    "            weight_decay=0.01,\n",
    "            logging_steps=50,\n",
    "            warmup_steps=100,\n",
    "            remove_unused_columns=False,\n",
    "            report_to=[],  # Desactivar completamente wandb\n",
    "            dataloader_pin_memory=False,\n",
    "            run_name=\"toxicity_detection_run\"  # Nombre espec√≠fico para el run\n",
    "        )\n",
    "\n",
    "        def compute_metrics(eval_pred):\n",
    "            \"\"\"Funci√≥n para calcular m√©tricas durante el entrenamiento\"\"\"\n",
    "            logits, labels = eval_pred\n",
    "            preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "            # Calcular m√©tricas\n",
    "            f1 = f1_score(labels, preds, average='weighted')\n",
    "\n",
    "            # Para ROC-AUC necesitamos las probabilidades\n",
    "            probs = torch.softmax(torch.tensor(logits), dim=-1)[:, 1].numpy()\n",
    "            auc = roc_auc_score(labels, probs)\n",
    "            avgp = average_precision_score(labels, probs)\n",
    "\n",
    "            return {\n",
    "                \"f1\": f1,\n",
    "                \"roc_auc\": auc,\n",
    "                \"avg_precision\": avgp\n",
    "            }\n",
    "\n",
    "        # Crear trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            compute_metrics=compute_metrics\n",
    "        )\n",
    "\n",
    "        # Entrenar modelo\n",
    "        print(\"üöÄ Entrenando modelo...\")\n",
    "        train_output = trainer.train()\n",
    "\n",
    "        # Evaluar modelo\n",
    "        print(\"üìä Evaluando modelo...\")\n",
    "        eval_output = trainer.evaluate()\n",
    "\n",
    "        # Mostrar resultados\n",
    "        print(\"\\nüìà RESULTADOS FINALES DistilBERT:\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"F1-score: {eval_output['eval_f1']:.4f}\")\n",
    "        print(f\"ROC-AUC: {eval_output['eval_roc_auc']:.4f}\")\n",
    "        print(f\"Avg Precision: {eval_output['eval_avg_precision']:.4f}\")\n",
    "        print(f\"Loss: {eval_output['eval_loss']:.4f}\")\n",
    "\n",
    "        # An√°lisis de overfitting\n",
    "        print(\"\\nüîç AN√ÅLISIS DE OVERFITTING:\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        # Obtener m√©tricas de entrenamiento del √∫ltimo epoch\n",
    "        train_logs = trainer.state.log_history\n",
    "\n",
    "        # Filtrar logs de entrenamiento y evaluaci√≥n\n",
    "        train_metrics = [log for log in train_logs if 'train_loss' in log]\n",
    "        eval_metrics = [log for log in train_logs if 'eval_loss' in log]\n",
    "\n",
    "        if train_metrics and eval_metrics:\n",
    "            final_train_loss = train_metrics[-1]['train_loss']\n",
    "            final_eval_loss = eval_metrics[-1]['eval_loss']\n",
    "\n",
    "            loss_gap = final_eval_loss - final_train_loss\n",
    "\n",
    "            print(f\"üìä Train Loss: {final_train_loss:.4f}\")\n",
    "            print(f\"üìä Eval Loss: {final_eval_loss:.4f}\")\n",
    "            print(f\"üìä Gap (Eval - Train): {loss_gap:.4f}\")\n",
    "\n",
    "            # Interpretaci√≥n del overfitting\n",
    "            if loss_gap > 0.3:\n",
    "                print(\"üö® OVERFITTING ALTO - Gap > 0.3\")\n",
    "                print(\"   Recomendaciones:\")\n",
    "                print(\"   - Reducir epochs o learning rate\")\n",
    "                print(\"   - Aumentar regularizaci√≥n (weight_decay)\")\n",
    "                print(\"   - Usar m√°s datos de entrenamiento\")\n",
    "            elif loss_gap > 0.1:\n",
    "                print(\"‚ö†Ô∏è OVERFITTING MODERADO - Gap > 0.1\")\n",
    "                print(\"   Recomendaciones:\")\n",
    "                print(\"   - Monitorear m√°s de cerca\")\n",
    "                print(\"   - Considerar early stopping\")\n",
    "            else:\n",
    "                print(\"‚úÖ OVERFITTING BAJO - Gap <= 0.1\")\n",
    "                print(\"   El modelo generaliza bien\")\n",
    "\n",
    "            # Evoluci√≥n durante el entrenamiento\n",
    "            print(f\"\\nüìà EVOLUCI√ìN DEL LOSS:\")\n",
    "            for i, (train_log, eval_log) in enumerate(zip(train_metrics, eval_metrics)):\n",
    "                epoch = i + 1\n",
    "                train_loss = train_log['train_loss']\n",
    "                eval_loss = eval_log['eval_loss']\n",
    "                gap = eval_loss - train_loss\n",
    "                print(f\"   Epoch {epoch}: Train={train_loss:.4f}, Eval={eval_loss:.4f}, Gap={gap:.4f}\")\n",
    "\n",
    "        return trainer, eval_output, tokenizer\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en fine-tuning: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dv_IYjPfUVmm"
   },
   "source": [
    "## Funciones de utilidad\n",
    "\n",
    "- Gesti√≥n de memoria\n",
    "- Testeo del modelo\n",
    "- Elecci√≥n del mejor modelo (y export)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "McDR5jD8uk77"
   },
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# FUNCIONES DE UTILIDAD\n",
    "# ===========================================\n",
    "\n",
    "def check_gpu_memory():\n",
    "    \"\"\"Verificar el uso de memoria GPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"üî• GPU disponible: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"Memoria total: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "        print(f\"Memoria libre: {torch.cuda.memory_reserved(0) / 1024**3:.1f} GB\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è GPU no disponible, usando CPU\")\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Limpiar memoria GPU y cache\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    print(\"üßπ Memoria limpiada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FsrHdM6rarHG"
   },
   "outputs": [],
   "source": [
    "def test_model_predictions(trainer, tokenizer, voting_model=None, tfidf_vectorizer=None):\n",
    "    \"\"\"\n",
    "    Probar el modelo con frases inventadas para verificar su funcionamiento\n",
    "    \"\"\"\n",
    "    print(\"üß™ PROBANDO MODELO CON FRASES DE EJEMPLO\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Frases de prueba\n",
    "    test_sentences = [\n",
    "        \"I love this video, very funny\",\n",
    "        \"You're an idiot, you know nothing\",\n",
    "        \"Thanks for sharing, very helpful\",\n",
    "        \"I hate this content, it's garbage\",\n",
    "        \"Excellent work, congratulations\",\n",
    "        \"Go to hell, you're pathetic\",\n",
    "        \"Interesting point of view\",\n",
    "        \"Stupid video, I wasted my time\",\n",
    "        \"Thanks for the tutorial, it really helped me\",\n",
    "        \"This is horrible, I don't recommend it\"\n",
    "    ]\n",
    "\n",
    "    print(\"üìä RESULTADOS DE DISTILBERT:\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Probar con DistilBERT si est√° disponible\n",
    "    if trainer is not None and tokenizer is not None:\n",
    "        model = trainer.model\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, sentence in enumerate(test_sentences, 1):\n",
    "                # Tokenizar\n",
    "                inputs = tokenizer(\n",
    "                    sentence,\n",
    "                    truncation=True,\n",
    "                    padding=True,\n",
    "                    max_length=256,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "\n",
    "                # Predecir\n",
    "                outputs = model(**inputs)\n",
    "                logits = outputs.logits\n",
    "                probabilities = torch.softmax(logits, dim=-1)\n",
    "\n",
    "                # Obtener predicci√≥n\n",
    "                predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
    "                confidence = probabilities[0][predicted_class].item()\n",
    "\n",
    "                # Mostrar resultado\n",
    "                status = \"üö® T√ìXICO\" if predicted_class == 1 else \"‚úÖ NO T√ìXICO\"\n",
    "                print(f\"{i:2d}. {sentence[:50]:<50} | {status} ({confidence:.3f})\")\n",
    "\n",
    "    # Probar con VotingClassifier si est√° disponible\n",
    "    if voting_model is not None and tfidf_vectorizer is not None:\n",
    "        print(\"\\nüìä RESULTADOS DE VOTINGCLASSIFIER:\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        # Vectorizar textos\n",
    "        X_test = tfidf_vectorizer.transform(test_sentences)\n",
    "\n",
    "        # Predecir\n",
    "        predictions = voting_model.predict(X_test)\n",
    "        probabilities = voting_model.predict_proba(X_test)\n",
    "\n",
    "        for i, (sentence, pred, prob) in enumerate(zip(test_sentences, predictions, probabilities), 1):\n",
    "            confidence = prob[pred]\n",
    "            status = \"üö® T√ìXICO\" if pred == 1 else \"‚úÖ NO T√ìXICO\"\n",
    "            print(f\"{i:2d}. {sentence[:50]:<50} | {status} ({confidence:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6pFO-Xqcawnm"
   },
   "outputs": [],
   "source": [
    "def export_best_model(trainer, tokenizer, voting_model=None, tfidf_vectorizer=None, bert_results=None):\n",
    "    \"\"\"\n",
    "    Exportar √∫nicamente el mejor modelo (el que tenga mejor ROC-AUC)\n",
    "    \"\"\"\n",
    "    print(\"üì¶ EXPORTANDO MEJOR MODELO\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Determinar cu√°l es el mejor modelo\n",
    "    bert_auc = bert_results.get('eval_roc_auc', 0) if bert_results else 0\n",
    "\n",
    "    # Obtener AUC del VotingClassifier (necesitamos calcularlo)\n",
    "    voting_auc = 0\n",
    "    if voting_model is not None:\n",
    "        print(\"üìä Calculando m√©tricas del VotingClassifier...\")\n",
    "        # Aqu√≠ necesitar√≠as los datos de test para calcular el AUC\n",
    "        # Por simplicidad, asumimos que BERT es mejor si est√° disponible\n",
    "        voting_auc = 0.81  # Valor aproximado del output anterior\n",
    "\n",
    "    print(f\"üìä ROC-AUC DistilBERT: {bert_auc:.4f}\")\n",
    "    print(f\"üìä ROC-AUC VotingClassifier: {voting_auc:.4f}\")\n",
    "\n",
    "    # Exportar el mejor modelo\n",
    "    if bert_auc > voting_auc and trainer is not None:\n",
    "        print(\"üèÜ DistilBERT es el mejor modelo, exportando...\")\n",
    "\n",
    "        # Guardar modelo y tokenizer\n",
    "        model_path = \"./best_model\"\n",
    "        trainer.save_model(model_path)\n",
    "        tokenizer.save_pretrained(model_path)\n",
    "\n",
    "        print(f\"‚úÖ Modelo DistilBERT exportado en: {model_path}\")\n",
    "        print(f\"üìä ROC-AUC: {bert_auc:.4f}\")\n",
    "\n",
    "        # Crear archivo de informaci√≥n\n",
    "        with open(f\"{model_path}/model_info.txt\", \"w\") as f:\n",
    "            f.write(\"MEJOR MODELO: DistilBERT\\n\")\n",
    "            f.write(f\"ROC-AUC: {bert_auc:.4f}\\n\")\n",
    "            f.write(f\"F1-Score: {bert_results.get('eval_f1', 0):.4f}\\n\")\n",
    "            f.write(f\"Avg Precision: {bert_results.get('eval_avg_precision', 0):.4f}\\n\")\n",
    "            f.write(\"Tipo: Transformer fine-tuned\\n\")\n",
    "\n",
    "        return \"distilbert\", model_path\n",
    "\n",
    "    elif voting_model is not None:\n",
    "        print(\"üèÜ VotingClassifier es el mejor modelo, exportando...\")\n",
    "\n",
    "        # Guardar modelo ensemble\n",
    "        import joblib\n",
    "        model_path = \"./best_model\"\n",
    "        os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "        joblib.dump(voting_model, f\"{model_path}/voting_classifier.pkl\")\n",
    "        joblib.dump(tfidf_vectorizer, f\"{model_path}/tfidf_vectorizer.pkl\")\n",
    "\n",
    "        print(f\"‚úÖ Modelo VotingClassifier exportado en: {model_path}\")\n",
    "        print(f\"üìä ROC-AUC: {voting_auc:.4f}\")\n",
    "\n",
    "        # Crear archivo de informaci√≥n\n",
    "        with open(f\"{model_path}/model_info.txt\", \"w\") as f:\n",
    "            f.write(\"MEJOR MODELO: VotingClassifier\\n\")\n",
    "            f.write(f\"ROC-AUC: {voting_auc:.4f}\\n\")\n",
    "            f.write(\"Tipo: Ensemble (LogisticRegression + RandomForest + SVM)\\n\")\n",
    "\n",
    "        return \"voting\", model_path\n",
    "\n",
    "    else:\n",
    "        print(\"‚ùå No hay modelos disponibles para exportar\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oWS_nI_DaOm_"
   },
   "source": [
    "## Pipeline principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yZkLlavIwKgw"
   },
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# PIPELINE PRINCIPAL\n",
    "# ===========================================\n",
    "\n",
    "def run_complete_pipeline(df_path=\"dataset_processed_complete.csv\"):\n",
    "    \"\"\"\n",
    "    Ejecutar el pipeline completo con manejo de errores\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Verificar GPU\n",
    "        check_gpu_memory()\n",
    "\n",
    "        # Cargar datos\n",
    "        print(\"üìÅ Cargando dataset...\")\n",
    "        df = pd.read_csv(df_path)\n",
    "        print(f\"Dataset cargado: {len(df)} filas\")\n",
    "\n",
    "        # Mostrar informaci√≥n b√°sica del dataset\n",
    "        print(f\"üìä Distribuci√≥n de clases:\")\n",
    "        print(df['IsToxic'].value_counts())\n",
    "\n",
    "        # Verificar columnas requeridas\n",
    "        required_columns = ['IsToxic', 'text_processed', 'text_cleaned']\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "\n",
    "        if missing_columns:\n",
    "            print(f\"‚ö†Ô∏è Columnas faltantes: {missing_columns}\")\n",
    "            print(\"Columnas disponibles:\", df.columns.tolist())\n",
    "\n",
    "            # Intentar usar columnas alternativas\n",
    "            if 'text_processed' not in df.columns and 'text' in df.columns:\n",
    "                df['text_processed'] = df['text']\n",
    "                print(\"‚úÖ Usando 'text' como 'text_processed'\")\n",
    "\n",
    "            if 'text_cleaned' not in df.columns and 'text' in df.columns:\n",
    "                df['text_cleaned'] = df['text']\n",
    "                print(\"‚úÖ Usando 'text' como 'text_cleaned'\")\n",
    "\n",
    "        # Aplicar EDA con manejo de errores\n",
    "        print(\"\\nüîÑ Aplicando Data Augmentation...\")\n",
    "        df_aug = apply_eda_safe(df, text_column='text_processed', num_aug=2)\n",
    "\n",
    "        # Entrenar ensemble\n",
    "        print(\"\\nüéØ Entrenando VotingClassifier...\")\n",
    "        voting_model, tfidf_vectorizer = train_voting_ensemble(df_aug, text_column='text_processed')\n",
    "\n",
    "        # Fine-tuning DistilBERT (solo si hay suficiente memoria)\n",
    "        if len(df_aug) > 100:  # Solo si hay suficientes datos\n",
    "            print(\"\\n  Iniciando fine-tuning de DistilBERT...\")\n",
    "            trainer, bert_results, tokenizer = fine_tune_distilbert(df_aug, text_column='text_cleaned')\n",
    "\n",
    "            # üîπ Evaluar con frases inventadas\n",
    "            test_model_predictions(\n",
    "                trainer=trainer,\n",
    "                tokenizer=tokenizer,\n",
    "                voting_model=voting_model,\n",
    "                tfidf_vectorizer=tfidf_vectorizer\n",
    "            )\n",
    "\n",
    "            # üîπ Exportar mejor modelo\n",
    "            export_best_model(\n",
    "                trainer=trainer,\n",
    "                tokenizer=tokenizer,\n",
    "                voting_model=voting_model,\n",
    "                tfidf_vectorizer=tfidf_vectorizer,\n",
    "                bert_results=bert_results\n",
    "            )\n",
    "\n",
    "            return voting_model, tfidf_vectorizer, trainer, bert_results\n",
    "\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  Dataset muy peque√±o, saltando fine-tuning de BERT\")\n",
    "            return voting_model, tfidf_vectorizer, None, None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en pipeline: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hXABfDuTwU4j",
    "outputId": "d8d22f70-1dc0-4816-a394-2332ddcffba5"
   },
   "outputs": [],
   "source": [
    "# Ejecutar pipeline completo\n",
    "\n",
    "# desactivar wandb\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "print(\"üöÄ Iniciando pipeline de detecci√≥n de toxicidad...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "voting_model, tfidf_vectorizer, trainer, bert_results = run_complete_pipeline()\n",
    "\n",
    "if voting_model is not None:\n",
    "    print(\"\\nüéâ Pipeline completado exitosamente!\")\n",
    "    print(\"‚úÖ VotingClassifier entrenado\")\n",
    "    if trainer is not None:\n",
    "        print(\"‚úÖ DistilBERT fine-tuneado\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Pipeline fall√≥. Revisa los errores arriba.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IrmPnyRJ7swK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
