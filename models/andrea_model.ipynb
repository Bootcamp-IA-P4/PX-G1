{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6B4KqT39YY-n"
   },
   "source": [
    "# Detecci√≥n de toxicidad en comentarios de Youtube. Modelado avanzado\n",
    "\n",
    "Este notebook implementa un pipeline completo que incluye:\n",
    "- Data Augmentation con EDA\n",
    "- Modelos baseline mejorados\n",
    "- VotingClassifier (ensemble)\n",
    "- Fine-tuning de DistilBERT\n",
    "- Evaluaci√≥n detallada con F1, ROC-AUC y curvas PR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "juQwRvLwvZKo",
    "outputId": "4c5eedfb-534d-476e-ff0a-ff3047ab6fa7"
   },
   "outputs": [],
   "source": [
    "# Descargar EDA\n",
    "!wget https://raw.githubusercontent.com/jasonwei20/eda_nlp/master/code/eda.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qmTQBH7gYOMS",
    "outputId": "6a85712f-a139-4eba-b7ba-f2a21273a511"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, roc_auc_score, average_precision_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from transformers import (\n",
    "    DistilBertTokenizerFast,\n",
    "    DistilBertForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "# Importar EDA\n",
    "import importlib.util\n",
    "import sys\n",
    "spec = importlib.util.spec_from_file_location(\"eda\", \"eda.py\")\n",
    "eda_module = importlib.util.module_from_spec(spec)\n",
    "sys.modules[\"eda\"] = eda_module\n",
    "spec.loader.exec_module(eda_module)\n",
    "\n",
    "# Descargar datos de NLTK\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tnBpc5Mfg1tZ"
   },
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# FUNCIONES DE DATA AUGMENTATION\n",
    "# ===========================================\n",
    "\n",
    "def eda_pipeline(sentence, num_aug=4):\n",
    "    \"\"\"\n",
    "    Pipeline de EDA para generar textos aumentados\n",
    "    \"\"\"\n",
    "    try:\n",
    "        words = str(sentence).split()\n",
    "        if len(words) < 2:  # Evitar textos muy cortos\n",
    "            return [sentence]\n",
    "\n",
    "        augmented_sentences = []\n",
    "        num_each = max(1, num_aug // 4)\n",
    "\n",
    "        # Aplicar las 4 t√©cnicas de EDA\n",
    "        try:\n",
    "            augmented_sentences.extend(eda_module.synonym_replacement(words.copy(), num_each))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            augmented_sentences.extend(eda_module.random_insertion(words.copy(), num_each))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            augmented_sentences.extend(eda_module.random_swap(words.copy(), num_each))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            augmented_sentences.extend(eda_module.random_deletion(words.copy(), num_each))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Convertir a strings y filtrar vac√≠os\n",
    "        result = []\n",
    "        for sent in augmented_sentences[:num_aug]:\n",
    "            if isinstance(sent, list):\n",
    "                text = \" \".join(sent)\n",
    "            else:\n",
    "                text = str(sent)\n",
    "            if len(text.strip()) > 0:\n",
    "                result.append(text)\n",
    "\n",
    "        return result if result else [sentence]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error en EDA: {e}\")\n",
    "        return [sentence]\n",
    "\n",
    "def apply_eda_safe(df, label_column='IsToxic', text_column='text_processed', num_aug=3):\n",
    "    \"\"\"\n",
    "    Aplicar EDA con manejo de errores para evitar problemas en Colab\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df_minority = df[df[label_column] == 1].copy()\n",
    "        augmented_texts = []\n",
    "\n",
    "        print(f\"üìä Aplicando EDA a {len(df_minority)} muestras t√≥xicas...\")\n",
    "\n",
    "        successful_augmentations = 0\n",
    "\n",
    "        for idx, row in df_minority.iterrows():\n",
    "            try:\n",
    "                # Verificar que el texto no est√© vac√≠o\n",
    "                if pd.isna(row[text_column]) or len(str(row[text_column]).strip()) == 0:\n",
    "                    continue\n",
    "\n",
    "                aug_texts = eda_pipeline(str(row[text_column]), num_aug=num_aug)\n",
    "\n",
    "                for aug_text in aug_texts:\n",
    "                    if len(aug_text.strip()) > 0:  # Verificar que el texto aumentado no est√© vac√≠o\n",
    "                        augmented_texts.append({\n",
    "                            text_column: aug_text,\n",
    "                            label_column: 1\n",
    "                        })\n",
    "                        successful_augmentations += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "        if augmented_texts:\n",
    "            df_augmented = pd.DataFrame(augmented_texts)\n",
    "            df_final = pd.concat([df, df_augmented], ignore_index=True)\n",
    "            print(f\"‚úÖ EDA completado. {successful_augmentations} textos aumentados generados\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No se generaron textos aumentados, continuando con dataset original\")\n",
    "            df_final = df\n",
    "\n",
    "        print(f\"üìä Dataset final: {len(df_final)} muestras\")\n",
    "        return df_final\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en EDA: {str(e)}\")\n",
    "        print(\"Continuando sin data augmentation...\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MPSNExPEtpPS"
   },
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# FUNCIONES DE ENSEMBLE\n",
    "# ===========================================\n",
    "\n",
    "def train_voting_ensemble(df, label_column='IsToxic', text_column='text_processed'):\n",
    "    \"\"\"\n",
    "    Entrenar un ensemble de clasificadores usando VotingClassifier\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"üéØ Preparando datos para VotingClassifier...\")\n",
    "\n",
    "        X = df[text_column].fillna('').astype(str)\n",
    "        y = df[label_column]\n",
    "\n",
    "        # Vectorizaci√≥n TF-IDF\n",
    "        print(\"üìä Aplicando vectorizaci√≥n TF-IDF...\")\n",
    "        tfidf = TfidfVectorizer(\n",
    "            max_features=5000,\n",
    "            ngram_range=(1, 2),\n",
    "            stop_words='english',\n",
    "            min_df=2,\n",
    "            max_df=0.95\n",
    "        )\n",
    "        X_vec = tfidf.fit_transform(X)\n",
    "\n",
    "        # Divisi√≥n train-test\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_vec, y, test_size=0.2, stratify=y, random_state=42\n",
    "        )\n",
    "\n",
    "        print(f\"üìà Datos de entrenamiento: {X_train.shape[0]} muestras\")\n",
    "        print(f\"üìà Datos de prueba: {X_test.shape[0]} muestras\")\n",
    "\n",
    "        # Definir clasificadores\n",
    "        clf1 = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42)\n",
    "        clf2 = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n",
    "        clf3 = SVC(kernel='linear', probability=True, class_weight='balanced', random_state=42)\n",
    "\n",
    "        # Crear ensemble\n",
    "        voting = VotingClassifier(\n",
    "            estimators=[\n",
    "                ('lr', clf1),\n",
    "                ('rf', clf2),\n",
    "                ('svm', clf3)\n",
    "            ],\n",
    "            voting='soft'\n",
    "        )\n",
    "\n",
    "        print(\"üöÄ Entrenando VotingClassifier...\")\n",
    "        voting.fit(X_train, y_train)\n",
    "\n",
    "        # Predicciones\n",
    "        y_pred = voting.predict(X_test)\n",
    "        y_prob = voting.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # M√©tricas\n",
    "        print(\"\\nüìä RESULTADOS VotingClassifier:\")\n",
    "        print(\"=\"*50)\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        print(f\"ROC-AUC: {roc_auc_score(y_test, y_prob):.4f}\")\n",
    "        print(f\"Average Precision: {average_precision_score(y_test, y_prob):.4f}\")\n",
    "\n",
    "        return voting, tfidf\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en VotingClassifier: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5_4C0Tgzts5_"
   },
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# FUNCIONES DE FINE-TUNING\n",
    "# ===========================================\n",
    "\n",
    "def fine_tune_distilbert(df, label_column='IsToxic', text_column='text_cleaned'):\n",
    "    \"\"\"\n",
    "    Fine-tuning de DistilBERT con par√°metros corregidos para compatibilidad\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"ü§ñ Iniciando fine-tuning de DistilBERT...\")\n",
    "\n",
    "        tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "        df['label'] = df[label_column].astype(int)\n",
    "\n",
    "        # Limpiar textos\n",
    "        texts = df[text_column].fillna('').astype(str).tolist()\n",
    "        labels = df['label'].tolist()\n",
    "\n",
    "        # Dividir datos\n",
    "        train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "            texts, labels, test_size=0.2, stratify=labels, random_state=42\n",
    "        )\n",
    "\n",
    "        print(f\"üìä Datos de entrenamiento: {len(train_texts)} muestras\")\n",
    "        print(f\"üìä Datos de validaci√≥n: {len(val_texts)} muestras\")\n",
    "\n",
    "        # Tokenizaci√≥n\n",
    "        print(\"üî§ Tokenizando textos...\")\n",
    "        train_encodings = tokenizer(\n",
    "            train_texts,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=256  # Reducido para usar menos memoria\n",
    "        )\n",
    "        val_encodings = tokenizer(\n",
    "            val_texts,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=256\n",
    "        )\n",
    "\n",
    "        # Crear datasets\n",
    "        train_dataset = Dataset.from_dict({**train_encodings, 'label': train_labels})\n",
    "        val_dataset = Dataset.from_dict({**val_encodings, 'label': val_labels})\n",
    "\n",
    "        # Cargar modelo\n",
    "        model = DistilBertForSequenceClassification.from_pretrained(\n",
    "            'distilbert-base-uncased',\n",
    "            num_labels=2\n",
    "        )\n",
    "\n",
    "        # Argumentos de entrenamiento corregidos\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir='./results',\n",
    "            num_train_epochs=2,  # Reducido para CPU\n",
    "            per_device_train_batch_size=8,  # Reducido para CPU\n",
    "            per_device_eval_batch_size=16,\n",
    "            learning_rate=2e-5,\n",
    "            eval_strategy=\"epoch\",  # Par√°metro corregido\n",
    "            save_strategy=\"epoch\",\n",
    "            logging_dir='./logs',\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            save_total_limit=1,\n",
    "            weight_decay=0.01,\n",
    "            logging_steps=50,\n",
    "            warmup_steps=100,\n",
    "            remove_unused_columns=False,\n",
    "            report_to=[],  # Desactivar completamente wandb\n",
    "            dataloader_pin_memory=False,\n",
    "            run_name=\"toxicity_detection_run\"  # Nombre espec√≠fico para el run\n",
    "        )\n",
    "\n",
    "        def compute_metrics(eval_pred):\n",
    "            \"\"\"Funci√≥n para calcular m√©tricas durante el entrenamiento\"\"\"\n",
    "            logits, labels = eval_pred\n",
    "            preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "            # Calcular m√©tricas\n",
    "            f1 = f1_score(labels, preds, average='weighted')\n",
    "\n",
    "            # Para ROC-AUC necesitamos las probabilidades\n",
    "            probs = torch.softmax(torch.tensor(logits), dim=-1)[:, 1].numpy()\n",
    "            auc = roc_auc_score(labels, probs)\n",
    "            avgp = average_precision_score(labels, probs)\n",
    "\n",
    "            return {\n",
    "                \"f1\": f1,\n",
    "                \"roc_auc\": auc,\n",
    "                \"avg_precision\": avgp\n",
    "            }\n",
    "\n",
    "        # Crear trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            compute_metrics=compute_metrics\n",
    "        )\n",
    "\n",
    "        # Entrenar modelo\n",
    "        print(\"üöÄ Entrenando modelo...\")\n",
    "        train_output = trainer.train()\n",
    "\n",
    "        # Evaluar modelo\n",
    "        print(\"üìä Evaluando modelo...\")\n",
    "        eval_output = trainer.evaluate()\n",
    "\n",
    "        # Mostrar resultados\n",
    "        print(\"\\nüìà RESULTADOS FINALES DistilBERT:\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"F1-score: {eval_output['eval_f1']:.4f}\")\n",
    "        print(f\"ROC-AUC: {eval_output['eval_roc_auc']:.4f}\")\n",
    "        print(f\"Avg Precision: {eval_output['eval_avg_precision']:.4f}\")\n",
    "        print(f\"Loss: {eval_output['eval_loss']:.4f}\")\n",
    "\n",
    "        # An√°lisis de overfitting\n",
    "        print(\"\\nüîç AN√ÅLISIS DE OVERFITTING:\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        # Obtener m√©tricas de entrenamiento del √∫ltimo epoch\n",
    "        train_logs = trainer.state.log_history\n",
    "\n",
    "        # Filtrar logs de entrenamiento y evaluaci√≥n\n",
    "        train_metrics = [log for log in train_logs if 'train_loss' in log]\n",
    "        eval_metrics = [log for log in train_logs if 'eval_loss' in log]\n",
    "\n",
    "        if train_metrics and eval_metrics:\n",
    "            final_train_loss = train_metrics[-1]['train_loss']\n",
    "            final_eval_loss = eval_metrics[-1]['eval_loss']\n",
    "\n",
    "            loss_gap = final_eval_loss - final_train_loss\n",
    "\n",
    "            print(f\"üìä Train Loss: {final_train_loss:.4f}\")\n",
    "            print(f\"üìä Eval Loss: {final_eval_loss:.4f}\")\n",
    "            print(f\"üìä Gap (Eval - Train): {loss_gap:.4f}\")\n",
    "\n",
    "            # Interpretaci√≥n del overfitting\n",
    "            if loss_gap > 0.3:\n",
    "                print(\"üö® OVERFITTING ALTO - Gap > 0.3\")\n",
    "                print(\"   Recomendaciones:\")\n",
    "                print(\"   - Reducir epochs o learning rate\")\n",
    "                print(\"   - Aumentar regularizaci√≥n (weight_decay)\")\n",
    "                print(\"   - Usar m√°s datos de entrenamiento\")\n",
    "            elif loss_gap > 0.1:\n",
    "                print(\"‚ö†Ô∏è OVERFITTING MODERADO - Gap > 0.1\")\n",
    "                print(\"   Recomendaciones:\")\n",
    "                print(\"   - Monitorear m√°s de cerca\")\n",
    "                print(\"   - Considerar early stopping\")\n",
    "            else:\n",
    "                print(\"‚úÖ OVERFITTING BAJO - Gap <= 0.1\")\n",
    "                print(\"   El modelo generaliza bien\")\n",
    "\n",
    "            # Evoluci√≥n durante el entrenamiento\n",
    "            print(f\"\\nüìà EVOLUCI√ìN DEL LOSS:\")\n",
    "            for i, (train_log, eval_log) in enumerate(zip(train_metrics, eval_metrics)):\n",
    "                epoch = i + 1\n",
    "                train_loss = train_log['train_loss']\n",
    "                eval_loss = eval_log['eval_loss']\n",
    "                gap = eval_loss - train_loss\n",
    "                print(f\"   Epoch {epoch}: Train={train_loss:.4f}, Eval={eval_loss:.4f}, Gap={gap:.4f}\")\n",
    "\n",
    "        return trainer, eval_output\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en fine-tuning: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "McDR5jD8uk77"
   },
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# FUNCIONES DE UTILIDAD\n",
    "# ===========================================\n",
    "\n",
    "def check_gpu_memory():\n",
    "    \"\"\"Verificar el uso de memoria GPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"üî• GPU disponible: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"Memoria total: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "        print(f\"Memoria libre: {torch.cuda.memory_reserved(0) / 1024**3:.1f} GB\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è GPU no disponible, usando CPU\")\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Limpiar memoria GPU y cache\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    print(\"üßπ Memoria limpiada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yZkLlavIwKgw"
   },
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# PIPELINE PRINCIPAL\n",
    "# ===========================================\n",
    "\n",
    "def run_complete_pipeline(df_path=\"dataset_processed_complete.csv\"):\n",
    "    \"\"\"\n",
    "    Ejecutar el pipeline completo con manejo de errores\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Verificar GPU\n",
    "        check_gpu_memory()\n",
    "\n",
    "        # Cargar datos\n",
    "        print(\"üìÅ Cargando dataset...\")\n",
    "        df = pd.read_csv(df_path)\n",
    "        print(f\"Dataset cargado: {len(df)} filas\")\n",
    "\n",
    "        # Mostrar informaci√≥n b√°sica del dataset\n",
    "        print(f\"üìä Distribuci√≥n de clases:\")\n",
    "        print(df['IsToxic'].value_counts())\n",
    "\n",
    "        # Verificar columnas requeridas\n",
    "        required_columns = ['IsToxic', 'text_processed', 'text_cleaned']\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "\n",
    "        if missing_columns:\n",
    "            print(f\"‚ö†Ô∏è Columnas faltantes: {missing_columns}\")\n",
    "            print(\"Columnas disponibles:\", df.columns.tolist())\n",
    "\n",
    "            # Intentar usar columnas alternativas\n",
    "            if 'text_processed' not in df.columns and 'text' in df.columns:\n",
    "                df['text_processed'] = df['text']\n",
    "                print(\"‚úÖ Usando 'text' como 'text_processed'\")\n",
    "\n",
    "            if 'text_cleaned' not in df.columns and 'text' in df.columns:\n",
    "                df['text_cleaned'] = df['text']\n",
    "                print(\"‚úÖ Usando 'text' como 'text_cleaned'\")\n",
    "\n",
    "        # Aplicar EDA con manejo de errores\n",
    "        print(\"\\nüîÑ Aplicando Data Augmentation...\")\n",
    "        df_aug = apply_eda_safe(df, text_column='text_processed', num_aug=2)\n",
    "\n",
    "        # Entrenar ensemble\n",
    "        print(\"\\nüéØ Entrenando VotingClassifier...\")\n",
    "        voting_model, tfidf_vectorizer = train_voting_ensemble(df_aug, text_column='text_processed')\n",
    "\n",
    "        # Fine-tuning DistilBERT (solo si hay suficiente memoria)\n",
    "        if len(df_aug) > 100:  # Solo si hay suficientes datos\n",
    "            print(\"\\nü§ñ Iniciando fine-tuning de DistilBERT...\")\n",
    "            trainer, bert_results = fine_tune_distilbert(df_aug, text_column='text_cleaned')\n",
    "            return voting_model, tfidf_vectorizer, trainer, bert_results\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Dataset muy peque√±o, saltando fine-tuning de BERT\")\n",
    "            return voting_model, tfidf_vectorizer, None, None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en pipeline: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "824fdef3bda84df69b1d5878794a99fa",
      "e2c7a55b513845b0a557f8454d9c2c55",
      "5ab1a9e33de74318aa06a844862b6de9",
      "df4efdd55e0944518c2fb195f4d8ad08",
      "0a24728eab9c47c2824e36bb1ae1ad70",
      "b5d0ceae38bc4b709cade737a6d9b2c3",
      "93ae592f2e2e4497aa0e80c0f80f8731",
      "7bcb6f2c79c84f809b09350ef560de5c",
      "942a4f81c15141af8386aed11066c67f",
      "cff7958af7ee40e4adf8b35004ba9d9a",
      "ff97150b69404c4e9787573ed3760c23",
      "8be39764190a45eda370429173838fbe",
      "66e04a722e8c460e90bceb6ef886fe08",
      "0dec1dba9a994f8d9842817e24a28072",
      "4cf27876ab8b424eac11efc9831e7768",
      "46c651cf040d48f79eb580ce65c5c963",
      "516ffc732ff749808ff972e31ffc03de",
      "e06e25b2b90341b6ae11dd227a2ae4c4",
      "8edc310ae44041298a57062f5195ca55",
      "01a24dae2a304f558dde624e843d28c0",
      "613f721adfc541d68afb113b6aa200b1",
      "a2961a49929945e79918295cf88139c0",
      "ba1593ce0522440da193c340c2830491",
      "36d36cbf53c743a7975e6eedad9fe2ba",
      "49f1c9ad81654dec829567e5f2fc55cd",
      "3ac14c6cbc984062a3178d0ddc7a8d2a",
      "de5777a3165448109e5bf6ba68c86111",
      "8e63e8b7f12046e5aa73ff646aef8024",
      "48ef9fff0dc3440fbf46633656b98bb3",
      "a93a4f3d28044c35af522d418e6b1213",
      "abe4efb66b9c4633acb0488b2429e100",
      "5c150e77959e4d98b085cb517b6c6b57",
      "29c935b94fc8491ca810cd7a13282769",
      "d96812e514b54308a8d328dd8fc42fb4",
      "1f4b4dff573348f78378632ad85da4aa",
      "1ed56b6039db4959b3fe9457f5f25406",
      "9292f928314a4cf29a9c87a821d751f5",
      "c6a32326e83643d2a03ad05ba2fa326f",
      "6014ccbf9b8f44fea29c8c5d7972207f",
      "9d8925f401cd42e9883958baca418306",
      "0598c216f12b45d895a7d0b68042400b",
      "dbb232b807a24b948b493c3eea16e999",
      "6ad2b5775b2947119bf6969334d999ba",
      "0f5df86f67704063b1d1875ddd175e7f",
      "cc78deece11642a2adcc861090615297",
      "44e15d2720534b549f78cd99085aa52c",
      "dab76a55cbf741e38d7a7307253a852a",
      "a27baad06e90496591a3235ba4a8cd27",
      "86b22d161b7d41a3b758b11a400024e6",
      "ca925c537eb14b7ca3c53323908da233",
      "e2c9e04e1713494287f49b5d0e01f6b0",
      "81a66be46faa4bbb99ae0f6706187897",
      "2da7504b3d19437a92c16cfcc44bbd91",
      "32f7372523d84f20903cd34ba50f6797",
      "ab4c621f88554da996093e52e135d96f"
     ]
    },
    "id": "hXABfDuTwU4j",
    "outputId": "cca372b6-6341-4732-9a8c-a62e70a87aed"
   },
   "outputs": [],
   "source": [
    "# Ejecutar pipeline completo\n",
    "\n",
    "# desactivar wandb\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "print(\"üöÄ Iniciando pipeline de detecci√≥n de toxicidad...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "voting_model, tfidf_vectorizer, trainer, bert_results = run_complete_pipeline()\n",
    "\n",
    "if voting_model is not None:\n",
    "    print(\"\\nüéâ Pipeline completado exitosamente!\")\n",
    "    print(\"‚úÖ VotingClassifier entrenado\")\n",
    "    if trainer is not None:\n",
    "        print(\"‚úÖ DistilBERT fine-tuneado\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Pipeline fall√≥. Revisa los errores arriba.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
