{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ne6l2oiNuEV7"
   },
   "source": [
    "# **Modelado Avanzado y Ajuste de Hiperpar√°metros**\n",
    "\n",
    "Este notebook es la extensi√≥n avanzada del trabajo iniciado en los modelos baseline (`andrea-model-training-baseline.ipynb`). Mientras que all√≠ us√°bamos modelos cl√°sicos (LogisticRegression, MultinomialNB, etc.) con vectorizaci√≥n textual (TF-IDF, CountVectorizer), aqu√≠ damos el salto a:\n",
    "\n",
    "- T√©cnicas modernas de NLP: embeddings + LSTM\n",
    "- Ajuste de hiperpar√°metros impl√≠cito mediante regularizaci√≥n\n",
    "- Evaluaci√≥n formal del overfitting, requisito espec√≠fico del proyecto\n",
    "\n",
    "**Objetivo**: Crear un modelo m√°s expresivo (deep learning) y evaluar su generalizaci√≥n, asegurando que no haya overfitting significativo (diferencia ‚â§ 5pp en F1 entre entrenamiento y test)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ruQLDv2iuF70"
   },
   "source": [
    "## **Librer√≠as y Carga de datos**\n",
    "\n",
    "#### **¬øQu√© datos usamos y por qu√©?**\n",
    "- Usamos el dataset `text_cleaned`, que proviene del pipeline de preprocesamiento.\n",
    "- La columna `text_cleaned` fue limpiada: sin emojis, contracciones, URLs, ruido.\n",
    "- Elegimos `IsToxic` como etiqueta, siguiendo la misma l√≥gica del baseline, para poder comparar.\n",
    "\n",
    "üß† Esto garantiza continuidad metodol√≥gica: estamos evaluando si el modelo avanzado realmente mejora el baseline en condiciones comparables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_4h8yoOft6MA"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle, json, os, re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "88eeT92tuURq"
   },
   "outputs": [],
   "source": [
    "# Par√°metros\n",
    "max_words = 10000\n",
    "max_len = 150\n",
    "embedding_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bcMYavC8uvA-"
   },
   "outputs": [],
   "source": [
    "# üì• Cargar datos\n",
    "DATA_DIR = '../processed_data/text_cleaned'\n",
    "X_train = pd.read_csv(f'{DATA_DIR}/X_train.csv').squeeze()\n",
    "X_test = pd.read_csv(f'{DATA_DIR}/X_test.csv').squeeze()\n",
    "y_train = pd.read_csv(f'{DATA_DIR}/y_train.csv')['IsToxic']\n",
    "y_test = pd.read_csv(f'{DATA_DIR}/y_test.csv')['IsToxic']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1S-nA9SUu1gh"
   },
   "source": [
    "## **Tokenizaci√≥n y secuenciaci√≥n**\n",
    "\n",
    "### **¬øQu√© hacemos aqu√≠?**\n",
    "\n",
    "- **Tokenizer**: construye un vocabulario a partir del texto. Cada palabra se convierte en un entero.\n",
    "- **OOV token**: maneja palabras desconocidas en test con un token especial.\n",
    "\n",
    "- **pad_sequences**: asegura que todas las secuencias tengan la misma longitud (truncando o completando con ceros al final).\n",
    "\n",
    "### **¬øPor qu√© es necesario?**\n",
    "\n",
    "üß† Los modelos de deep learning no entienden strings. Necesitan secuencias de enteros, de longitud fija. Este proceso traduce lenguaje natural a formato num√©rico estructurado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-g6qkRz3u7N8"
   },
   "outputs": [],
   "source": [
    "# üî§ Tokenizaci√≥n y secuenciaci√≥n\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "raJtIkzEvcZ2"
   },
   "source": [
    "## **Definici√≥n del modelo avanzado (Embedding + LSTM)**\n",
    "\n",
    "### **Arquitectura detallada**\n",
    "\n",
    "| Capa                 | Funci√≥n                                                              |\n",
    "| -------------------- | -------------------------------------------------------------------- |\n",
    "| `Embedding`          | Transforma cada palabra (√≠ndice) en un vector denso y aprendible     |\n",
    "| `Bidirectional LSTM` | Modelo secuencial que lee el texto hacia adelante y hacia atr√°s      |\n",
    "| `Dense + ReLU`       | Capa oculta completamente conectada                                  |\n",
    "| `Dropout`            | Previene overfitting desconectando neuronas al azar en entrenamiento |\n",
    "| `Dense + sigmoid`    | Salida binaria (probabilidad de toxicidad)                           |\n",
    "\n",
    "\n",
    "### **T√©cnicas avanzadas usadas aqu√≠:**\n",
    "\n",
    "- Embeddings aprendibles: los vectores de palabra se ajustan durante el entrenamiento.\n",
    "\n",
    "- Regularizaci√≥n L2 (kernel_regularizer=l2(0.01)): penaliza pesos grandes ‚Üí evita que el modelo se sobreajuste.\n",
    "\n",
    "- Dropout: mejora la generalizaci√≥n.\n",
    "\n",
    "- Bidirectional: LSTM lee el texto en ambas direcciones, capturando m√°s contexto sem√°ntico.\n",
    "\n",
    "üß† Este modelo es m√°s expresivo y flexible que los modelos lineales del baseline, por lo que se espera mejor rendimiento, pero tambi√©n mayor riesgo de overfitting si no se controla bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "id": "8bhEto-zvz1T",
    "outputId": "953bc176-09ef-4a79-8ecd-e8a9dd56c9d3"
   },
   "outputs": [],
   "source": [
    "# üß† Modelo LSTM con embeddings\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len),\n",
    "    Bidirectional(LSTM(64, return_sequences=False, dropout=0.3, recurrent_dropout=0.3, kernel_regularizer=l2(0.01))),\n",
    "    Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZWoXjFHmv3PJ"
   },
   "source": [
    "## **Entrenamiento con early stopping**\n",
    "\n",
    "- Entrenamos con el 80% del `train`, validamos en el 20% restante.\n",
    "- Si la `val_loss` no mejora durante 3 √©pocas, se detiene el entrenamiento.\n",
    "- Se restauran los mejores pesos (no los √∫ltimos) para evitar sobreajuste final.\n",
    "\n",
    "\n",
    "üß† Early stopping es una t√©cnica de regularizaci√≥n pr√°ctica que evita el sobreentrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aBlJe9DlwKoq",
    "outputId": "c2ea497b-e6b9-495b-d28f-1156c508528f"
   },
   "outputs": [],
   "source": [
    "# ‚è≥ Entrenamiento con early stopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "history = model.fit(\n",
    "    X_train_pad, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=15,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ebEloBxgwZQc"
   },
   "source": [
    "## **Evaluaci√≥n y detecci√≥n de overfitting**\n",
    "\n",
    "###**¬øQu√© hacemos?**\n",
    "- Generamos predicciones binarias (prob > 0.5).\n",
    "- Calculamos F1-score, la m√©trica m√°s relevante para clasificaci√≥n desequilibrada.\n",
    "- Medimos la diferencia absoluta entre el F1 de entrenamiento y test.\n",
    "\n",
    "###**¬øPor qu√© 5pp como umbral?**\n",
    "Un delta mayor a 5 puntos porcentuales en F1 (por ejemplo, 0.85 vs 0.75) indica que el modelo aprendi√≥ demasiado bien el train, pero no logra generalizar ‚Üí overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P41NurFVwtSJ",
    "outputId": "d72a1cbf-0e53-492e-898e-750ad4f75d9c"
   },
   "outputs": [],
   "source": [
    "# üìà Evaluaci√≥n\n",
    "train_preds = (model.predict(X_train_pad) > 0.5).astype(int)\n",
    "test_preds = (model.predict(X_test_pad) > 0.5).astype(int)\n",
    "\n",
    "train_f1 = f1_score(y_train, train_preds)\n",
    "test_f1 = f1_score(y_test, test_preds)\n",
    "\n",
    "diff = abs(train_f1 - test_f1) * 100\n",
    "print(f\"Train F1: {train_f1:.4f}  |  Test F1: {test_f1:.4f}  | Œî = {diff:.2f}pp\")\n",
    "if diff <= 5:\n",
    "    print(\"‚úÖ No hay overfitting significativo\")\n",
    "else:\n",
    "    print(\"‚ùå Posible overfitting: diferencia superior a 5pp\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8294ieVwyUu"
   },
   "source": [
    "## **Reporte final y curvas**\n",
    "\n",
    "Incluye:\n",
    "- Precision: qu√© proporci√≥n de los positivos predichos eran realmente positivos\n",
    "- Recall: qu√© proporci√≥n de los positivos reales fueron detectados\n",
    "- F1-score: media arm√≥nica entre ambas\n",
    "- Soporte: cu√°ntos ejemplos hay por clase\n",
    "\n",
    "Dos gr√°ficos clave:\n",
    "\n",
    "- Accuracy Train vs Validation\n",
    "  - Si divergen mucho ‚Üí el modelo est√° memorizando\n",
    "\n",
    "- Loss Train vs Validation\n",
    "  - Un val_loss creciente mientras train_loss baja ‚Üí clara se√±al de overfitting\n",
    "\n",
    "üß† Estas gr√°ficas son herramientas esenciales para el diagn√≥stico temprano de problemas de generalizaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6BVJlqa5xKe9",
    "outputId": "ed8086bc-ae59-455c-b818-0b93321118cc"
   },
   "outputs": [],
   "source": [
    "# üßæ Reporte detallado\n",
    "print(\"\\nClassification Report (Test):\")\n",
    "print(classification_report(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "hiyr-v9cxOAf",
    "outputId": "4357b7ef-7dce-438b-eadb-5aa8a41d237e"
   },
   "outputs": [],
   "source": [
    "\n",
    "# üìä Visualizaci√≥n de curvas de entrenamiento\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['accuracy'], label='Train Acc')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
