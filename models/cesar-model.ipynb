{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv(\"../data/youtoxic_english_1000.csv\")\n",
    "\n",
    "print(f\"Filas: {df.shape[0]}, Columnas: {df.shape[1]}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Revisar columnas con solo ceros o sin representaci√≥n significativa\n",
    "etiquetas = [\n",
    "    \"IsToxic\", \"IsAbusive\", \"IsThreat\", \"IsProvocative\", \"IsObscene\",\n",
    "    \"IsHatespeech\", \"IsRacist\", \"IsNationalist\", \"IsSexist\", \"IsHomophobic\",\n",
    "    \"IsReligiousHate\", \"IsRadicalism\"\n",
    "]\n",
    "\n",
    "# Mostrar cu√°ntas veces aparece \"True\" en cada etiqueta\n",
    "print(\"Etiquetas activas:\")\n",
    "print(df[etiquetas].sum().sort_values())\n",
    "\n",
    "# 2. Eliminar columnas sin representaci√≥n (sum == 0 o sum == 1)\n",
    "etiquetas_utiles = [col for col in etiquetas if df[col].sum() > 1]\n",
    "print(\"\\nEtiquetas que conservaremos:\", etiquetas_utiles)\n",
    "\n",
    "# 3. Eliminar columnas irrelevantes para el modelado\n",
    "columnas_irrelevantes = [\"CommentId\", \"VideoId\"]\n",
    "df.drop(columns=columnas_irrelevantes, inplace=True)\n",
    "\n",
    "# 4. (Opcional) Eliminar duplicados por texto\n",
    "df.drop_duplicates(subset=\"Text\", inplace=True)\n",
    "\n",
    "# 5. Dejar el DataFrame con solo las columnas √∫tiles\n",
    "columnas_utiles = [\"Text\"] + etiquetas_utiles\n",
    "df = df[columnas_utiles]\n",
    "\n",
    "# 6. Confirmar\n",
    "print(f\"\\nDataset limpio ‚Üí Filas: {df.shape[0]}, Columnas: {df.shape[1]}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Limpieza previa del dataset\n",
    "\n",
    "Antes de aplicar el preprocesamiento textual, revisamos las columnas del dataset:\n",
    "\n",
    "- Eliminamos etiquetas que **no tienen representaci√≥n suficiente** (por ejemplo, `IsHomophobic`, `IsRadicalism`) ya que no aportar√≠an valor al modelo.\n",
    "- Quitamos columnas irrelevantes como `CommentId` y `VideoId`.\n",
    "- Eliminamos posibles duplicados exactos en los comentarios (`Text`).\n",
    "\n",
    "Esto nos permite trabajar sobre un dataset m√°s limpio, centrado en los comentarios y en las etiquetas que tienen informaci√≥n √∫til. De este modo, evitamos introducir ruido o clases vac√≠as en el modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "\n",
    "# Cargar modelo de ingl√©s\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Funci√≥n para limpiar y lematizar texto\n",
    "def preprocesar_texto(texto):\n",
    "    # 1. Eliminar URLs, s√≠mbolos especiales y n√∫meros\n",
    "    texto = re.sub(r\"http\\S+|www\\S+|[^a-zA-Z\\s]\", \"\", texto.lower())\n",
    "    \n",
    "    # 2. Procesar con SpaCy\n",
    "    doc = nlp(texto)\n",
    "\n",
    "    # 3. Eliminar stopwords y obtener lemas\n",
    "    tokens_limpios = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "    \n",
    "    return \" \".join(tokens_limpios)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar limpieza sobre una copia del texto\n",
    "df[\"CleanText\"] = df[\"Text\"].apply(preprocesar_texto)\n",
    "\n",
    "# Ver resultado en 3 ejemplos\n",
    "df[[\"Text\", \"CleanText\"]].sample(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Preprocesamiento del texto con SpaCy\n",
    "\n",
    "En este paso limpiamos y normalizamos los comentarios para que puedan ser procesados por modelos de machine learning. Para ello utilizamos **SpaCy**, una librer√≠a especializada en procesamiento de lenguaje natural.\n",
    "\n",
    "Nuestra funci√≥n `preprocesar_texto()` realiza las siguientes tareas:\n",
    "\n",
    "1. **Elimina elementos innecesarios** del texto original:\n",
    "   - URLs, s√≠mbolos, n√∫meros y puntuaci√≥n.\n",
    "   - Convierte todo a min√∫sculas.\n",
    "\n",
    "2. **Tokeniza** (divide en palabras) y procesa cada palabra con SpaCy:\n",
    "   - Elimina las **palabras vac√≠as** (*stopwords*) como \"the\", \"you\", \"and\".\n",
    "   - Obtiene la **forma base** (*lema*) de cada palabra, por ejemplo:\n",
    "     - \"running\" ‚Üí \"run\"\n",
    "     - \"was\" ‚Üí \"be\"\n",
    "     - \"better\" ‚Üí \"good\"\n",
    "\n",
    "3. **Reconstruye el texto limpio**, que usaremos como entrada para vectorizaci√≥n y modelado.\n",
    "\n",
    "Este preprocesamiento mejora significativamente la calidad de las caracter√≠sticas que extraeremos del texto, ya que reduce el ruido y normaliza el lenguaje.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esto puede tardar unos segundos la primera vez (1.000 textos)\n",
    "df[\"CleanText\"] = df[\"Text\"].apply(preprocesar_texto)\n",
    "\n",
    "# Comprobar algunos ejemplos de texto original vs limpio\n",
    "df[[\"Text\", \"CleanText\"]].sample(5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Crear los vectorizadores\n",
    "vectorizer_bow = CountVectorizer(max_features=20)\n",
    "vectorizer_tfidf = TfidfVectorizer(max_features=20)\n",
    "\n",
    "# Aplicar sobre el texto limpio\n",
    "X_bow = vectorizer_bow.fit_transform(df[\"CleanText\"])\n",
    "X_tfidf = vectorizer_tfidf.fit_transform(df[\"CleanText\"])\n",
    "\n",
    "# Convertir a DataFrames para verlos\n",
    "df_bow = pd.DataFrame(X_bow.toarray(), columns=vectorizer_bow.get_feature_names_out())\n",
    "df_tfidf = pd.DataFrame(X_tfidf.toarray(), columns=vectorizer_tfidf.get_feature_names_out())\n",
    "\n",
    "# Mostrar las primeras filas de cada uno\n",
    "print(\"üî¢ Bag of Words:\")\n",
    "display(df_bow.head())\n",
    "\n",
    "print(\"‚ú® TF-IDF:\")\n",
    "display(df_tfidf.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Comparaci√≥n entre Bag of Words y TF-IDF\n",
    "\n",
    "Ambas t√©cnicas convierten texto en vectores num√©ricos, pero lo hacen de forma distinta:\n",
    "\n",
    "- **Bag of Words** simplemente cuenta cu√°ntas veces aparece cada palabra.\n",
    "- **TF-IDF** ajusta esos conteos seg√∫n la rareza de cada palabra en el conjunto total.\n",
    "\n",
    "#### ¬øQu√© vemos?\n",
    "- En BoW, los valores son enteros (frecuencias puras).\n",
    "- En TF-IDF, los valores son decimales ‚Üí ajustados por importancia.\n",
    "- Palabras comunes (como \"people\") tendr√°n menos peso en TF-IDF si aparecen en casi todos los comentarios.\n",
    "\n",
    "Ambas representaciones son √∫tiles, pero **TF-IDF suele funcionar mejor en clasificaci√≥n** al eliminar ruido estad√≠stico y destacar las palabras que verdaderamente diferencian un texto de otro.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
