{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv(\"../data/youtoxic_english_1000.csv\")\n",
    "\n",
    "print(f\"Filas: {df.shape[0]}, Columnas: {df.shape[1]}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Revisar columnas con solo ceros o sin representación significativa\n",
    "etiquetas = [\n",
    "    \"IsToxic\", \"IsAbusive\", \"IsThreat\", \"IsProvocative\", \"IsObscene\",\n",
    "    \"IsHatespeech\", \"IsRacist\", \"IsNationalist\", \"IsSexist\", \"IsHomophobic\",\n",
    "    \"IsReligiousHate\", \"IsRadicalism\"\n",
    "]\n",
    "\n",
    "# Mostrar cuántas veces aparece \"True\" en cada etiqueta\n",
    "print(\"Etiquetas activas:\")\n",
    "print(df[etiquetas].sum().sort_values())\n",
    "\n",
    "# 2. Eliminar columnas sin representación (sum == 0 o sum == 1)\n",
    "etiquetas_utiles = [col for col in etiquetas if df[col].sum() > 1]\n",
    "print(\"\\nEtiquetas que conservaremos:\", etiquetas_utiles)\n",
    "\n",
    "# 3. Eliminar columnas irrelevantes para el modelado\n",
    "columnas_irrelevantes = [\"CommentId\", \"VideoId\"]\n",
    "df.drop(columns=columnas_irrelevantes, inplace=True)\n",
    "\n",
    "# 4. (Opcional) Eliminar duplicados por texto\n",
    "df.drop_duplicates(subset=\"Text\", inplace=True)\n",
    "\n",
    "# 5. Dejar el DataFrame con solo las columnas útiles\n",
    "columnas_utiles = [\"Text\"] + etiquetas_utiles\n",
    "df = df[columnas_utiles]\n",
    "\n",
    "# 6. Confirmar\n",
    "print(f\"\\nDataset limpio → Filas: {df.shape[0]}, Columnas: {df.shape[1]}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Limpieza previa del dataset\n",
    "\n",
    "Antes de aplicar el preprocesamiento textual, revisamos las columnas del dataset:\n",
    "\n",
    "- Eliminamos etiquetas que **no tienen representación suficiente** (por ejemplo, `IsHomophobic`, `IsRadicalism`) ya que no aportarían valor al modelo.\n",
    "- Quitamos columnas irrelevantes como `CommentId` y `VideoId`.\n",
    "- Eliminamos posibles duplicados exactos en los comentarios (`Text`).\n",
    "\n",
    "Esto nos permite trabajar sobre un dataset más limpio, centrado en los comentarios y en las etiquetas que tienen información útil. De este modo, evitamos introducir ruido o clases vacías en el modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "\n",
    "# Cargar modelo de inglés\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Función para limpiar y lematizar texto\n",
    "def preprocesar_texto(texto):\n",
    "    # 1. Eliminar URLs, símbolos especiales y números\n",
    "    texto = re.sub(r\"http\\S+|www\\S+|[^a-zA-Z\\s]\", \"\", texto.lower())\n",
    "    \n",
    "    # 2. Procesar con SpaCy\n",
    "    doc = nlp(texto)\n",
    "\n",
    "    # 3. Eliminar stopwords y obtener lemas\n",
    "    tokens_limpios = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "    \n",
    "    return \" \".join(tokens_limpios)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar limpieza sobre una copia del texto\n",
    "df[\"CleanText\"] = df[\"Text\"].apply(preprocesar_texto)\n",
    "\n",
    "# Ver resultado en 3 ejemplos\n",
    "df[[\"Text\", \"CleanText\"]].sample(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Preprocesamiento del texto con SpaCy\n",
    "\n",
    "En este paso limpiamos y normalizamos los comentarios para que puedan ser procesados por modelos de machine learning. Para ello utilizamos **SpaCy**, una librería especializada en procesamiento de lenguaje natural.\n",
    "\n",
    "Nuestra función `preprocesar_texto()` realiza las siguientes tareas:\n",
    "\n",
    "1. **Elimina elementos innecesarios** del texto original:\n",
    "   - URLs, símbolos, números y puntuación.\n",
    "   - Convierte todo a minúsculas.\n",
    "\n",
    "2. **Tokeniza** (divide en palabras) y procesa cada palabra con SpaCy:\n",
    "   - Elimina las **palabras vacías** (*stopwords*) como \"the\", \"you\", \"and\".\n",
    "   - Obtiene la **forma base** (*lema*) de cada palabra, por ejemplo:\n",
    "     - \"running\" → \"run\"\n",
    "     - \"was\" → \"be\"\n",
    "     - \"better\" → \"good\"\n",
    "\n",
    "3. **Reconstruye el texto limpio**, que usaremos como entrada para vectorización y modelado.\n",
    "\n",
    "Este preprocesamiento mejora significativamente la calidad de las características que extraeremos del texto, ya que reduce el ruido y normaliza el lenguaje.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esto puede tardar unos segundos la primera vez (1.000 textos)\n",
    "df[\"CleanText\"] = df[\"Text\"].apply(preprocesar_texto)\n",
    "\n",
    "# Comprobar algunos ejemplos de texto original vs limpio\n",
    "df[[\"Text\", \"CleanText\"]].sample(5, random_state=42)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
