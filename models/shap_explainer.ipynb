{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Interpretabilidad del modelo de detecci√≥n de comentarios t√≥xicos\n",
    "\n",
    "Este an√°lisis tiene como objetivo explicar de forma clara y visual c√≥mo el modelo de clasificaci√≥n de comentarios t√≥xicos toma sus decisiones.\n",
    "\n",
    "Utilizaremos la librer√≠a **SHAP** para interpretar los resultados del modelo. SHAP nos ayuda a entender qu√© palabras influyen m√°s en que un comentario sea clasificado como t√≥xico o no.\n",
    "\n",
    "Este an√°lisis est√° pensado para ser comprendido tanto por perfiles t√©cnicos como no t√©cnicos (stakeholders).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import shap\n",
    "import numpy as np\n",
    "\n",
    "# Cargamos modelo y vectorizador\n",
    "model = joblib.load(\"../final_model/modelo_toxicidad_xgboost_final.pkl\")\n",
    "vectorizer = joblib.load(\"../final_model/vectorizer_toxicidad_final.pkl\")\n",
    "\n",
    "# Comentarios de ejemplo\n",
    "comments = [\n",
    "    \"You're disgusting and stupid.\",          # t√≥xico\n",
    "    \"Thank you for this amazing explanation.\", # no t√≥xico\n",
    "    \"I don't think this was helpful at all.\"   # ambiguo\n",
    "]\n",
    "\n",
    "# Vectorizamos comentarios\n",
    "X = vectorizer.transform(comments)\n",
    "\n",
    "# Obtenemos los nombres reales de las features (tokens del vocabulario)\n",
    "feature_names = vectorizer.get_feature_names_out(); # Usamos TreeExplainer porque el modelo es un XGBoost basado en √°rboles\n",
    "# Convertimos la matriz TF-IDF a densa con .toarray(), que es lo que necesita SHAP\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer(X.toarray())  # X fue definido previamente como vectorizado de ejemplos; # Mostramos los tokens m√°s influyentes a nivel global (positivo y negativo)\n",
    "shap.summary_plot(shap_values, X.toarray(), feature_names=feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el explicador SHAP con nombres reales\n",
    "explainer = shap.Explainer(model, feature_names=feature_names)\n",
    "\n",
    "# Obtenemos valores SHAP para cada comentario vectorizado\n",
    "shap_values = explainer(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Explicabilidad global con SHAP\n",
    "\n",
    "Vamos a utilizar SHAP para entender qu√© palabras del vocabulario aprendido por el modelo tienen un mayor impacto, en promedio, al predecir toxicidad.\n",
    "\n",
    "El gr√°fico generado por `shap.summary_plot` muestra:\n",
    "\n",
    "- En el eje **x**, el valor SHAP medio de cada palabra (cu√°nto contribuye, en general, a aumentar o disminuir la probabilidad de toxicidad).\n",
    "- En el eje **y**, las palabras m√°s influyentes (ordenadas de mayor a menor impacto).\n",
    "- **Cada punto** representa una aparici√≥n de esa palabra en alg√∫n comentario.\n",
    "- **El color** indica el valor del token en el comentario (alto = frecuente, bajo = poco frecuente).\n",
    "\n",
    "Este an√°lisis es √∫til para identificar qu√© t√©rminos el modelo considera m√°s t√≥xicos en general.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values)  # ‚úÖ Ahora muestra los tokens correctamente\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Explicabilidad global ‚Äì resumen por importancia media\n",
    "\n",
    "Este gr√°fico de barras muestra las palabras que tienen **mayor impacto medio absoluto** sobre las predicciones del modelo. Es decir, aquellas que m√°s influyen en general, independientemente de si ese impacto es positivo o negativo.\n",
    "\n",
    "- El valor num√©rico a la derecha de cada barra representa la **media del valor SHAP** absoluto de esa palabra.\n",
    "- A diferencia del gr√°fico anterior, **no se muestra la dispersi√≥n ni el color por valor**: este gr√°fico es √∫til para tener una visi√≥n r√°pida y priorizada de las palabras que m√°s peso tienen en las decisiones del modelo.\n",
    "\n",
    "Este an√°lisis ayuda a identificar qu√© tokens tienen mayor poder predictivo a lo largo de todo el conjunto de datos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Reflexi√≥n cr√≠tica: posibles sesgos y asociaciones espurias\n",
    "\n",
    "Uno de los tokens destacados en los gr√°ficos SHAP es **\"ferguson\"**. Aunque no es una palabra ofensiva en s√≠ misma, aparece con una contribuci√≥n media notable en las predicciones de toxicidad.\n",
    "\n",
    "Este caso es un ejemplo de lo que en aprendizaje autom√°tico se conoce como **asociaci√≥n espuria**: el modelo ha aprendido que la palabra \"ferguson\" suele aparecer en comentarios t√≥xicos, no porque sea una palabra t√≥xica, sino por el contexto social o medi√°tico en el que fue utilizada en el dataset original.\n",
    "\n",
    "Esto revela un posible **sesgo en los datos de entrenamiento**. Si no se revisa, el modelo podr√≠a clasificar como t√≥xicos comentarios informativos o respetuosos que simplemente mencionen ciertos temas o lugares sensibles.\n",
    "\n",
    "üîç Este hallazgo justifica el uso de t√©cnicas de interpretabilidad como **SHAP**, que nos permiten no solo entender c√≥mo decide el modelo, sino tambi√©n detectar y corregir errores, sesgos o asociaciones no deseadas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Comentario 1: T√≥xico\n",
    "plt.figure()\n",
    "shap.plots.waterfall(shap_values[0], max_display=10, show=False)\n",
    "plt.title(\"Comentario 1 ‚Äì T√≥xico\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Comentario 2: No t√≥xico\n",
    "plt.figure()\n",
    "shap.plots.waterfall(shap_values[1], max_display=10, show=False)\n",
    "plt.title(\"Comentario 2 ‚Äì No t√≥xico\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Ejemplos concretos: ¬øC√≥mo decide el modelo en cada comentario?\n",
    "\n",
    "Los siguientes gr√°ficos muestran c√≥mo el modelo llega a su decisi√≥n en **dos casos reales**. Utilizamos la visualizaci√≥n tipo `waterfall` de SHAP, que descompone la predicci√≥n en contribuciones individuales de cada token:\n",
    "\n",
    "- üî¥ **Rojo**: palabras que empujan la predicci√≥n hacia **t√≥xico**\n",
    "- üîµ **Azul**: palabras que empujan hacia **no t√≥xico**\n",
    "- `1¬∑palabra` ‚Üí la palabra est√° presente en el comentario\n",
    "- `0¬∑palabra` ‚Üí la palabra no aparece, pero SHAP estima su efecto si lo hiciera\n",
    "\n",
    "---\n",
    "\n",
    "### üî¥ Ejemplo 1 ‚Äì Comentario clasificado como **t√≥xico**\n",
    "\n",
    "- La palabra **\"stupid\"** est√° presente y tiene un peso importante hacia la toxicidad.\n",
    "- El modelo interpreta este t√©rmino como altamente ofensivo, y su presencia basta para elevar la predicci√≥n.\n",
    "- Los dem√°s tokens tienen impacto bajo o nulo.\n",
    "\n",
    "---\n",
    "\n",
    "### üîµ Ejemplo 2 ‚Äì Comentario clasificado como **no t√≥xico**\n",
    "\n",
    "- Aunque el texto podr√≠a parecer una cr√≠tica o estar en un contexto delicado, no contiene insultos ni lenguaje violento.\n",
    "- El modelo mantiene la predicci√≥n baja y descarta la toxicidad.\n",
    "- Esto sugiere que el modelo distingue correctamente entre **contenido cr√≠tico** y **contenido ofensivo**.\n",
    "\n",
    "---\n",
    "\n",
    "Gracias a esta explicaci√≥n local, podemos confirmar que:\n",
    "\n",
    "- El modelo no act√∫a como una caja negra: sus decisiones son rastreables y comprensibles.\n",
    "- Penaliza el uso expl√≠cito de lenguaje ofensivo, no los temas tratados.\n",
    "- SHAP nos permite validar que no hay sesgos evidentes en estos ejemplos, y que el modelo responde bien ante diferentes contextos.\n",
    "\n",
    "Estas visualizaciones refuerzan la confianza en el comportamiento del modelo y permiten detectar posibles errores antes de su despliegue.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Conclusiones finales y reflexi√≥n cr√≠tica\n",
    "\n",
    "Durante este an√°lisis, hemos evaluado no solo el rendimiento del modelo de clasificaci√≥n de toxicidad, sino tambi√©n su **capacidad de toma de decisiones y los posibles sesgos presentes** en su razonamiento interno, gracias a la interpretabilidad con SHAP.\n",
    "\n",
    "A trav√©s de gr√°ficos individuales (`waterfall`) y globales (`bar`), hemos podido identificar qu√© tokens tienen m√°s peso en la toma de decisiones del modelo y c√≥mo interact√∫an en diferentes tipos de comentarios (t√≥xicos y neutros).\n",
    "\n",
    "---\n",
    "\n",
    "### ¬øTiene sesgos nuestro modelo?\n",
    "\n",
    "**S√≠, muestra indicios de sesgos contextuales aprendidos durante el entrenamiento**, aunque no siempre se traducen en clasificaciones err√≥neas.\n",
    "\n",
    "#### Justificaci√≥n:\n",
    "\n",
    "- Palabras sensibles como `\"ferguson\"`, `\"cnn\"`, `\"black\"`, `\"african american\"`, `\"muslim\"`, etc. aparecen recurrentemente como tokens que **empujan hacia la predicci√≥n de toxicidad**, tanto en gr√°ficos individuales como globales.\n",
    "\n",
    "- En algunos comentarios **objetivos y no ofensivos**, el modelo muestra una **probabilidad elevada de toxicidad**, lo que indica que asocia ciertas palabras a patrones aprendidos como \"t√≥xicos\", **sin tener en cuenta el tono real del comentario**.\n",
    "\n",
    "- Sin embargo, el modelo **no clasifica autom√°ticamente como t√≥xicos** todos los comentarios que contienen esos t√©rminos. Esto sugiere que:\n",
    "\n",
    "  üîµ El modelo **no tiene un sesgo absoluto ni determinista**  \n",
    "  üî¥ Pero s√≠ ha aprendido **correlaciones espurias**  \n",
    "  _(Ejemplo: \"ferguson\" aparece en muchos comentarios t√≥xicos del dataset ‚Üí el modelo lo aprende como indicador de toxicidad)_\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusi√≥n cr√≠tica y valor a√±adido de SHAP\n",
    "\n",
    "Este an√°lisis muestra que el modelo, aunque funcional, **refleja los sesgos latentes de los datos de entrenamiento**. Por ello:\n",
    "\n",
    "- **SHAP es fundamental no solo para explicar predicciones, sino tambi√©n para auditar el modelo**\n",
    "- Detectar tokens con impacto indebido permite reflexionar sobre la necesidad de:\n",
    "  - Mejorar el dataset (m√°s ejemplos neutrales con palabras sensibles)\n",
    "  - Aplicar t√©cnicas de balanceo o filtrado sem√°ntico\n",
    "  - Incluir controles √©ticos antes de producci√≥n\n",
    "\n",
    "üí° En definitiva, este an√°lisis no solo explica el \"qu√©\" del modelo, sino tambi√©n el \"por qu√©\", y **abre la puerta a una mejora consciente y responsable de la IA aplicada al lenguaje.**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
